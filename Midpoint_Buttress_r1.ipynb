{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device name CPU\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    device_name = tf.test.gpu_device_name()\n",
    "else:\n",
    "    device_name = 'CPU'\n",
    "device_name = 'CPU' \n",
    "print(f'device name {device_name}')\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from math import cos,sin,tan,asin,acos,radians,sqrt,degrees,atan,atan2,copysign\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "import scipy\n",
    "from scipy.stats import norm\n",
    "import random\n",
    "import time\n",
    "import timeit\n",
    "import math\n",
    "import localization as lx\n",
    "import gzip\n",
    "\n",
    "import util.npose_util as nu\n",
    "import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "\n",
    "import joblib\n",
    "from sklearn.manifold import MDS\n",
    "import argparse\n",
    "from functools import partial\n",
    "from itertools import starmap,repeat,permutations\n",
    "\n",
    "from pymol import cmd, stored, selector\n",
    "\n",
    "import GenerateEndpoints as ge\n",
    "import HelixFit as hf\n",
    "import FitTransform as ft\n",
    "\n",
    "import seaborn as sns\n",
    "import util.RotationMethods as rm\n",
    "\n",
    "#reference helix for propogation\n",
    "zero_ih = nu.npose_from_file('util/zero_ih.pdb')\n",
    "tt = zero_ih.reshape(int(len(zero_ih)/5),5,4)\n",
    "stub = tt[7:10].reshape(15,4)\n",
    "\n",
    "np.set_printoptions(precision=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #load distance maps and endpoints dataset\n",
    "# def load_distance_map(name, dm_file='data/Fits_4H_dm_phi.npz'):\n",
    "#     rr = np.load(dm_file, allow_pickle=True)\n",
    "#     X_train, y_train , featNames = [rr[f] for f in rr.files]\n",
    "    \n",
    "    \n",
    "#     return X_train[y_train==name][:,:-4]\n",
    "\n",
    "# dm_file = 'data/Fits_4H_dm_phi.npz'\n",
    "# rr = np.load(dm_file, allow_pickle=True)\n",
    "# X_train, y_train , featNames = [rr[f] for f in rr.files]\n",
    "# # = 'data/4H_dataset/models/'\n",
    "# # cmd.load(f'{model_direc}{y_train[0]}.pdb')\n",
    "# # cmd.save(f'output/test.pdb')\n",
    "\n",
    "# #endpoints for data set \n",
    "# Fits4H_file = 'data/Fits_4H.csv'\n",
    "# dfRead = pd.read_csv(Fits4H_file)\n",
    "# df1 = ft.prepData_Str(dfRead,rmsd_filter=100)\n",
    "# df2 = ft.EndPoint(df1)\n",
    "# ep = df2.to_numpy()[:,:24].astype(float).reshape((-1,8,3))\n",
    "# X = ep\n",
    "#np.savez_compressed('data/ep_for_X.npz', ep=X)\n",
    "rr = np.load(f'data/ep_for_X.npz', allow_pickle=True)\n",
    "X = [rr[f] for f in rr.files][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_helix_ep(ep_in,helices_desired=[0,1],num_helices=4):\n",
    "    \n",
    "    num_ep = num_helices*2\n",
    "    hi = np.array(helices_desired,dtype=int)\n",
    "    h_ep = np.array(range(num_ep)).reshape((-1,2)) #generate helix to endpoint mapping\n",
    "    \n",
    "    #alternate example for indexing batch of X \n",
    "    #X.reshape((X.shape[0],-1))[:,indexarray]\n",
    "    \n",
    "    #select desired endpoints from  batch of endpoints\n",
    "    return ep_in[np.ix_(np.array(range(ep_in.shape[0])),h_ep[hi].flatten(), np.array(range(ep_in.shape[2])))]\n",
    "    \n",
    "def get_midpoint(ep_in,helices_desired=[0,1],num_helices=4):\n",
    "    \n",
    "    num_ep = num_helices*2\n",
    "    \n",
    "    ind_ep = index_helix_ep(ep_in, helices_desired=helices_desired, num_helices=4)\n",
    "    \n",
    "    #calculate midpoint\n",
    "    midpoint = ind_ep.sum(axis=1)/np.repeat(ind_ep.shape[1], ind_ep.shape[2])\n",
    "    \n",
    "    return midpoint\n",
    "\n",
    "def get_stubs_from_points(ep_in,index=[0,1,2]):\n",
    "#def get_stubs_from_n_ca_c(n, ca, c):\n",
    "    \"\"\"Modified from Brian's npose code  get_stubs_from_n_ca_c, index references 3 points, to define plane.\n",
    "    \"\"\"\n",
    "    e1 = ep_in[:,index[1]]-ep_in[:,index[0]]\n",
    "    e1 = np.divide( e1, np.linalg.norm(e1, axis=1)[..., None] )\n",
    "\n",
    "    e3 = np.cross( e1, ep_in[:,index[2]]-ep_in[:,index[0]], axis=1 )\n",
    "    e3 = np.divide( e3, np.linalg.norm(e3, axis=1)[..., None] )\n",
    "\n",
    "    e2 = np.cross( e3, e1, axis=1 )\n",
    "\n",
    "    stub = np.zeros((len(ep_in), 4, 4))\n",
    "    stub[...,:3,0] = e1\n",
    "    stub[...,:3,1] = e2\n",
    "    stub[...,:3,2] = e3\n",
    "    stub[...,:3,3] = ep_in[:,index[1]]\n",
    "    stub[...,3,3] = 1.0\n",
    "\n",
    "    return stub\n",
    "\n",
    "def xform_npose_2batch(xform, npose):\n",
    "    #single batch code  util.npose_util as xform_npose\n",
    "    return np.matmul(np.repeat(xform[:,np.newaxis,...],npose.shape[1],axis=1),npose[...,None]).squeeze(-1)\n",
    "\n",
    "def xform_to_z_plane(mobile, index_mobile=[0,1,2]):\n",
    "    \"\"\"rotate points into the z-plane for trilaterization. needs additional translation/reflection\"\"\"\n",
    "\n",
    "    mobile_stub = get_stubs_from_points(mobile, index=index_mobile)\n",
    "    mobile_stub_inv = np.linalg.inv(mobile_stub)\n",
    "    \n",
    "    z_plane_ref = np.repeat(np.array([[[0,0,0],[1,0,0],[1,1,0]]]), mobile.shape[0],axis=0)\n",
    "\n",
    "    ref_stub = get_stubs_from_points(z_plane_ref, index=[0,1,2])\n",
    "\n",
    "    xform = ref_stub @ mobile_stub_inv\n",
    "\n",
    "    return xform\n",
    "\n",
    "\n",
    "def rotate_base_tri_Zplane(endpoint_midpoints, target_point=4, index_mobile=[1,2,3], returnRotMat=False):\n",
    "    \"\"\"rotate points into the z-plane for trilaterization. Target point ensures that point is positive in Z\"\"\"\n",
    "    tp = target_point #target point\n",
    "    zplanexform = xform_to_z_plane(endpoint_midpoints,index_mobile=index_mobile) #one index start base triangle, default\n",
    "    #add one for npose rot calc\n",
    "    npose = np.concatenate((endpoint_midpoints, np.ones((endpoint_midpoints.shape[0],\n",
    "                                                         endpoint_midpoints.shape[1],1))),axis=2) \n",
    "    rot = xform_npose_2batch(zplanexform,npose) # double batch matrix multiplication, see npose, for one batch\n",
    "\n",
    "    #translate X domain to place first index of \"index_mobile\" to 0,0,0\n",
    "    rot[:,:,0] = rot[:,:,0]-np.expand_dims(rot[:,index_mobile[0],0],axis=1)\n",
    "    #based on target point guaranteed to be positive\n",
    "    #reflect new points across the z axis to positive if negative to match just choosing positive solutions\n",
    "    rot[...,2][rot[:,tp,2]<0] = -rot[...,2][rot[:,tp,2]<0]\n",
    "    \n",
    "    if not returnRotMat:\n",
    "        return rot[...,:3] #remove npose rotate dimension\n",
    "    else:\n",
    "        return rot[...,:3], zplanexform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#methods to index needed indices from generator\n",
    "\n",
    "def helix_dindex(helices_to_keep, num_helices=4, intraHelixDist=True):\n",
    "    \"\"\"Get index values for parts of the distance map\"\"\"\n",
    "    \n",
    "    #prep indices for distance map\n",
    "    num_ep = num_helices*2\n",
    "    mat_ind = np.array(range((num_ep)**2)).reshape((num_ep,num_ep))\n",
    "    iu1 = np.triu_indices(num_ep, 1)\n",
    "    \n",
    "    helix_used = np.array(helices_to_keep,dtype=int)\n",
    "    \n",
    "    h_ep = np.array(range(num_ep)).reshape((-1,2)) #generate helix to endpoint mapping\n",
    "    \n",
    "    tot_ind = []\n",
    "    \n",
    "    if intraHelixDist:\n",
    "        #get indices of distance map that correspond to each helix, overlap is distances between specified endpoints\n",
    "        for x in helix_used:\n",
    "            new_ind = np.intersect1d(mat_ind[h_ep[x]], mat_ind.T[h_ep[x]])\n",
    "            tot_ind.extend(new_ind)\n",
    "    \n",
    "    \n",
    "    for x in permutations(helix_used,2):\n",
    "        new_ind = np.intersect1d(mat_ind[h_ep[x[0]]], mat_ind.T[h_ep[x[1]]])\n",
    "        tot_ind.extend(new_ind)\n",
    "    \n",
    "    #convert to generator indices (indices of iu1 array)\n",
    "    out_ind = []\n",
    "    for x in tot_ind:\n",
    "        if len(np.nonzero(mat_ind[iu1]==x)[0])>0:\n",
    "            out_ind.append(np.nonzero(mat_ind[iu1]==x))\n",
    "\n",
    "    return np.sort(np.array(out_ind).flatten())\n",
    "\n",
    "\n",
    "def point_dindex(target_points, ref=[4], num_helices = 4):\n",
    "    \n",
    "    num_ep = num_helices*2\n",
    "    mat_ind = np.array(range((num_ep)**2)).reshape((num_ep,num_ep))\n",
    "    iu1 = np.triu_indices(num_ep, 1)\n",
    "    \n",
    "    dindex = []\n",
    "    \n",
    "    for tp in target_points:\n",
    "        for ref_ind in ref:\n",
    "            dindex.append(mat_ind[ref_ind,tp]) #indices for distances to target point\n",
    "    \n",
    "    dindex = np.array(dindex)\n",
    "    \n",
    "    out_ind = []\n",
    "    for x in dindex.flatten():\n",
    "        out_ind.append(np.nonzero(mat_ind[iu1]==x))\n",
    "        \n",
    "    out_ind = np.array(out_ind)\n",
    "    \n",
    "    return out_ind.reshape((len(target_points),-1))\n",
    "\n",
    "def target_dindex(target_points, oneRef = True, num_helices = 5, baseTri_out=True):\n",
    "    \"\"\"Distance map indices for base triangle and output distance map\"\"\"\n",
    "    \n",
    "    num_ep = num_helices*2\n",
    "    mat_ind = np.array(range((num_ep)**2)).reshape((num_ep,num_ep))\n",
    "    iu1 = np.triu_indices(num_ep, 1)\n",
    "\n",
    "    if oneRef:\n",
    "        ref = [1,2,3]\n",
    "        base_tri = [mat_ind[1][2],mat_ind[2][3],mat_ind[1][3]] #p1 to p2, p2 to p3, p1 to p3\n",
    "        \n",
    "    else:\n",
    "        ref = [0,1,2]\n",
    "        base_tri = [mat_ind[0][1],mat_ind[1][2],mat_ind[0][3]] #p0 to p1, p1 to p2, p0 to p3\n",
    "    \n",
    "    dindex = []\n",
    "    \n",
    "    for tp in target_points:\n",
    "        dindex.append(mat_ind[ref,tp]) #indices for distances to target point\n",
    "    \n",
    "    dindex = np.array(dindex)\n",
    "    \n",
    "    out_ind = []\n",
    "    for x in dindex.flatten():\n",
    "        out_ind.append(np.nonzero(mat_ind[iu1]==x))\n",
    "        \n",
    "    out_ind = np.array(out_ind)\n",
    "    \n",
    "    return out_ind.reshape((-1,len(base_tri))),base_tri\n",
    "\n",
    "def minMax_indices(distance_index, point_index, minmax_obj):\n",
    "    \n",
    "    #assemble conversions \n",
    "    #converts output from generator back to real distances\n",
    "    dMin_all = tf.convert_to_tensor(minmax_obj.data_min_, dtype=tf.float32)\n",
    "    mScale_all = tf.convert_to_tensor(minmax_obj.scale_, dtype = tf.float32)\n",
    "    mMin = tf.convert_to_tensor(minmax_obj.feature_range[0], dtype = tf.float32)\n",
    "\n",
    "    #index just the distances we need for calculation\n",
    "    dMin = tf.gather(dMin_all, distance_index,axis=0)\n",
    "    mScale = tf.gather(mScale_all, distance_index,axis=0)\n",
    "\n",
    "    #indexes we need to determine the +/- z value of the new points\n",
    "    pindex = point_dindex([5,6,7], ref=[4], num_helices = 4)\n",
    "    dMin_nwp = tf.gather(dMin_all, point_index,axis=0)\n",
    "    mScale_nwp = tf.gather(mScale_all, point_index,axis=0)\n",
    "    \n",
    "    return dMin, mScale, mMin, dMin_nwp,  mScale_nwp \n",
    "\n",
    "def ref_distmap_index(distances, num_helices = 4):\n",
    "    \n",
    "    num_ep = num_helices*2\n",
    "    mat_ind = np.array(range((num_ep)**2)).reshape((num_ep,num_ep))\n",
    "    iu1 = np.triu_indices(num_ep, 1)\n",
    "    iu1_flat = iu1[0]*num_ep+iu1[1]\n",
    "    \n",
    "    return distances[np.ix_(range(distances.shape[0]),iu1_flat)]\n",
    "\n",
    "def convert_dMat_to_iu1_index(indices_in, num_helices = 4):\n",
    "    \"\"\"Converts indices on flattened distance index to iu1 single indices\"\"\"\n",
    "    \n",
    "    \n",
    "    conv_array = np.array(indices_in).flatten()\n",
    "    \n",
    "    num_ep = num_helices*2\n",
    "    mat_ind = np.array(range((num_ep)**2)).reshape((num_ep,num_ep))\n",
    "    iu1 = np.triu_indices(num_ep, 1)\n",
    "    \n",
    "    #convert to generator indices (indices of iu1 array)\n",
    "    out_ind = []\n",
    "    for x in conv_array:\n",
    "        if len(np.nonzero(mat_ind[iu1]==x)[0])>0:\n",
    "            out_ind.append(np.nonzero(mat_ind[iu1]==x))\n",
    "            \n",
    "    out_ind = np.array(out_ind)\n",
    "        \n",
    "    return out_ind.reshape(conv_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_base_triangle_trilateriation(dindex, base_tri, distance_map):\n",
    "    \"\"\"Return x,y,z coords on z-plane of base triangle of tetrahedron from a distance map.\"\"\"\n",
    "    \n",
    "#     dindex, base_tri = target_dindex(targ_dind, oneRef = oneRef, num_helices = num_helices)\n",
    "# #     print(dindex)\n",
    "# #     print(base_tri)\n",
    "    \n",
    "    #test case input data: prep base triangles for trilateration at zplane, (0,0,0) (dvar,0,0) (ivar,jvar,0)\n",
    "    desired_dm = distance_map[:, base_tri] #base tri from dindex\n",
    "\n",
    "    dvar_index = tf.convert_to_tensor(0 ,dtype=tf.int32)\n",
    "    s2_index = tf.convert_to_tensor(2 ,dtype=tf.int32) # we would like the angle across from side 2\n",
    "    s3_index = tf.convert_to_tensor(1 ,dtype=tf.int32)\n",
    "\n",
    "    #x value representing center of 2nd sphere at (dvar,0,0) aka s1\n",
    "    dvar = tf.reshape(tf.gather(desired_dm, dvar_index,axis=1),(-1,1)) #side 1\n",
    "    s2 = tf.reshape(tf.gather(desired_dm,   s2_index,axis=1),(-1,1))\n",
    "    s3 = tf.reshape(tf.gather(desired_dm,   s3_index,axis=1),(-1,1))\n",
    "\n",
    "    #calculate the opposite angle of the the third side of base triangle using law of cosines\n",
    "    s1sq = tf.square(dvar)\n",
    "    s2sq = tf.square(s2)\n",
    "    s3sq = tf.square(s3)\n",
    "    ang3 = np.arccos((-s3sq+s2sq+s1sq)/(2*dvar*s2))\n",
    "\n",
    "    #take third point of base triangle via distance * vector\n",
    "    v13 = tf.concat([tf.cos(ang3), tf.sin(ang3), tf.zeros_like(ang3)], axis=1)\n",
    "    p3 = s2*v13\n",
    "    #center points of 3rd sphere\n",
    "    ivar = tf.reshape(p3[:,0],(-1,1))\n",
    "    jvar = tf.reshape(p3[:,1],(-1,1))\n",
    "\n",
    "\n",
    "    #convert all to float32 to match generator output\n",
    "    #expand to dindex size \n",
    "\n",
    "    dvar = tf.cast(tf.repeat(dvar,dindex.shape[0],axis=1),dtype=tf.float32)\n",
    "    ivar = tf.cast(tf.repeat(ivar,dindex.shape[0],axis=1),dtype=tf.float32)\n",
    "    jvar = tf.cast(tf.repeat(jvar,dindex.shape[0],axis=1),dtype=tf.float32)\n",
    "    \n",
    "    return dvar, ivar, jvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buttress_via_mask_only(gen_obj, ref_map, target_mp, batch_size=32,cycles=100, input_z=None, \n",
    "                          rate=0.05, target_ep=[4,5,6,7], num_helices=4, oneRef=True,\n",
    "                          scale=5.0, z_size=12):\n",
    "    \n",
    "    \n",
    "    batch = batch_size*ref_map.shape[0]\n",
    "    ref_map = np.repeat(ref_map, batch_size, axis=0)\n",
    "    \n",
    "    #input to generator (determinstic output)\n",
    "    if input_z is None:\n",
    "        input_z = tf.random.uniform(shape=(batch, z_size), minval=-1, maxval=1)\n",
    "        \n",
    "    input_z_var = tf.Variable(input_z)\n",
    "    \n",
    "    #establish indices for distances to reference\n",
    "    #prep base triangle, convert distances from minmax to regular\n",
    "    dindex, base_tri = target_dindex(target_ep, oneRef = True, num_helices = num_helices)\n",
    "    base_tri = convert_dMat_to_iu1_index(base_tri) #dirty\n",
    "    pindex = point_dindex(target_ep[1:], ref=[target_ep[0]], num_helices = num_helices)\n",
    "\n",
    "    #convert generator output to 'real distances'\n",
    "    #dMin, mScale, mMin, dMin_nwp,  mScale_nwp = minMax_indices(dindex, pindex, brec.mm)\n",
    "    mmTuple = minMax_indices(dindex, pindex, gen_obj.mm)\n",
    "\n",
    "    # prepare base triangle for trilateriation (z plane , p1 at origin, p2 positive x)\n",
    "    #dvar, ivar, jvar = prep_base_triangle_trilateriation(dist[:batch], targ_dind = [4,5,6,7], oneRef = True, num_helices=4)\n",
    "    #baseTuple = prep_base_triangle_trilateriation(dist[:batch], targ_dind = target_ep, oneRef = True, num_helices=num_helices)\n",
    "\n",
    "    baseTuple = prep_base_triangle_trilateriation(dindex, base_tri, ref_map)\n",
    "    \n",
    "    \n",
    "    #mask for keeping buttress helices in same orientation\n",
    "    h_index = helix_dindex([0,1], num_helices=4, intraHelixDist=True)\n",
    "    helix_keep_mask = np.zeros((ref_map.shape[1],),dtype=np.int32)\n",
    "    helix_keep_mask[h_index] = 1\n",
    "    helix_keep_mask = tf.convert_to_tensor(helix_keep_mask,dtype=tf.float32)\n",
    "    \n",
    "    #store grads and inputs as we backpropagate\n",
    "    z=[]\n",
    "    grads = []\n",
    "\n",
    "    scale = tf.constant(scale)\n",
    "    ref_map_conv = gen_obj.mm.transform(ref_map)\n",
    "    ref_map_conv = tf.convert_to_tensor(ref_map_conv,dtype=tf.float32)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=rate)\n",
    "\n",
    "    g_o = gen_obj.g(input_z_var)\n",
    "    masked_loss = maskLoss(ref_map_conv, g_o, helix_keep_mask)\n",
    "    mp_loss  = tf.divide(midpoints_loss(g_o, target_mp, mmTuple, baseTuple, dindex, pindex, batch), scale)\n",
    "    loss = tf.reduce_sum(mp_loss,axis=1) + tf.reduce_sum(masked_loss,axis=1)\n",
    "\n",
    "    print('start_masked',np.round(np.sum(masked_loss),2))\n",
    "    print('start_mp',np.round(np.sum(mp_loss),2))\n",
    "\n",
    "\n",
    "    for t in range(1,cycles):\n",
    "\n",
    "        #compute Loss\n",
    "        with tf.GradientTape() as g_tape:\n",
    "            g_tape.watch(input_z_var)\n",
    "            g_o = gen_obj.g(input_z_var)\n",
    "            masked_loss = maskLoss(ref_map_conv, g_o, helix_keep_mask)\n",
    "            mp_loss  = tf.divide(midpoints_loss(g_o, target_mp, mmTuple, baseTuple, dindex, pindex, batch),scale)\n",
    "\n",
    "#             loss = tf.reduce_sum(mp_loss,axis=1) + tf.reduce_sum(masked_loss,axis=1)\n",
    "\n",
    "        g_grads = g_tape.gradient(masked_loss, input_z_var)\n",
    "\n",
    "        optimizer.apply_gradients(zip([g_grads],[input_z_var]))\n",
    "        \n",
    "        z.append(tf.identity(input_z_var))\n",
    "\n",
    "    print('end_masked', np.round(np.sum(masked_loss),2))\n",
    "    print('end_mp', np.round(np.sum(mp_loss),2))\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fullBUTT_GPU(gen_obj, ref_map, target_mp, batch_size=32,cycles=100, input_z=None, \n",
    "                          rate=0.05, target_ep=[4,5,6,7], num_helices=4, oneRef=True,\n",
    "                          scale=5.0, z_size=12):\n",
    "    \n",
    "    batch_indices = np.repeat(np.array(range(ref_map.shape[0])),batch_size)\n",
    "    batch = batch_indices.shape[0]\n",
    "    ref_map = np.repeat(ref_map, batch_size, axis=0)\n",
    "    \n",
    "    #establish indices for distances to reference\n",
    "    #prep base triangle, convert distances from minmax to regular\n",
    "    dindex, base_tri = target_dindex(target_ep, oneRef = True, num_helices = num_helices)\n",
    "    base_tri = convert_dMat_to_iu1_index(base_tri) #dirty\n",
    "    pindex = point_dindex(target_ep[1:], ref=[target_ep[0]], num_helices = num_helices)\n",
    "\n",
    "    #convert generator output to 'real distances'\n",
    "    #dMin, mScale, mMin, dMin_nwp,  mScale_nwp = minMax_indices(dindex, pindex, brec.mm)\n",
    "    mmTuple = minMax_indices(dindex, pindex, gen_obj.mm)\n",
    "\n",
    "    # prepare base triangle for trilateriation (z plane , p1 at origin, p2 positive x)\n",
    "    #dvar, ivar, jvar = prep_base_triangle_trilateriation(dist[:batch], targ_dind = [4,5,6,7], oneRef = True, num_helices=4)\n",
    "    #baseTuple = prep_base_triangle_trilateriation(dist[:batch], targ_dind = target_ep, oneRef = True, num_helices=num_helices)\n",
    "\n",
    "    baseTuple = prep_base_triangle_trilateriation(dindex, base_tri, ref_map)\n",
    "    \n",
    "    dMin, mScale, mMin, dMin_nwp,  mScale_nwp = mmTuple\n",
    "    dvar,ivar,jvar = baseTuple\n",
    "    \n",
    "    #mask for keeping buttress helices in same orientation\n",
    "    h_index = helix_dindex([0,1], num_helices=4, intraHelixDist=True)\n",
    "    helix_keep_mask = np.zeros((ref_map.shape[1],),dtype=np.int32)\n",
    "    helix_keep_mask[h_index] = 1\n",
    "    helix_keep_mask = tf.convert_to_tensor(helix_keep_mask,dtype=tf.float32)\n",
    "    \n",
    "    @tf.function\n",
    "    def maskLoss(y_actual, y_pred,mask):\n",
    "        \"\"\"Loss Function for mantaing shape of input helices\"\"\"\n",
    "        custom_loss_val = tf.multiply(mask,tf.square(y_actual-y_pred))\n",
    "        return custom_loss_val\n",
    "\n",
    "    @tf.function\n",
    "    def midpoints_loss(g1, target, \n",
    "                       dvar, ivar, jvar,\n",
    "                       dMin, mScale, mMin, dMin_nwp, mScale_nwp, \n",
    "                       dindex, pindex, batch_size, zr_ind, z_reflect_tensor):\n",
    "        \"\"\"Loss function to move output of two generated helices to target midpoint\"\"\"\n",
    "\n",
    "\n",
    "        #now using dindex gather the desired indices for tetrahedron calcs\n",
    "\n",
    "        #radius of the spheres, aka the distances to unmasked endpoints\n",
    "        g2 = tf.gather(g1,dindex,axis=1)\n",
    "\n",
    "        #see https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler\n",
    "        #inspect .scale_\n",
    "        conv_dist = tf.add(tf.divide(tf.subtract(g2, mMin), mScale),dMin)\n",
    "        #transpose lets you easily grab all distances with gather/axis \n",
    "        conv_dist_squared = tf.transpose(tf.square(conv_dist),perm=[0, 2, 1]) \n",
    "\n",
    "        r1_sq = tf.gather(conv_dist_squared, 0, axis=1) \n",
    "        r2_sq =  tf.gather(conv_dist_squared,1, axis=1) \n",
    "        r3_sq = tf.gather(conv_dist_squared, 2, axis=1)\n",
    "\n",
    "        #calculate coordinates of spherial intersect\n",
    "        x = tf.divide(tf.add(tf.subtract(r1_sq,r2_sq),tf.square(dvar)),tf.multiply(2.0,dvar))\n",
    "        y1 = tf.divide(tf.add(tf.add(tf.subtract(r1_sq,r3_sq), tf.square(ivar)), tf.square(jvar)),tf.multiply(2.0,jvar))\n",
    "        y = tf.subtract(y1,tf.multiply(tf.divide(ivar,jvar),x))\n",
    "\n",
    "        pre_z = tf.subtract(tf.subtract(r1_sq,tf.square(x)),tf.square(y))\n",
    "        fixed_z = tf.clip_by_value(pre_z, 1e-10, 100)\n",
    "\n",
    "        #adds  to negative values to 0 for sqrt,\n",
    "        #I think is okay as zero z will imply lengthening of distances to match a non-zero target midpoint,\n",
    "        #pushing the network in the desired direction?\n",
    "\n",
    "        z = tf.sqrt(fixed_z) #assume positive solution\n",
    "        z_neg = tf.multiply(z,-1) #assume negative soluation\n",
    "\n",
    "        #new points, with both assumptions\n",
    "        nwp = tf.concat((tf.reshape(x,(batch_size,-1,1)),\n",
    "                        tf.reshape(y,(batch_size,-1,1)),\n",
    "                        tf.reshape(z,(batch_size,-1,1))), axis=2)  #\n",
    "\n",
    "        nwp_negz = tf.concat((tf.reshape(x,(batch_size,-1,1)),\n",
    "                        tf.reshape(y,(batch_size,-1,1)),\n",
    "                        tf.reshape(z_neg,(batch_size,-1,1))), axis=2)  #\n",
    "\n",
    "        #some positive solutions assumptions,\n",
    "        # assume first [i4] is actual positive use remaining distances of i4 to (i5,i6,i7) to determine z sign\n",
    "        # closest to matching distance is used\n",
    "\n",
    "\n",
    "        #let's start by calculating all i4 to (i5,i6,i7) distances\n",
    "\n",
    "        #stop the gradients since these are used to index gather and scatter\n",
    "        #unsqueeze at two different dimensionsq to broadcast into matrix MX1 by 1XN to MXN \n",
    "        nwp_p =  tf.stop_gradient(tf.expand_dims(nwp,axis=1) - tf.expand_dims(nwp,axis=2))\n",
    "        nwp_n =  tf.stop_gradient(tf.expand_dims(nwp,axis=1) - tf.expand_dims(nwp_negz,axis=2))\n",
    "\n",
    "        nwp_dist_pz = tf.reshape(tf.sqrt(tf.reduce_sum(tf.square(nwp_p), 3)),(-1,4,4)) #distance calc +1e6?\n",
    "        nwp_dist_nz = tf.reshape(tf.sqrt(tf.reduce_sum(tf.square(nwp_n), 3)),(-1,4,4))  #distance calc\n",
    "\n",
    "        z_pn_dist_pre_con = tf.gather(g1,pindex,axis=1)\n",
    "        z_pn_dist = tf.add(tf.divide(tf.subtract(z_pn_dist_pre_con, mMin), mScale_nwp),dMin_nwp)\n",
    "\n",
    "        #index p4 to p5,p6,p7\n",
    "        #rewrite as non-slice version of this\n",
    "\n",
    "        nwp_dist_pz_c = tf.squeeze(tf.gather(tf.gather(nwp_dist_pz, [0], axis=1), [1,2,3], axis=2))\n",
    "        nwp_dist_nz_c = tf.squeeze(tf.gather(tf.gather(nwp_dist_nz, [0], axis=1), [1,2,3], axis=2))\n",
    "\n",
    "        nwp_dist_pz_c = tf.expand_dims(nwp_dist_pz_c,axis=2)\n",
    "        nwp_dist_nz_c = tf.expand_dims(nwp_dist_nz_c,axis=2)\n",
    "        \n",
    "        \n",
    "\n",
    "        # #using a single distance decide the z assumption and apply\n",
    "        correct_z_assum = tf.abs(z_pn_dist - nwp_dist_nz_c) < tf.abs(z_pn_dist - nwp_dist_pz_c)\n",
    "        cz = tf.squeeze(tf.multiply(tf.cast(correct_z_assum,tf.int32),-2))\n",
    "        \n",
    "        z_reflect_tensor = tf.ones_like(nwp, dtype=tf.int32)\n",
    "        \n",
    "        nwp_mult = tf.cast(tf.tensor_scatter_nd_add(z_reflect_tensor, z_reflect_ind, cz),dtype=tf.float32)\n",
    "        nwp_final = tf.multiply(nwp_mult,nwp)\n",
    "\n",
    "        midpoint = tf.reduce_mean(nwp_final,axis=1)\n",
    "        return tf.square(tf.subtract(midpoint,target)) # means squared loss to desired midpoint\n",
    "    \n",
    "\n",
    "        #input to generator (determinstic output)\n",
    "    \n",
    "    ref_map_conv = gen_obj.mm.transform(ref_map)\n",
    "    #controlling z reflection during trilaterization\n",
    "    z_r_innerInd = np.repeat(tf.convert_to_tensor([[[1,2],[2,2],[3,2]]]),batch,axis=0)\n",
    "    #batch index\n",
    "    zfi_bi =np.expand_dims(np.array(range(batch)).reshape((-1,1)).repeat(3,axis=1),axis=2)\n",
    "    z_reflect_ind = np.concatenate((zfi_bi,z_r_innerInd),axis=2)\n",
    "    \n",
    "    \n",
    "    if input_z is None:\n",
    "        input_z = tf.random.uniform(shape=(batch, z_size), minval=-1, maxval=1)\n",
    "    \n",
    "    \n",
    "    with tf.device(device_name):\n",
    "        input_z_var = tf.Variable(input_z)\n",
    "        ref_map_ = tf.Variable(ref_map_conv,dtype=tf.float32)\n",
    "        scale_, z_reflect_ind_ = tf.constant(scale), tf.constant(z_r_innerInd)\n",
    "        target_mp_, batch_ = tf.constant(target_mp),  tf.constant(batch)\n",
    "        dindex_, pindex_ = tf.constant(dindex), tf.constant(pindex)\n",
    "        dMin_, mScale_, mMin_ = tf.constant(dMin), tf.constant(mScale), tf.constant(mMin),\n",
    "        dMin_nwp_,  mScale_nwp_ =  tf.constant(dMin_nwp),  tf.constant(mScale_nwp)\n",
    "        dvar_, ivar_, jvar_ = tf.constant(dvar), tf.constant(ivar), tf.constant(jvar)\n",
    "        helix_keep_mask_ = tf.convert_to_tensor(helix_keep_mask, dtype=tf.float32)\n",
    "        z_reflect_tensor_ = tf.ones((batch,len(target_ep),3), dtype=tf.int32) #final tensor\n",
    "        \n",
    "\n",
    "    #store grads and inputs as we backpropagate\n",
    "    z=[]\n",
    "    loss_mask = []\n",
    "    loss_mp = []\n",
    "    grads = []\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=rate)\n",
    "\n",
    "    g_o = gen_obj.g(input_z_var)\n",
    "    masked_loss = maskLoss(ref_map_, g_o, helix_keep_mask_)\n",
    "    \n",
    "    mp_loss  = tf.divide(midpoints_loss(g_o, target_mp_, \n",
    "                        dvar_, ivar_, jvar_,\n",
    "                        dMin_, mScale_, mMin_, dMin_nwp_, mScale_nwp_, \n",
    "                        dindex_, pindex_, batch_,z_reflect_ind_, z_reflect_tensor_), scale_)\n",
    "    print('start_masked',np.round(np.sum(masked_loss),2))\n",
    "    print('start_mp',np.round(np.sum(mp_loss),2))\n",
    "    \n",
    "    for t in range(1,cycles):\n",
    "\n",
    "#         #compute Loss\n",
    "        with tf.GradientTape() as g_tape:\n",
    "            g_tape.watch(input_z_var)\n",
    "            g_o = gen_obj.g(input_z_var)\n",
    "            masked_loss = maskLoss(ref_map_, g_o, helix_keep_mask_)\n",
    "            mp_loss  = tf.divide(midpoints_loss(g_o, target_mp_, \n",
    "                        dvar_, ivar_, jvar_,\n",
    "                        dMin_, mScale_, mMin_, dMin_nwp_, mScale_nwp_, \n",
    "                        dindex_, pindex_, batch_,z_reflect_ind_, z_reflect_tensor_ ), scale_)\n",
    "\n",
    "            loss = tf.reduce_sum(mp_loss,axis=1) + tf.reduce_sum(masked_loss,axis=1)\n",
    "\n",
    "        g_grads = g_tape.gradient(loss, input_z_var)\n",
    "\n",
    "        optimizer.apply_gradients(zip([g_grads],[input_z_var]))\n",
    "        \n",
    "        z.append(tf.identity(input_z_var).numpy())\n",
    "        loss_mask.append(tf.identity(masked_loss).numpy())\n",
    "        loss_mp.append(tf.identity(mp_loss).numpy())\n",
    "\n",
    "    print('end_masked', np.round(np.sum(masked_loss),2))\n",
    "    print('end_mp', np.round(np.sum(mp_loss),2))\n",
    "    \n",
    "    return z, loss_mask, loss_mp, batch_indices\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buttress_ep_from_z(gen_obj, gen_z, starting_ep , loss_midpoint, loss_masked, batchIndices,\n",
    "                       max_loss_mp = 0.001, max_loss_mask = 0.001):\n",
    "    \n",
    "    \n",
    "    best_mp = np.sum(loss_midpoint<max_loss_mp,axis=1)>2 # 3 total mp loss outputs (x,y,z of midpoint to target)\n",
    "    best_mask = np.sum(loss_masked<max_loss_mask,axis=1)>27 # 28 total mask loss point (2 helices)\n",
    "\n",
    "    mask_mp_bool = np.logical_and(best_mp, best_mask)       \n",
    "\n",
    "    identified_z = gen_z[mask_mp_bool]\n",
    "    print(f'Outputs passing filters: {len(identified_z)}')\n",
    "    print(f'Total Outputs: {len(gen_z)}')\n",
    "    uInd = batchIndices[mask_mp_bool]\n",
    "    \n",
    "    orig_ep = starting_ep[uInd]\n",
    "    \n",
    "    \n",
    "    gen_obj.generate(z=12, input_z = identified_z, batch_size=identified_z.shape[0])\n",
    "    gen_obj.MDS_reconstruct_()\n",
    "    \n",
    "    out_ep = np.array(gen_obj.reconsMDS_)\n",
    "    \n",
    "    return align_generated_to_starting_ep(out_ep, orig_ep)\n",
    "\n",
    "def buttress_ep_from_z_mask_only(gen_obj, gen_z, starting_ep ,loss_masked, batchIndices, max_loss_mask = 0.002, max_out=100,\n",
    "                                 print_stats= False):\n",
    "    \n",
    "    \n",
    "    sm = np.sum(loss_masked,axis=1)\n",
    "    smi = np.argsort(sm)\n",
    "    sm_sort = sm[smi]\n",
    "    best_mask = sm_sort < max_loss_mask\n",
    "    \n",
    "    uInd = batchIndices[smi][best_mask]\n",
    "    \n",
    "    if print_stats:\n",
    "        print('Input Size: ',      len(sm))\n",
    "        print('Passing Filters: ', len(uInd))\n",
    "    \n",
    "    \n",
    "    if len(uInd)>max_out:\n",
    "        uInd = uInd[:max_out]\n",
    "    \n",
    "    identified_z = gen_z[uInd]\n",
    "    orig_ep = starting_ep[uInd]\n",
    "    \n",
    "    gen_obj.generate(z=12, input_z = identified_z, batch_size=identified_z.shape[0])\n",
    "    gen_obj.MDS_reconstruct_()\n",
    "    \n",
    "    out_ep = np.array(gen_obj.reconsMDS_)\n",
    "    if len(out_ep>0):\n",
    "        return align_generated_to_starting_ep(out_ep, orig_ep), uInd\n",
    "    else:\n",
    "        return [], []\n",
    "    \n",
    "    \n",
    "def align_generated_to_starting_ep(gen_ep, orig_ep):\n",
    "    \"\"\"Uses Kabsh to align generated endpoints onto original endpoints. Orig_Ep on origin with oneRef.\"\"\"\n",
    "    #Thanks to below for this code; modified to batch form\n",
    "    #moves gen_\n",
    "    #https://pymolwiki.org/index.php/Kabsch\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    #only center on first four points [first two helices]\n",
    "    gen_ep_4 = gen_ep[:,:4,:].copy()\n",
    "    orig_ep_4 = orig_ep[:,:4,:].copy()\n",
    "    \n",
    "    \n",
    "    #centering to prevent affine transformaiton\n",
    "    COM_orig = np.expand_dims(np.sum(orig_ep_4, axis=1)/orig_ep_4.shape[1]   ,axis=1)\n",
    "    COM_gen =  np.expand_dims(np.sum(gen_ep_4,  axis=1)/gen_ep_4.shape[1]     ,axis=1)\n",
    "    \n",
    "    gen_ep_4 = gen_ep_4 - COM_gen\n",
    "    orig_ep_4 = orig_ep_4 - COM_orig\n",
    "    \n",
    "    assert len(gen_ep_4) == len(orig_ep_4)\n",
    "    L = len(gen_ep_4)\n",
    "    assert L > 0\n",
    "    \n",
    "    #initial error estimate\n",
    "    E0 = np.sum( np.sum(np.square(gen_ep_4),axis=1),axis=1) + np.sum( np.sum(np.square(orig_ep_4),axis=1),axis=1)\n",
    "\n",
    "    # This beautiful step provides the answer.  V and Wt are the orthonormal\n",
    "    # bases that when multiplied by each other give us the rotation matrix, U.\n",
    "    # S, (Sigma, from SVD) provides us with the error!  Isn't SVD great!                                            #2                      #1\n",
    "    V, S, Wt = np.linalg.svd( np.matmul(np.transpose(gen_ep_4, axes=[0,2,1]), orig_ep_4))\n",
    "\n",
    "    # we already have our solution, in the results from SVD.\n",
    "    # we just need to check for reflections and then produce\n",
    "    # the rotation.  V and Wt are orthonormal, so their det's\n",
    "    # are +/-1.\n",
    "    reflect = np.linalg.det(V) * np.linalg.det(Wt)\n",
    "    #original solution, I will take both reflections\n",
    "    #multiples by 1 or -1 depending if relfect is negative (reflection)\n",
    "    proper_reflection = ((reflect>0).astype(np.int32)*-2+1) \n",
    "    S[:,-1] = S[:,-1]*proper_reflection\n",
    "    V[:,:,-1] = -V[:,:,-1]*proper_reflection.reshape((-1,1))\n",
    "\n",
    "    V_reflect = V.copy()\n",
    "    V_reflect[:,:,-1] = -V_reflect[:,:,-1]\n",
    "    #Error\n",
    "    RMSD = E0 - (2.0 * np.sum(S,axis=1))\n",
    "    RMSD = np.sqrt(np.abs(RMSD / L))\n",
    "    \n",
    "    #generate rotation matrices\n",
    "    U = np.matmul(V, Wt)\n",
    "    U_reflect = np.matmul(V_reflect, Wt)\n",
    "    \n",
    "    final_ep_full         = np.matmul((gen_ep-COM_gen), U) + COM_orig\n",
    "    final_ep_full_reflect = np.matmul((gen_ep-COM_gen), U_reflect) + COM_orig\n",
    "    \n",
    "    return final_ep_full, final_ep_full_reflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep endpoint dataset for use with easy use with Trilateration:\n",
    "#Essentially identify 3 points on 1st two helices (Rotate/Translate to Z-plane) with \n",
    "#index mobile 1 at 0,0,0, target point in the positive z (trilateration assumtion)\n",
    "#roughly 10% of z values of helices 3/4 are in the negative feild, with point enforced 4 positive\n",
    "\n",
    "#distance map of ep dataset\n",
    "#unsqueeze at two different dimensionsq to broadcast into matrix MX1 by 1XN to MXN \n",
    "dX = np.expand_dims(X,axis=1) - np.expand_dims(X,axis=2)\n",
    "dist = np.sqrt(np.sum(dX**2, 3))  #+ 1e-6) #this dataset is good \n",
    "dist = dist.reshape((dist.shape[0],-1))\n",
    "\n",
    "mp_01 = get_midpoint(X,helices_desired=[0,1])\n",
    "mp_23 = get_midpoint(X,helices_desired=[2,3])\n",
    "# d= np.linalg.norm(mp_02-mp_01,axis=1)\n",
    "# sns.histplot(d)\n",
    "\n",
    "#mp distance map\n",
    "ep_mp = np.hstack((X.reshape((-1,24)),mp_01,mp_23)).reshape(-1,10,3) #helix12mp=8  helix34mp=9\n",
    "#unsqueeze at two different dimensionsq to broadcast into matrix MX1 by 1XN to MXN \n",
    "dep_mp = np.expand_dims(ep_mp,axis=1) - np.expand_dims(ep_mp,axis=2)\n",
    "dist_mp = np.sqrt(np.sum(dep_mp**2, 3))  #+ 1e-6) #this dataset is good \n",
    "dist_mp = dist_mp.reshape((dist_mp.shape[0],-1))\n",
    "\n",
    "\n",
    "zp_ep_mp = rotate_base_tri_Zplane(ep_mp,  target_point=4, index_mobile=[1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# #code for visualizing mp\n",
    "# mpVec = zp_ep_mp[:,9,:]-zp_ep_mp[:,8,:]\n",
    "# x1,x2 = zp_ep_mp[:,9,0], zp_ep_mp[:,8,0]\n",
    "# y1,y2 = zp_ep_mp[:,9,1], zp_ep_mp[:,8,1]\n",
    "# z1,z2 = zp_ep_mp[:,9,2], zp_ep_mp[:,8,2]\n",
    "\n",
    "# print(f' x1: {np.mean(x1)} y1: {np.mean(y1)} z1: {np.mean(z1)}' )\n",
    "# print(f' x2: {np.mean(x2)} y2: {np.mean(y2)} z2: {np.mean(z2)}' )\n",
    "\n",
    "# plt.figure(figsize = (10, 8))\n",
    "# plt_axes = plt.axes(projection = '3d')\n",
    "# plt_axes.scatter3D(x1, y1, z1, alpha=0.01)\n",
    "\n",
    "# #d= np.linalg.norm(,axis=1)\n",
    "# sns.histplot(zp_ep_mp[:,9,2])\n",
    "# sns.jointplot(x=mpVec[:,0], y=mpVec[:,1], kind = 'kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiate array to hold endpoints\n",
    "batch=100\n",
    "helix_pairs = 10 #even number to reshape later\n",
    "num_helices = 2\n",
    "mi = 0 #master index\n",
    "\n",
    "#random sample starting endpoints to buttress\n",
    "refi_all = list(range(ep_mp.shape[0]))\n",
    "ref_ind = np.array(random.sample(refi_all , batch))\n",
    "\n",
    "#master_ep contains endpoints in original coordinates space\n",
    "master_ep = ep_mp[ref_ind ,:4,...]\n",
    "#use mp to determine which reflection of the distance map to add, start by \n",
    "target_midpoint = tf.convert_to_tensor([7.0, 7.0, 9.0])\n",
    "curr_mp = ep_mp[ref_ind,8,...] - np.array([[0,0,8]])\n",
    "\n",
    "#starting concat special to orient points with original, helices to intialize directionality of addition (z refelect)\n",
    "#points unused except for this orientation step and maintaining distance maps indexing\n",
    "current_quad_prez = np.concatenate((ep_mp[ref_ind ,:4,...], ep_mp[ref_ind ,4:8,...]), axis=1)\n",
    "\n",
    "current_quad = rotate_base_tri_Zplane(current_quad_prez,  target_point=4, index_mobile=[1,2,3])\n",
    "\n",
    "start_dist = np.expand_dims(current_quad,axis=1) - np.expand_dims(current_quad,axis=2)\n",
    "dist = np.sqrt(np.sum(start_dist**2, 3))\n",
    "dist = dist.reshape((dist.shape[0],-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device name GPU\n"
     ]
    }
   ],
   "source": [
    "if tf.config.list_physical_devices('GPU'):\n",
    "    device_name = tf.test.gpu_device_name()\n",
    "else:\n",
    "    device_name = 'CPU'\n",
    "device_name = 'GPU' \n",
    "print(f'device name {device_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gen=\"data/BestGenerator\"\n",
    "\n",
    "with tf.device(device_name):\n",
    "    gen_obj = ge.BatchRecon(gen)\n",
    "output1=gen_obj.generate(z=12,batch_size=12) #example generator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#will need to add method to convert helix 1,2,3,4 to just helix 1,2 (remove 1,2 convert 3,4 to )\n",
    "ref_map_base = ref_distmap_index(dist, num_helices=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_masked 8482.68\n",
      "start_mp 11195.16\n",
      "end_masked 308.13\n",
      "end_mp 1123.76\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "#GPU ##33s  with 10*50000 start with 200 cycles\n",
    "#CPU ##39s\n",
    "output_z, loss_mask, loss_mp, batchInd=fullBUTT_GPU(gen_obj, ref_map_base , target_midpoint, batch_size=100,cycles=200, input_z=None, \n",
    "                          rate=0.05, target_ep=[4,5,6,7], num_helices=4, oneRef=True,\n",
    "                          scale=100.0, z_size=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Size:  10000\n",
      "Passing Filters:  1003\n"
     ]
    }
   ],
   "source": [
    "# final_ep = buttress_ep_from_z(gen_obj, output_z[-1], current_quad, loss_mp[-1], loss_mask[-1], batchInd, \n",
    "#                               max_loss_mp=0.05, max_loss_mask=0.001)\n",
    "\n",
    "(fa, fa_reflect), uInd =buttress_ep_from_z_mask_only(gen_obj, output_z[-1], current_quad_prez ,loss_mask[-1], batchInd, max_loss_mask = 0.0005,\n",
    "                                 print_stats= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the midpoint of the reflections\n",
    "fa_mp = get_midpoint(fa,helices_desired=[2,3],num_helices=4)\n",
    "fa_reflect_mp = get_midpoint(fa_reflect,helices_desired=[2,3],num_helices=4)\n",
    "\n",
    "final_ep = np.zeros_like(fa)\n",
    "\n",
    "measure1 = np.linalg.norm(fa_mp - curr_mp[uInd],axis=1)\n",
    "measure2 =  np.linalg.norm(fa_reflect_mp - curr_mp[uInd],axis=1)\n",
    "\n",
    "final_ep[np.nonzero((measure1<measure2))] = fa[np.nonzero((measure1<measure2))]\n",
    "final_ep[np.nonzero((measure1>measure2))] = fa_reflect[np.nonzero((measure1>measure2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_ep2 = np.concatenate((master_ep[uInd],final_ep[:,4:,:]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 8, 3)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_ep2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_ep2 = np.concatenate((master_ep[uInd],fa_reflect[:,4:,:]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(10):\n",
    "    hf.HelicalProtein.makePointPDB(master_ep2[x] ,f't{x}.pdb',outDirec='output/') \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#method to view points in pymol  \n",
    "hf.HelicalProtein.makePointPDB(current_quad_prez[uInd][0],f't1.pdb',outDirec='output/')\n",
    "hf.HelicalProtein.makePointPDB(final_aligned[0] ,f't2.pdb',outDirec='output/') \n",
    "hf.HelicalProtein.makePointPDB(fa_reflect[0] ,f't3.pdb',outDirec='output/') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate new ep onto quad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need just 4 ep per layer, aka 2 helix unit additions.\n",
    "#master_ep = np.zeros((batch,4,3))\n",
    "#seed with base two units, need distance map\n",
    "\n",
    "mi = 0\n",
    "master_ep = zp_ep_mp[ref_ind ,:4,...]\n",
    "\n",
    "\n",
    "#distances indexed from generator are for helix quads\n",
    "#though the unit here is technically helix pairs\n",
    "#zeros are padded to maintain consistency for distance indices functions\n",
    "current_quad = np.concatenate((master_ep,np.zeros_like(master_ep)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_ep = np.concatenate((master_ep[uInd], final_ep[:,4:,...]), axis=1) #add endpoints to buttressed lest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi=1\n",
    "current_quad = np.concatenate((master_ep[:,mi*4:],np.zeros_like(master_ep[:,mi*4:])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11.83,  4.  , -3.06],\n",
       "       [-7.85, -8.03, -8.82],\n",
       "       [-4.91, -8.82, -0.95],\n",
       "       [ 7.38, -5.19, 10.82],\n",
       "       [ 0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ]])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_quad[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -1.87,   6.71,  -6.55],\n",
       "       [-11.21,  -3.73,   1.3 ],\n",
       "       [ -4.28,  -0.96,   9.06],\n",
       "       [ 10.9 ,  16.01,  -1.81],\n",
       "       [ 11.83,   4.  ,  -3.06],\n",
       "       [ -7.85,  -8.03,  -8.82],\n",
       "       [ -4.91,  -8.82,  -0.95],\n",
       "       [  7.38,  -5.19,  10.82]])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_ep[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_zp_ep' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-711aa4c982ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m npose = np.concatenate((new_zp_ep, np.ones((new_zp_ep.shape[0],\n\u001b[0m\u001b[0;32m      2\u001b[0m                                             new_zp_ep.shape[1],1))),axis=2) # add rotation dimension\n\u001b[0;32m      3\u001b[0m \u001b[0minv_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxform_to_superimpose_nposes\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mnew_zp_ep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaster_ep\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muInd\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_mobile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mref_resnum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0morig_coord_new_ep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxform_npose_2batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minv_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnpose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# remove rotation dimension\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'new_zp_ep' is not defined"
     ]
    }
   ],
   "source": [
    "npose = np.concatenate((new_zp_ep, np.ones((new_zp_ep.shape[0],\n",
    "                                            new_zp_ep.shape[1],1))),axis=2) # add rotation dimension\n",
    "inv_x = xform_to_superimpose_nposes( new_zp_ep, master_ep[uInd], index_mobile=[1,2,3],ref_resnum=[1,2,3] )\n",
    "\n",
    "orig_coord_new_ep = xform_npose_2batch(inv_x,npose)[...,:3] # remove rotation dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9.12,   5.95,  -0.54],\n",
       "       [ -8.31,   7.91,   6.58],\n",
       "       [ -8.97,   3.65,  -2.58],\n",
       "       [ 11.79,   0.34, -16.05],\n",
       "       [ 10.87,  -4.44,  -6.8 ],\n",
       "       [ -1.04,   0.51,  13.15],\n",
       "       [ -1.27,  -9.1 ,  10.86],\n",
       "       [-12.19,  -4.83,  -4.63]])"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_coord_new_ep[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#method to view points in pymol  \n",
    "hf.HelicalProtein.makePointPDB(orig_coord_new_ep[0][:4],f't1.pdb',outDirec='output/')\n",
    "hf.HelicalProtein.makePointPDB(master_ep[uInd][0] ,f't2.pdb',outDirec='output/')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.83,  2.98,  0.86],\n",
       "       [10.43, 13.66, 15.2 ],\n",
       "       [15.56, 13.53, 13.64],\n",
       "       [14.86,  7.09, -9.64]])"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_ep[uInd][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7, 36, 47, 47, 36,  9, 24, 97, 36,  7, 44, 17,  7, 47, 57, 26, 36,\n",
       "       17,  9, 94, 97, 97, 36, 79, 63, 24, 57, 20, 17, 57, 52, 73, 58, 10,\n",
       "       43, 57, 29, 19])"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uInd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def xform_to_z_plane(mobile, index_mobile=[0,1,2]):\n",
    "    \"\"\"rotate points into the z-plane for trilaterization. needs additional translation/reflection\"\"\"\n",
    "\n",
    "    mobile_stub = get_stubs_from_points(mobile, index=index_mobile)\n",
    "    mobile_stub_inv = np.linalg.inv(mobile_stub)\n",
    "    \n",
    "    z_plane_ref = np.repeat(np.array([[[0,0,0],[1,0,0],[1,1,0]]]), mobile.shape[0],axis=0)\n",
    "\n",
    "    ref_stub = get_stubs_from_points(z_plane_ref, index=[0,1,2])\n",
    "\n",
    "    xform = ref_stub @ mobile_stub_inv\n",
    "\n",
    "    return xform\n",
    "\n",
    "\n",
    "def rotate_base_tri_Zplane(endpoint_midpoints, target_point=4, index_mobile=[1,2,3], returnRotMat=False):\n",
    "    \"\"\"rotate points into the z-plane for trilaterization. Target point ensures that point is positive in Z\"\"\"\n",
    "    tp = target_point #target point\n",
    "    zplanexform = xform_to_z_plane(endpoint_midpoints,index_mobile=index_mobile) #one index start base triangle, default\n",
    "    #add one for npose rot calc\n",
    "    npose = np.concatenate((endpoint_midpoints, np.ones((endpoint_midpoints.shape[0],\n",
    "                                                         endpoint_midpoints.shape[1],1))),axis=2) \n",
    "    rot = xform_npose_2batch(zplanexform,npose) # double batch matrix multiplication, see npose, for one batch\n",
    "\n",
    "    #translate X domain to place first index of \"index_mobile\" to 0,0,0\n",
    "    rot[:,:,0] = rot[:,:,0]-np.expand_dims(rot[:,index_mobile[0],0],axis=1)\n",
    "    #based on target point guaranteed to be positive\n",
    "    #reflect new points across the z axis to positive if negative to match just choosing positive solutions\n",
    "    rot[...,2][rot[:,tp,2]<0] = -rot[...,2][rot[:,tp,2]<0]\n",
    "    \n",
    "    if not returnRotMat:\n",
    "        return rot[...,:3] #remove npose rotate dimension\n",
    "    else:\n",
    "        return rot[...,:3], zplanexform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to view points in pymol  \n",
    "hf.HelicalProtein.makePointPDB(current_quad[uInd][0],f't1.pdb',outDirec='output/')\n",
    "hf.HelicalProtein.makePointPDB(new_zp_ep[0] ,f't2.pdb',outDirec='output/')    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fa_tfpy",
   "language": "python",
   "name": "fa_tfpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
