{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from math import cos,sin,tan,asin,acos,radians,sqrt,degrees,atan,atan2,copysign\n",
    "import numpy as np\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "import scipy\n",
    "from scipy.stats import norm\n",
    "import random\n",
    "import time\n",
    "import timeit\n",
    "import math\n",
    "import localization as lx\n",
    "import gzip\n",
    "\n",
    "import util.npose_util as nu\n",
    "import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "from sklearn.manifold import MDS\n",
    "import argparse\n",
    "from functools import partial\n",
    "from itertools import starmap,repeat,permutations\n",
    "\n",
    "from pymol import cmd, stored, selector\n",
    "\n",
    "import GenerateEndpoints as ge\n",
    "import HelixFit as hf\n",
    "import FitTransform as ft\n",
    "\n",
    "import seaborn as sns\n",
    "import util.RotationMethods as rm\n",
    "\n",
    "#reference helix for propogation\n",
    "zero_ih = nu.npose_from_file('util/zero_ih.pdb')\n",
    "tt = zero_ih.reshape(int(len(zero_ih)/5),5,4)\n",
    "stub = tt[7:10].reshape(15,4)\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load distance maps and endpoints dataset\n",
    "def load_distance_map(name, dm_file='data/Fits_4H_dm_phi.npz'):\n",
    "    rr = np.load(dm_file, allow_pickle=True)\n",
    "    X_train, y_train , featNames = [rr[f] for f in rr.files]\n",
    "    \n",
    "    \n",
    "    return X_train[y_train==name][:,:-4]\n",
    "\n",
    "dm_file = 'data/Fits_4H_dm_phi.npz'\n",
    "rr = np.load(dm_file, allow_pickle=True)\n",
    "X_train, y_train , featNames = [rr[f] for f in rr.files]\n",
    "# = 'data/4H_dataset/models/'\n",
    "# cmd.load(f'{model_direc}{y_train[0]}.pdb')\n",
    "# cmd.save(f'output/test.pdb')\n",
    "\n",
    "#endpoints for data set \n",
    "Fits4H_file = 'data/Fits_4H.csv'\n",
    "dfRead = pd.read_csv(Fits4H_file)\n",
    "df1 = ft.prepData_Str(dfRead,rmsd_filter=100)\n",
    "df2 = ft.EndPoint(df1)\n",
    "ep = df2.to_numpy()[:,:24].astype(float).reshape((-1,8,3))\n",
    "X = ep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_helix_ep(ep_in,helices_desired=[0,1],num_helices=4):\n",
    "    \n",
    "    num_ep = num_helices*2\n",
    "    hi = np.array(helices_desired,dtype=int)\n",
    "    h_ep = np.array(range(num_ep)).reshape((-1,2)) #generate helix to endpoint mapping\n",
    "    \n",
    "    #alternate example for indexing batch of X \n",
    "    #X.reshape((X.shape[0],-1))[:,indexarray]\n",
    "    \n",
    "    #select desired endpoints from  batch of endpoints\n",
    "    return ep_in[np.ix_(np.array(range(ep_in.shape[0])),h_ep[hi].flatten(), np.array(range(ep_in.shape[2])))]\n",
    "    \n",
    "def get_midpoint(ep_in,helices_desired=[0,1],num_helices=4):\n",
    "    \n",
    "    num_ep = num_helices*2\n",
    "    \n",
    "    ind_ep = index_helix_ep(ep_in, helices_desired=helices_desired, num_helices=4)\n",
    "    \n",
    "    #calculate midpoint\n",
    "    midpoint = ind_ep.sum(axis=1)/np.repeat(ind_ep.shape[1], ind_ep.shape[2])\n",
    "    \n",
    "    return midpoint\n",
    "\n",
    "def get_stubs_from_points(ep_in,index=[0,1,2]):\n",
    "#def get_stubs_from_n_ca_c(n, ca, c):\n",
    "    \"\"\"Modified from Brian's npose code  get_stubs_from_n_ca_c, index references 3 points, to define plane.\n",
    "    \"\"\"\n",
    "    e1 = ep_in[:,index[1]]-ep_in[:,index[0]]\n",
    "    e1 = np.divide( e1, np.linalg.norm(e1, axis=1)[..., None] )\n",
    "\n",
    "    e3 = np.cross( e1, ep_in[:,index[2]]-ep_in[:,index[0]], axis=1 )\n",
    "    e3 = np.divide( e3, np.linalg.norm(e3, axis=1)[..., None] )\n",
    "\n",
    "    e2 = np.cross( e3, e1, axis=1 )\n",
    "\n",
    "    stub = np.zeros((len(ep_in), 4, 4))\n",
    "    stub[...,:3,0] = e1\n",
    "    stub[...,:3,1] = e2\n",
    "    stub[...,:3,2] = e3\n",
    "    stub[...,:3,3] = ep_in[:,index[1]]\n",
    "    stub[...,3,3] = 1.0\n",
    "\n",
    "    return stub\n",
    "\n",
    "def xform_npose_2batch(xform, npose):\n",
    "    #single batch code  util.npose_util as xform_npose\n",
    "    return np.matmul(np.repeat(xform[:,np.newaxis,...],npose.shape[1],axis=1),npose[...,None]).squeeze(-1)\n",
    "\n",
    "def xform_to_z_plane(mobile, index_mobile=[0,1,2]):\n",
    "    \"\"\"rotate points into the z-plane for trilaterization. needs additional translation/reflection\"\"\"\n",
    "\n",
    "    mobile_stub = get_stubs_from_points(mobile, index=index_mobile)\n",
    "    mobile_stub_inv = np.linalg.inv(mobile_stub)\n",
    "    \n",
    "    z_plane_ref = np.repeat(np.array([[[0,0,0],[1,0,0],[1,1,0]]]), mobile.shape[0],axis=0)\n",
    "\n",
    "    ref_stub = get_stubs_from_points(z_plane_ref, index=[0,1,2])\n",
    "\n",
    "    xform = ref_stub @ mobile_stub_inv\n",
    "\n",
    "    return xform\n",
    "\n",
    "\n",
    "def rotate_base_tri_Zplane(endpoint_midpoints, target_point=4, index_mobile=[1,2,3]):\n",
    "    \"\"\"rotate points into the z-plane for trilaterization. Target point ensures that point is positive in Z\"\"\"\n",
    "    tp = target_point #target point\n",
    "    zplanexform = xform_to_z_plane(endpoint_midpoints,index_mobile=index_mobile) #one index start base triangle, default\n",
    "    #add one for npose rot calc\n",
    "    npose = np.concatenate((endpoint_midpoints, np.ones((endpoint_midpoints.shape[0],\n",
    "                                                         endpoint_midpoints.shape[1],1))),axis=2) \n",
    "    rot = xform_npose_2batch(zplanexform,npose) # double batch matrix multiplication, see npose, for one batch\n",
    "\n",
    "    #translate X domain to place first index of \"index_mobile\" to 0,0,0\n",
    "    rot[:,:,0] = rot[:,:,0]-np.expand_dims(rot[:,index_mobile[0],0],axis=1)\n",
    "    #based on target point guaranteed to be positive\n",
    "    #reflect new points across the z axis to positive if negative to match just choosing positive solutions\n",
    "    rot[...,2][rot[:,tp,2]<0] = -rot[...,2][rot[:,tp,2]<0]\n",
    "    \n",
    "    return rot[...,:3] #remove npose rotate dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep endpoint dataset for use with easy use with Trilateration:\n",
    "#Essentially identify 3 points on 1st two helices (Rotate/Translate to Z-plane) with \n",
    "#index mobile 1 at 0,0,0, target point in the positive z (trilateration assumtion)\n",
    "#roughly 10% of z values of helices 3/4 are in the negative feild, with point enforced 4 positive\n",
    "\n",
    "#distance map of ep dataset\n",
    "#unsqueeze at two different dimensionsq to broadcast into matrix MX1 by 1XN to MXN \n",
    "dX = np.expand_dims(X,axis=1) - np.expand_dims(X,axis=2)\n",
    "dist = np.sqrt(np.sum(dX**2, 3))  #+ 1e-6) #this dataset is good \n",
    "dist = dist.reshape((dist.shape[0],-1))\n",
    "\n",
    "mp_01 = get_midpoint(X,helices_desired=[0,1])\n",
    "mp_23 = get_midpoint(X,helices_desired=[2,3])\n",
    "# d= np.linalg.norm(mp_02-mp_01,axis=1)\n",
    "# sns.histplot(d)\n",
    "\n",
    "#mp distance map\n",
    "ep_mp = np.hstack((X.reshape((-1,24)),mp_01,mp_23)).reshape(-1,10,3) #helix12mp=8  helix34mp=9\n",
    "#unsqueeze at two different dimensionsq to broadcast into matrix MX1 by 1XN to MXN \n",
    "dep_mp = np.expand_dims(ep_mp,axis=1) - np.expand_dims(ep_mp,axis=2)\n",
    "dist_mp = np.sqrt(np.sum(dep_mp**2, 3))  #+ 1e-6) #this dataset is good \n",
    "dist_mp = dist_mp.reshape((dist_mp.shape[0],-1))\n",
    "\n",
    "\n",
    "zp_ep_mp = rotate_base_tri_Zplane(ep_mp,  target_point=4, index_mobile=[1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.5 , 12.48,  5.82])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zp_ep_mp[0,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#methods to index needed indices from generator\n",
    "\n",
    "def helix_dindex(helices_to_keep, num_helices=4, intraHelixDist=True):\n",
    "    \"\"\"Get index values for parts of the distance map\"\"\"\n",
    "    \n",
    "    #prep indices for distance map\n",
    "    num_ep = num_helices*2\n",
    "    mat_ind = np.array(range((num_ep)**2)).reshape((num_ep,num_ep))\n",
    "    iu1 = np.triu_indices(num_ep, 1)\n",
    "    \n",
    "    helix_used = np.array(helices_to_keep,dtype=int)\n",
    "    \n",
    "    h_ep = np.array(range(num_ep)).reshape((-1,2)) #generate helix to endpoint mapping\n",
    "    \n",
    "    tot_ind = []\n",
    "    \n",
    "    if intraHelixDist:\n",
    "        #get indices of distance map that correspond to each helix, overlap is distances between specified endpoints\n",
    "        for x in helix_used:\n",
    "            new_ind = np.intersect1d(mat_ind[h_ep[x]], mat_ind.T[h_ep[x]])\n",
    "            tot_ind.extend(new_ind)\n",
    "    \n",
    "    \n",
    "    for x in permutations(helix_used,2):\n",
    "        new_ind = np.intersect1d(mat_ind[h_ep[x[0]]], mat_ind.T[h_ep[x[1]]])\n",
    "        tot_ind.extend(new_ind)\n",
    "    \n",
    "    #convert to generator indices (indices of iu1 array)\n",
    "    out_ind = []\n",
    "    for x in tot_ind:\n",
    "        if len(np.nonzero(mat_ind[iu1]==x)[0])>0:\n",
    "            out_ind.append(np.nonzero(mat_ind[iu1]==x))\n",
    "\n",
    "    return np.sort(np.array(out_ind).flatten())\n",
    "\n",
    "\n",
    "def point_dindex(target_points, ref=[4], num_helices = 4):\n",
    "    \n",
    "    num_ep = num_helices*2\n",
    "    mat_ind = np.array(range((num_ep)**2)).reshape((num_ep,num_ep))\n",
    "    iu1 = np.triu_indices(num_ep, 1)\n",
    "    \n",
    "    dindex = []\n",
    "    \n",
    "    for tp in target_points:\n",
    "        for ref_ind in ref:\n",
    "            dindex.append(mat_ind[ref_ind,tp]) #indices for distances to target point\n",
    "    \n",
    "    dindex = np.array(dindex)\n",
    "    \n",
    "    out_ind = []\n",
    "    for x in dindex.flatten():\n",
    "        out_ind.append(np.nonzero(mat_ind[iu1]==x))\n",
    "        \n",
    "    out_ind = np.array(out_ind)\n",
    "    \n",
    "    return out_ind.reshape((len(target_points),-1))\n",
    "\n",
    "def target_dindex(target_points, oneRef = True, num_helices = 5, baseTri_out=True):\n",
    "    \"\"\"Distance map indices for base triangle and output distance map\"\"\"\n",
    "    \n",
    "    num_ep = num_helices*2\n",
    "    mat_ind = np.array(range((num_ep)**2)).reshape((num_ep,num_ep))\n",
    "    iu1 = np.triu_indices(num_ep, 1)\n",
    "\n",
    "    if oneRef:\n",
    "        ref = [1,2,3]\n",
    "        base_tri = [mat_ind[1][2],mat_ind[2][3],mat_ind[1][3]] #p1 to p2, p2 to p3, p1 to p3\n",
    "        \n",
    "    else:\n",
    "        ref = [0,1,2]\n",
    "        base_tri = [mat_ind[0][1],mat_ind[1][2],mat_ind[0][3]] #p0 to p1, p1 to p2, p0 to p3\n",
    "    \n",
    "    dindex = []\n",
    "    \n",
    "    for tp in target_points:\n",
    "        dindex.append(mat_ind[ref,tp]) #indices for distances to target point\n",
    "    \n",
    "    dindex = np.array(dindex)\n",
    "    \n",
    "    out_ind = []\n",
    "    for x in dindex.flatten():\n",
    "        out_ind.append(np.nonzero(mat_ind[iu1]==x))\n",
    "        \n",
    "    out_ind = np.array(out_ind)\n",
    "    \n",
    "    return out_ind.reshape((-1,len(base_tri))),base_tri\n",
    "\n",
    "def minMax_indices(distance_index, point_index, minmax_obj):\n",
    "    \n",
    "    #assemble conversions \n",
    "    #converts output from generator back to real distances\n",
    "    dMin_all = tf.convert_to_tensor(minmax_obj.data_min_, dtype=tf.float32)\n",
    "    mScale_all = tf.convert_to_tensor(minmax_obj.scale_, dtype = tf.float32)\n",
    "    mMin = tf.convert_to_tensor(minmax_obj.feature_range[0], dtype = tf.float32)\n",
    "\n",
    "    #index just the distances we need for calculation\n",
    "    dMin = tf.gather(dMin_all, distance_index,axis=0)\n",
    "    mScale = tf.gather(mScale_all, distance_index,axis=0)\n",
    "\n",
    "    #indexes we need to determine the +/- z value of the new points\n",
    "    pindex = point_dindex([5,6,7], ref=[4], num_helices = 4)\n",
    "    dMin_nwp = tf.gather(dMin_all, point_index,axis=0)\n",
    "    mScale_nwp = tf.gather(mScale_all, point_index,axis=0)\n",
    "    \n",
    "    return dMin, mScale, mMin, dMin_nwp,  mScale_nwp \n",
    "\n",
    "def ref_distmap_index(distances, num_helices = 4):\n",
    "    \n",
    "    num_ep = num_helices*2\n",
    "    mat_ind = np.array(range((num_ep)**2)).reshape((num_ep,num_ep))\n",
    "    iu1 = np.triu_indices(num_ep, 1)\n",
    "    iu1_flat = iu1[0]*8+iu1[1]\n",
    "    \n",
    "#     testDist1 = brec.mm.transform(testDist)\n",
    "\n",
    "#     g1 = tf.convert_to_tensor(testDist1[:batch],dtype=tf.float32)\n",
    "    \n",
    "    return distances[np.ix_(range(distances.shape[0]),iu1_flat)]\n",
    "\n",
    "def convert_dMat_to_iu1_index(indices_in, num_helices = 4):\n",
    "    \"\"\"Converts indices on flattened distance index to iu1 single indices\"\"\"\n",
    "    \n",
    "    \n",
    "    conv_array = np.array(indices_in).flatten()\n",
    "    \n",
    "    num_ep = num_helices*2\n",
    "    mat_ind = np.array(range((num_ep)**2)).reshape((num_ep,num_ep))\n",
    "    iu1 = np.triu_indices(num_ep, 1)\n",
    "    \n",
    "    #convert to generator indices (indices of iu1 array)\n",
    "    out_ind = []\n",
    "    for x in conv_array:\n",
    "        if len(np.nonzero(mat_ind[iu1]==x)[0])>0:\n",
    "            out_ind.append(np.nonzero(mat_ind[iu1]==x))\n",
    "            \n",
    "    out_ind = np.array(out_ind)\n",
    "        \n",
    "    return out_ind.reshape(conv_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_base_triangle_trilateriation(dindex, base_tri, distance_map):\n",
    "    \"\"\"Return x,y,z coords on z-plane of base triangle of tetrahedron from a distance map.\"\"\"\n",
    "    \n",
    "#     dindex, base_tri = target_dindex(targ_dind, oneRef = oneRef, num_helices = num_helices)\n",
    "# #     print(dindex)\n",
    "# #     print(base_tri)\n",
    "    \n",
    "    #test case input data: prep base triangles for trilateration at zplane, (0,0,0) (dvar,0,0) (ivar,jvar,0)\n",
    "    desired_dm = distance_map[:, base_tri] #base tri from dindex\n",
    "\n",
    "    dvar_index = tf.convert_to_tensor(0 ,dtype=tf.int32)\n",
    "    s2_index = tf.convert_to_tensor(2 ,dtype=tf.int32) # we would like the angle across from side 2\n",
    "    s3_index = tf.convert_to_tensor(1 ,dtype=tf.int32)\n",
    "\n",
    "    #x value representing center of 2nd sphere at (dvar,0,0) aka s1\n",
    "    dvar = tf.reshape(tf.gather(desired_dm, dvar_index,axis=1),(-1,1)) #side 1\n",
    "    s2 = tf.reshape(tf.gather(desired_dm,   s2_index,axis=1),(-1,1))\n",
    "    s3 = tf.reshape(tf.gather(desired_dm,   s3_index,axis=1),(-1,1))\n",
    "\n",
    "    #calculate the opposite angle of the the third side of base triangle using law of cosines\n",
    "    s1sq = tf.square(dvar)\n",
    "    s2sq = tf.square(s2)\n",
    "    s3sq = tf.square(s3)\n",
    "    ang3 = np.arccos((-s3sq+s2sq+s1sq)/(2*dvar*s2))\n",
    "\n",
    "    #take third point of base triangle via distance * vector\n",
    "    v13 = tf.concat([tf.cos(ang3), tf.sin(ang3), tf.zeros_like(ang3)], axis=1)\n",
    "    p3 = s2*v13\n",
    "    #center points of 3rd sphere\n",
    "    ivar = tf.reshape(p3[:,0],(-1,1))\n",
    "    jvar = tf.reshape(p3[:,1],(-1,1))\n",
    "\n",
    "\n",
    "    #convert all to float32 to match generator output\n",
    "    #expand to dindex size \n",
    "\n",
    "    dvar = tf.cast(tf.repeat(dvar,dindex.shape[0],axis=1),dtype=tf.float32)\n",
    "    ivar = tf.cast(tf.repeat(ivar,dindex.shape[0],axis=1),dtype=tf.float32)\n",
    "    jvar = tf.cast(tf.repeat(jvar,dindex.shape[0],axis=1),dtype=tf.float32)\n",
    "    \n",
    "    return dvar, ivar, jvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensor flow loss function to guide backprop\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def maskLoss(y_actual, y_pred,mask):\n",
    "    \"\"\"Loss Function for mantaing shape of input helices\"\"\"\n",
    "    custom_loss_val = tf.multiply(mask,tf.square(y_actual-y_pred))\n",
    "    return custom_loss_val\n",
    "\n",
    "@tf.function\n",
    "def midpoints_loss(g1, target, mmTuple, baseTuple, dindex, pindex, batch_size):\n",
    "    \"\"\"Loss function to move output of two generated helices to target midpoint\"\"\"\n",
    "    \n",
    "    \n",
    "    #now using dindex gather the desired indices for tetrahedron calcs\n",
    "\n",
    "    #radius of the spheres, aka the distances to unmasked endpoints\n",
    "    g2 = tf.gather(g1,dindex,axis=1)\n",
    "    \n",
    "    dvar, ivar, jvar = baseTuple \n",
    "    dMin, mScale, mMin, dMin_nwp,  mScale_nwp = mmTuple\n",
    "    \n",
    "\n",
    "    #see https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler\n",
    "    #inspect .scale_\n",
    "    conv_dist = tf.add(tf.divide(tf.subtract(g2, mMin), mScale),dMin)\n",
    "    #transpose lets you easily grab all distances with gather/axis \n",
    "    conv_dist_squared = tf.transpose(tf.square(conv_dist),perm=[0, 2, 1]) \n",
    "\n",
    "    r1_sq = tf.gather(conv_dist_squared, 0, axis=1) \n",
    "    r2_sq =  tf.gather(conv_dist_squared,1, axis=1) \n",
    "    r3_sq = tf.gather(conv_dist_squared, 2, axis=1)\n",
    "\n",
    "    #calculate coordinates of spherial intersect\n",
    "    x = tf.divide(tf.add(tf.subtract(r1_sq,r2_sq),tf.square(dvar)),tf.multiply(2.0,dvar))\n",
    "    y1 = tf.divide(tf.add(tf.add(tf.subtract(r1_sq,r3_sq), tf.square(ivar)), tf.square(jvar)),tf.multiply(2.0,jvar))\n",
    "    y = tf.subtract(y1,tf.multiply(tf.divide(ivar,jvar),x))\n",
    "    \n",
    "    pre_z = tf.subtract(tf.subtract(r1_sq,tf.square(x)),tf.square(y))\n",
    "    fixed_z = tf.clip_by_value(pre_z, 1e-10, 100)\n",
    "    \n",
    "    #adds  to negative values to 0 for sqrt,\n",
    "    #I think is okay as zero z will imply lengthening of distances to match a non-zero target midpoint,\n",
    "    #pushing the network in the desired direction?\n",
    "    \n",
    "    z = tf.sqrt(fixed_z) #assume positive solution\n",
    "    z_neg = tf.multiply(z,-1) #assume negative soluation\n",
    "\n",
    "    #new points, with both assumptions\n",
    "    nwp = tf.concat((tf.reshape(x,(batch_size,-1,1)),\n",
    "                    tf.reshape(y,(batch_size,-1,1)),\n",
    "                    tf.reshape(z,(batch_size,-1,1))), axis=2)  #\n",
    "\n",
    "    nwp_negz = tf.concat((tf.reshape(x,(batch_size,-1,1)),\n",
    "                    tf.reshape(y,(batch_size,-1,1)),\n",
    "                    tf.reshape(z_neg,(batch_size,-1,1))), axis=2)  #\n",
    "    \n",
    "    #some positive solutions assumptions,\n",
    "    # assume first [i4] is actual positive use remaining distances of i4 to (i5,i6,i7) to determine z sign\n",
    "    # closest to matching distance is used\n",
    "\n",
    "\n",
    "    #let's start by calculating all i4 to (i5,i6,i7) distances\n",
    "\n",
    "    #stop the gradients since these are used to index gather and scatter\n",
    "    #unsqueeze at two different dimensionsq to broadcast into matrix MX1 by 1XN to MXN \n",
    "    nwp_p =  tf.stop_gradient(tf.expand_dims(nwp,axis=1) - tf.expand_dims(nwp,axis=2))\n",
    "    nwp_n =  tf.stop_gradient(tf.expand_dims(nwp,axis=1) - tf.expand_dims(nwp_negz,axis=2))\n",
    "    \n",
    "    nwp_dist_pz = tf.reshape(tf.sqrt(tf.reduce_sum(tf.square(nwp_p), 3)),(-1,4,4)) #distance calc +1e6?\n",
    "    nwp_dist_nz = tf.reshape(tf.sqrt(tf.reduce_sum(tf.square(nwp_n), 3)),(-1,4,4))  #distance calc\n",
    "        \n",
    "    z_pn_dist_pre_con = tf.gather(g1,pindex,axis=1)\n",
    "    z_pn_dist = tf.add(tf.divide(tf.subtract(z_pn_dist_pre_con, mMin), mScale_nwp),dMin_nwp)\n",
    "    \n",
    "    #index p4 to p5,p6,p7\n",
    "    #rewrite as non-slice version of this\n",
    "    \n",
    "    nwp_dist_pz_c = tf.squeeze(tf.gather(tf.gather(nwp_dist_pz, [0], axis=1), [1,2,3], axis=2))\n",
    "    nwp_dist_nz_c = tf.squeeze(tf.gather(tf.gather(nwp_dist_nz, [0], axis=1), [1,2,3], axis=2))\n",
    "      \n",
    "    nwp_dist_pz_c = tf.expand_dims(nwp_dist_pz_c,axis=2)\n",
    "    nwp_dist_nz_c = tf.expand_dims(nwp_dist_nz_c,axis=2)\n",
    "\n",
    "#     #using a single distance decide the z assumption and apply\n",
    "    correct_z_assum = tf.abs(z_pn_dist - nwp_dist_nz_c) < tf.abs(z_pn_dist - nwp_dist_pz_c)\n",
    "    d = tf.add(tf.multiply(tf.cast(correct_z_assum,tf.float32),-2),1)\n",
    "    e = tf.repeat(tf.ones_like(d),2,axis=2)\n",
    "    f = tf.ones((batch_size,1,3),dtype=tf.float32)\n",
    "    g = tf.concat((f,tf.concat((e,d),axis=2)),axis=1) #add the rest of the residues as ones\n",
    "    nwp_final = tf.multiply(g,nwp)\n",
    "    \n",
    "    midpoint = tf.reduce_mean(nwp_final,axis=1)\n",
    "    return tf.square(tf.subtract(midpoint,target)) # means squared loss to desired midpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buttress_via_midpoint(gen_obj, ref_map, target_mp, batch_size=32,cycles=100, input_z=None, \n",
    "                          rate=0.05, target_ep=[4,5,6,7], num_helices=4, oneRef=True,\n",
    "                          scale=5.0, z_size=12):\n",
    "    \n",
    "    batch_indices = np.repeat(np.array(range(ref_map.shape[0])),batch_size)\n",
    "    batch = batch_indices.shape[0]\n",
    "    ref_map = np.repeat(ref_map, batch_size, axis=0)\n",
    "    \n",
    "    #input to generator (determinstic output)\n",
    "    if input_z is None:\n",
    "        input_z = tf.random.uniform(shape=(batch, z_size), minval=-1, maxval=1)\n",
    "        \n",
    "    input_z_var = tf.Variable(input_z)\n",
    "    \n",
    "    #establish indices for distances to reference\n",
    "    #prep base triangle, convert distances from minmax to regular\n",
    "    dindex, base_tri = target_dindex(target_ep, oneRef = True, num_helices = num_helices)\n",
    "    base_tri = convert_dMat_to_iu1_index(base_tri) #dirty\n",
    "    pindex = point_dindex(target_ep[1:], ref=[target_ep[0]], num_helices = num_helices)\n",
    "\n",
    "    #convert generator output to 'real distances'\n",
    "    #dMin, mScale, mMin, dMin_nwp,  mScale_nwp = minMax_indices(dindex, pindex, brec.mm)\n",
    "    mmTuple = minMax_indices(dindex, pindex, gen_obj.mm)\n",
    "\n",
    "    # prepare base triangle for trilateriation (z plane , p1 at origin, p2 positive x)\n",
    "    #dvar, ivar, jvar = prep_base_triangle_trilateriation(dist[:batch], targ_dind = [4,5,6,7], oneRef = True, num_helices=4)\n",
    "    #baseTuple = prep_base_triangle_trilateriation(dist[:batch], targ_dind = target_ep, oneRef = True, num_helices=num_helices)\n",
    "\n",
    "    baseTuple = prep_base_triangle_trilateriation(dindex, base_tri, ref_map)\n",
    "    \n",
    "    dvar,ivar,jvar = baseTuple\n",
    "    \n",
    "    if tf.reduce_sum(dvar)<0:\n",
    "        print('dvar',dvar)\n",
    "    if tf.reduce_sum(ivar)<0:\n",
    "        print('ivar',ivar)\n",
    "    if tf.reduce_sum(jvar)<0:\n",
    "        print('jvar',jvar)\n",
    "    \n",
    "    \n",
    "    #mask for keeping buttress helices in same orientation\n",
    "    h_index = helix_dindex([0,1], num_helices=4, intraHelixDist=True)\n",
    "    helix_keep_mask = np.zeros((ref_map.shape[1],),dtype=np.int32)\n",
    "    helix_keep_mask[h_index] = 1\n",
    "    helix_keep_mask = tf.convert_to_tensor(helix_keep_mask,dtype=tf.float32)\n",
    "    \n",
    "    #store grads and inputs as we backpropagate\n",
    "    z=[]\n",
    "    loss_mask = []\n",
    "    loss_mp = []\n",
    "    grads = []\n",
    "\n",
    "    scale = tf.constant(scale)\n",
    "    ref_map_conv = gen_obj.mm.transform(ref_map)\n",
    "    ref_map_conv = tf.convert_to_tensor(ref_map_conv,dtype=tf.float32)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=rate)\n",
    "\n",
    "    g_o = gen_obj.g(input_z_var)\n",
    "    masked_loss = maskLoss(ref_map_conv, g_o, helix_keep_mask)\n",
    "    mp_loss  = tf.divide(midpoints_loss(g_o, target_mp, mmTuple, baseTuple, dindex, pindex, batch), scale)\n",
    "    loss = tf.reduce_sum(mp_loss,axis=1) + tf.reduce_sum(masked_loss,axis=1)\n",
    "\n",
    "    print('start_masked',np.round(np.sum(masked_loss),2))\n",
    "    print('start_mp',np.round(np.sum(mp_loss),2))\n",
    "\n",
    "\n",
    "    for t in range(1,cycles):\n",
    "\n",
    "        #compute Loss\n",
    "        with tf.GradientTape() as g_tape:\n",
    "            g_tape.watch(input_z_var)\n",
    "            g_o = gen_obj.g(input_z_var)\n",
    "            masked_loss = maskLoss(ref_map_conv, g_o, helix_keep_mask)\n",
    "            mp_loss  = tf.divide(midpoints_loss(g_o, target_mp, mmTuple, baseTuple, dindex, pindex, batch),scale)\n",
    "\n",
    "            loss = tf.reduce_sum(mp_loss,axis=1) + tf.reduce_sum(masked_loss,axis=1)\n",
    "\n",
    "        g_grads = g_tape.gradient(loss, input_z_var)\n",
    "\n",
    "        optimizer.apply_gradients(zip([g_grads],[input_z_var]))\n",
    "        \n",
    "        z.append(tf.identity(input_z_var).numpy())\n",
    "        loss_mask.append(tf.identity(masked_loss).numpy())\n",
    "        loss_mp.append(tf.identity(mp_loss).numpy())\n",
    "\n",
    "    print('end_masked', np.round(np.sum(masked_loss),2))\n",
    "    print('end_mp', np.round(np.sum(mp_loss),2))\n",
    "    \n",
    "    return z, loss_mask, loss_mp, batch_indices\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.35, 12.37, -5.96],\n",
       "       [ 0.  , -0.  , -0.  ],\n",
       "       [10.4 ,  0.  ,  0.  ],\n",
       "       [23.18, 23.99,  0.  ],\n",
       "       [14.48, 22.47,  4.66],\n",
       "       [ 6.26,  0.85, 11.63],\n",
       "       [ 0.1 ,  5.49,  7.34],\n",
       "       [ 5.17, 21.1 , -0.35],\n",
       "       [10.73,  9.09, -1.49],\n",
       "       [ 6.5 , 12.48,  5.82]])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(zp_ep_mp[0],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gen=\"data/BestGenerator\"\n",
    "gen_obj = ge.BatchRecon(gen)\n",
    "output1=gen_obj.generate(z=12,batch_size=12) #example generator\n",
    "\n",
    "target_midpoint = tf.convert_to_tensor([5.0,10.0,5.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "#will need to add method to convert helix 1,2,3,4 to just helix 1,2 (remove 1,2 convert 3,4 to )\n",
    "ref_map_base = ref_distmap_index(dist, num_helices=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_masked 512.73\n",
      "start_mp 696.4\n",
      "end_masked 12.54\n",
      "end_mp 557.57\n"
     ]
    }
   ],
   "source": [
    "output_z = buttress_via_mask_only(gen_obj, ref_map_base[:10], target_midpoint, batch_size=64,cycles=200, input_z=None, \n",
    "                          rate=0.01, target_ep=[4,5,6,7], num_helices=4, oneRef=False,\n",
    "                          scale=100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_masked 523.3\n",
      "start_mp 272.62\n",
      "end_masked 12.24\n",
      "end_mp 21.81\n"
     ]
    }
   ],
   "source": [
    "#gen_obj  generator network to propagate through\n",
    "#ref_map_base   distance map to buttress from\n",
    "#target in space (after z-orientation)\n",
    "#oneRef - False -> use first three endpoints of two helices to buttress [0,1,2], True = use [1,2,3]\n",
    "#scale how much to divide midpoints loss by to miss with mask loss since it is no longer normalized to min max of generator\n",
    "#batch, (one batch per eacy reference distance map (ref_map_base))\n",
    "\n",
    "\n",
    "output_z, loss_mask, loss_mp, batchInd = buttress_via_midpoint(gen_obj, ref_map_base[:20], target_midpoint, batch_size=32,cycles=200, input_z=None, \n",
    "                          rate=0.05, target_ep=[4,5,6,7], num_helices=4, oneRef=True,\n",
    "                          scale=200.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.sum(loss_mp[-1],axis=1)<0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "640"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.sum(loss_mp[-1],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sum(loss_mask[-1]<0.001,axis=1)>27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mp = np.sum(loss_mp[-1]<0.001,axis=1)>2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91, 12)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zuse = output_z[-1][best_mp]\n",
    "zuse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "uInd = batchInd[best_mp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  2,  2,  2,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  4,\n",
       "        4,  4,  4,  5,  5,  5,  5,  5,  5,  5,  6,  6,  7,  7,  7,  7,  7,\n",
       "        7,  7,  7,  7,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10,\n",
       "       10, 10, 10, 11, 11, 11, 11, 12, 12, 13, 14, 15, 15, 15, 15, 15, 16,\n",
       "       16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17,\n",
       "       17, 17, 18, 18, 18, 19])"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uInd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "zuse = output_z[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_obj.generate(z=12,input_z=zuse,batch_size=zuse.shape[0])\n",
    "gen_obj.MDS_reconstruct_()\n",
    "gen_obj.reconstructionError()\n",
    "npo = gen_obj.to_npose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_ep = np.array(gen_obj.reconsMDS_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check and see real midpoints shift relative to proper reference\n",
    "\n",
    "\n",
    "\n",
    "# d= np.linalg.norm(mp_02-mp_01,axis=1)\n",
    "# sns.histplot(d)\n",
    "\n",
    "\n",
    "                                                                    #oneRef indices\n",
    "out_zp_ep = rotate_base_tri_Zplane(out_ep,  target_point=4, index_mobile=[1,2,3])\n",
    "\n",
    "\n",
    "out_mp_01 = get_midpoint(out_zp_ep,helices_desired=[0,1],num_helices=4)\n",
    "out_mp_23 = get_midpoint(out_zp_ep,helices_desired=[2,3],num_helices=4)\n",
    "\n",
    "\n",
    "#mp distance map\n",
    "out_ep_mp = np.hstack((out_zp_ep.reshape((-1,24)), out_mp_01, out_mp_23)).reshape(-1,10,3) #helix12mp=8  helix34mp=9\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomp = np.sqrt(np.sum(np.square(out_ep_mp[:,9]-target_midpoint),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6555066"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(tomp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "outDirec= 'output/'\n",
    "for i,c in enumerate(gen_obj.npose_list):\n",
    "    nu.dump_npdb(c,f'{outDirec}build{i}.pdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "#moving 2 onto on\n",
    "\n",
    "#new endpoints #sel2\n",
    "out_zp_ep_4 = out_zp_ep[:,:4,:]\n",
    "\n",
    "#old endpoints #sel1\n",
    "original_ep = zp_ep_mp[uInd,:4,:] #remove 2 midpoints and 4 top points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kabsch\n",
    "#\n",
    "#Thanks to below for this code modified to batch form \n",
    "#https://pymolwiki.org/index.php/Kabsch\n",
    "\n",
    "# check for consistency\n",
    "assert len(out_zp_ep_4) == len(original_ep)\n",
    "L = len(out_zp_ep_4)\n",
    "assert L > 0\n",
    "#centering to prevent affine transformaiton\n",
    "#these are centered at zero on the z-plane already\n",
    "E0 = np.sum( np.sum(np.square(out_zp_ep_4),axis=1),axis=1) + np.sum( np.sum(np.square(original_ep),axis=1),axis=1)\n",
    "\n",
    "# This beautiful step provides the answer.  V and Wt are the orthonormal\n",
    "# bases that when multiplied by each other give us the rotation matrix, U.\n",
    "# S, (Sigma, from SVD) provides us with the error!  Isn't SVD great!\n",
    "\n",
    "#2 onto #1\n",
    "                                                       #2                      #1\n",
    "V, S, Wt = np.linalg.svd( np.matmul(np.transpose(out_zp_ep_4,axes=[0,2,1]), original_ep))\n",
    "\n",
    "# we already have our solution, in the results from SVD.\n",
    "# we just need to check for reflections and then produce\n",
    "# the rotation.  V and Wt are orthonormal, so their det's\n",
    "# are +/-1.\n",
    "\n",
    "reflect = np.linalg.det(V) * np.linalg.det(Wt)\n",
    "proper_reflection = ((reflect>0).astype(np.int32)*-2+1) #multiples by 1 or -1 depending if relfect is negative (reflection)\n",
    "\n",
    "S[:,-1] = S[:,-1]*proper_reflection\n",
    "V[:,:,-1] = -V[:,:,-1]*proper_reflection.reshape((-1,1))\n",
    "\n",
    "RMSD = E0 - (2.0 * np.sum(S,axis=1))\n",
    "RMSD = np.sqrt(np.abs(RMSD / L))\n",
    "\n",
    "U = np.matmul(V, Wt)\n",
    "final_ep = np.matmul(out_zp_ep_4, U) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ep_full = np.matmul(out_zp_ep, U) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save these comparisons for figure at some point\n",
    "\n",
    "#method to view points in pymol  \n",
    "hf.HelicalProtein.makePointPDB(list(final_ep[0]),f'final.pdb',outDirec='output/')\n",
    "hf.HelicalProtein.makePointPDB(list(original_ep[0]),f'buttend.pdb',outDirec='output/')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf.HelicalProtein.makePointPDB(list(out_zp_ep[0]),f'final_pre_full.pdb',outDirec='output/')\n",
    "hf.HelicalProtein.makePointPDB(list(out_zp_ep_4[0]),f'final_pre.pdb',outDirec='output/')\n",
    "hf.HelicalProtein.makePointPDB(list(final_ep_full[0]),f'final_full.pdb',outDirec='output/')\n",
    "hf.HelicalProtein.makePointPDB(list(zp_ep_mp[uInd,:8,:][0]),f'original_full.pdb',outDirec='output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Buttress as but"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "refName = y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " PyMOL not running, entering library mode (experimental)\n"
     ]
    }
   ],
   "source": [
    "ref = f'data/4H_dataset/models/{refName}'\n",
    "pDirec = 'output/'\n",
    "but.align_helices(ref,pDirec,helix_list=[1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Count'>"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mp_01 = get_midpoint(out_ep,helices_desired=[0,1])\n",
    "mp_02 = get_midpoint(out_ep,helices_desired=[2,3])\n",
    "d= np.linalg.norm(mp_02-mp_01,axis=1)\n",
    "sns.histplot(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -2.85,   8.55,  -1.27],\n",
       "       [  8.88,   0.21,   1.58],\n",
       "       [ -5.05,   4.57,   7.69],\n",
       "       [  3.24,  -1.97, -10.27],\n",
       "       [  2.75,  -7.65,   4.87],\n",
       "       [ -2.39, -10.91,   3.64],\n",
       "       [  4.59,  -5.28,  -3.98],\n",
       "       [ -1.6 ,   8.35,  -3.97],\n",
       "       [ -6.58,   6.26,   5.09],\n",
       "       [  3.25,   5.4 ,  -6.05],\n",
       "       [  5.5 ,  -5.48,  -6.96],\n",
       "       [ -3.  ,   4.92,  -9.09],\n",
       "       [  6.23,   6.7 ,  -3.78],\n",
       "       [ -6.  ,  -7.53,   6.84],\n",
       "       [ -5.22,   8.36,  -2.46],\n",
       "       [  9.11,   1.64,  -3.8 ],\n",
       "       [ -1.33,   4.8 ,  -8.36],\n",
       "       [  5.86,  -6.48,  -4.36],\n",
       "       [  9.62,   1.42,  -3.11],\n",
       "       [  9.31,   1.38,  -5.46],\n",
       "       [ -1.78,   6.66,  -7.73],\n",
       "       [ -2.63,   4.26,  -7.45],\n",
       "       [ -5.22,   7.68,   3.61],\n",
       "       [ -4.78,   6.73,  -4.8 ],\n",
       "       [ -2.3 ,  -2.91,   9.94],\n",
       "       [ -3.54,   9.3 ,  -4.18],\n",
       "       [ -0.42,   3.87, -10.  ],\n",
       "       [  5.56,  -2.13,  -7.37],\n",
       "       [  0.56,   5.47,  -9.51],\n",
       "       [ -4.27,  11.01,   2.76],\n",
       "       [  7.  ,   5.2 ,  -2.15],\n",
       "       [ -7.48,  -1.05,   6.  ],\n",
       "       [ -3.83,  -6.2 ,   6.36],\n",
       "       [ -1.81,  -6.65,   9.93],\n",
       "       [ -4.41,  -4.47,   9.05],\n",
       "       [  6.7 ,   1.52,  -4.42],\n",
       "       [  7.04,  -1.76,   9.46],\n",
       "       [  6.25,  -6.03,   3.9 ],\n",
       "       [ -3.65,   2.54,   8.19],\n",
       "       [  5.3 ,   2.25,  -7.72],\n",
       "       [  2.39,   7.41,  -6.27],\n",
       "       [  3.11,  -0.22,  10.43]])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp_02-mp_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to view points in pymol  \n",
    "hf.HelicalProtein.makePointPDB(list(ep_mp[0][1:5]),f'ep_midpoint.pdb',outDirec='output/')\n",
    "#hf.HelicalProtein.makePointPDB(dist_mp ,f'buttend.pdb',outDirec='output/')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buttress_via_mask_only(gen_obj, ref_map, target_mp, batch_size=32,cycles=100, input_z=None, \n",
    "                          rate=0.05, target_ep=[4,5,6,7], num_helices=4, oneRef=True,\n",
    "                          scale=5.0, z_size=12):\n",
    "    \n",
    "    \n",
    "    batch = batch_size*ref_map.shape[0]\n",
    "    ref_map = np.repeat(ref_map, batch_size, axis=0)\n",
    "    \n",
    "    #input to generator (determinstic output)\n",
    "    if input_z is None:\n",
    "        input_z = tf.random.uniform(shape=(batch, z_size), minval=-1, maxval=1)\n",
    "        \n",
    "    input_z_var = tf.Variable(input_z)\n",
    "    \n",
    "    #establish indices for distances to reference\n",
    "    #prep base triangle, convert distances from minmax to regular\n",
    "    dindex, base_tri = target_dindex(target_ep, oneRef = True, num_helices = num_helices)\n",
    "    base_tri = convert_dMat_to_iu1_index(base_tri) #dirty\n",
    "    pindex = point_dindex(target_ep[1:], ref=[target_ep[0]], num_helices = num_helices)\n",
    "\n",
    "    #convert generator output to 'real distances'\n",
    "    #dMin, mScale, mMin, dMin_nwp,  mScale_nwp = minMax_indices(dindex, pindex, brec.mm)\n",
    "    mmTuple = minMax_indices(dindex, pindex, gen_obj.mm)\n",
    "\n",
    "    # prepare base triangle for trilateriation (z plane , p1 at origin, p2 positive x)\n",
    "    #dvar, ivar, jvar = prep_base_triangle_trilateriation(dist[:batch], targ_dind = [4,5,6,7], oneRef = True, num_helices=4)\n",
    "    #baseTuple = prep_base_triangle_trilateriation(dist[:batch], targ_dind = target_ep, oneRef = True, num_helices=num_helices)\n",
    "\n",
    "    baseTuple = prep_base_triangle_trilateriation(dindex, base_tri, ref_map)\n",
    "    \n",
    "    \n",
    "    #mask for keeping buttress helices in same orientation\n",
    "    h_index = helix_dindex([0,1], num_helices=4, intraHelixDist=True)\n",
    "    helix_keep_mask = np.zeros((ref_map.shape[1],),dtype=np.int32)\n",
    "    helix_keep_mask[h_index] = 1\n",
    "    helix_keep_mask = tf.convert_to_tensor(helix_keep_mask,dtype=tf.float32)\n",
    "    \n",
    "    #store grads and inputs as we backpropagate\n",
    "    z=[]\n",
    "    grads = []\n",
    "\n",
    "    scale = tf.constant(scale)\n",
    "    ref_map_conv = gen_obj.mm.transform(ref_map)\n",
    "    ref_map_conv = tf.convert_to_tensor(ref_map_conv,dtype=tf.float32)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=rate)\n",
    "\n",
    "    g_o = gen_obj.g(input_z_var)\n",
    "    masked_loss = maskLoss(ref_map_conv, g_o, helix_keep_mask)\n",
    "    mp_loss  = tf.divide(midpoints_loss(g_o, target_mp, mmTuple, baseTuple, dindex, pindex, batch), scale)\n",
    "    loss = tf.reduce_sum(mp_loss,axis=1) + tf.reduce_sum(masked_loss,axis=1)\n",
    "\n",
    "    print('start_masked',np.round(np.sum(masked_loss),2))\n",
    "    print('start_mp',np.round(np.sum(mp_loss),2))\n",
    "\n",
    "\n",
    "    for t in range(1,cycles):\n",
    "\n",
    "        #compute Loss\n",
    "        with tf.GradientTape() as g_tape:\n",
    "            g_tape.watch(input_z_var)\n",
    "            g_o = gen_obj.g(input_z_var)\n",
    "            masked_loss = maskLoss(ref_map_conv, g_o, helix_keep_mask)\n",
    "            mp_loss  = tf.divide(midpoints_loss(g_o, target_mp, mmTuple, baseTuple, dindex, pindex, batch),scale)\n",
    "\n",
    "#             loss = tf.reduce_sum(mp_loss,axis=1) + tf.reduce_sum(masked_loss,axis=1)\n",
    "\n",
    "        g_grads = g_tape.gradient(masked_loss, input_z_var)\n",
    "\n",
    "        optimizer.apply_gradients(zip([g_grads],[input_z_var]))\n",
    "        \n",
    "        z.append(tf.identity(input_z_var))\n",
    "\n",
    "    print('end_masked', np.round(np.sum(masked_loss),2))\n",
    "    print('end_mp', np.round(np.sum(mp_loss),2))\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fa_tfpy",
   "language": "python",
   "name": "fa_tfpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
