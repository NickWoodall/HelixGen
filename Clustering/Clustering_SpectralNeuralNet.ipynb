{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "import sys, os\n",
    "# add directories in src/ to path\n",
    "sys.path.insert(0, 'SpectralNet-master/src/applications/')\n",
    "sys.path.insert(0, 'SpectralNet-master/src/')\n",
    "from spectralnet import run_net\n",
    "from core.data import get_data\n",
    "\n",
    "from sklearn.neighbors import LSHForest\n",
    "import joblib\n",
    "\n",
    "\n",
    "\n",
    "# '''\n",
    "# spectralnet.py: contains run function for spectralnet\n",
    "# '''\n",
    "import sys, os, pickle\n",
    "import traceback\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import normalized_mutual_info_score as nmi\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Lambda\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "from core import train\n",
    "from core import costs\n",
    "from core import networks\n",
    "from core.layer import stack_layers\n",
    "from core.util import get_scale, print_accuracy, get_cluster_sols, LearningHandler, make_layer_list, train_gen, get_y_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectralNet_FromWeights(data,params,siamWeightsPath,specWeightsPath):\n",
    "    \n",
    "     #\n",
    "    # UNPACK DATA\n",
    "    #\n",
    "\n",
    "    x_train, y_train, x_val, y_val, x_test, y_test = data['spectral']['train_and_test']\n",
    "    x_train_unlabeled, y_train_unlabeled, x_train_labeled, y_train_labeled = data['spectral']['train_unlabeled_and_labeled']\n",
    "    x_val_unlabeled, y_val_unlabeled, x_val_labeled, y_val_labeled = data['spectral']['val_unlabeled_and_labeled']\n",
    "\n",
    "    if 'siamese' in params['affinity']:\n",
    "        pairs_train, dist_train, pairs_val, dist_val = data['siamese']['train_and_test']\n",
    "\n",
    "    x = np.concatenate((x_train, x_val, x_test), axis=0)\n",
    "    y = np.concatenate((y_train, y_val, y_test), axis=0)\n",
    "\n",
    "    if len(x_train_labeled):\n",
    "        y_train_labeled_onehot = OneHotEncoder().fit_transform(y_train_labeled.reshape(-1, 1)).toarray()\n",
    "    else:\n",
    "        y_train_labeled_onehot = np.empty((0, len(np.unique(y))))\n",
    "\n",
    "    #\n",
    "    # SET UP INPUTS\n",
    "    #\n",
    "\n",
    "    # create true y placeholder (not used in unsupervised training)\n",
    "    y_true = tf.placeholder(tf.float32, shape=(None, params['n_clusters']), name='y_true')\n",
    "\n",
    "    batch_sizes = {\n",
    "            'Unlabeled': params['batch_size'],\n",
    "            'Labeled': params['batch_size'],\n",
    "            'Orthonorm': params.get('batch_size_orthonorm', params['batch_size']),\n",
    "            }\n",
    "\n",
    "    input_shape = x.shape[1:]\n",
    "\n",
    "    # spectralnet has three inputs -- they are defined here\n",
    "    inputs = {\n",
    "            'Unlabeled': Input(shape=input_shape,name='UnlabeledInput'),\n",
    "            'Labeled': Input(shape=input_shape,name='LabeledInput'),\n",
    "            'Orthonorm': Input(shape=input_shape,name='OrthonormInput'),\n",
    "            }\n",
    "\n",
    "    #\n",
    "    # DEFINE AND TRAIN SIAMESE NET\n",
    "    #\n",
    "\n",
    "    # run only if we are using a siamese network\n",
    "    if params['affinity'] == 'siamese':\n",
    "        siamese_net = networks.SiameseNet(inputs, params['arch'], params.get('siam_reg'), y_true)\n",
    "\n",
    "        history = siamese_net.train(pairs_train, dist_train, pairs_val, dist_val,\n",
    "                params['siam_lr'], params['siam_drop'], params['siam_patience'],\n",
    "                1, params['siam_batch_size'])\n",
    "        siamese_net.net.load_weights(siamWeightsPath, by_name=True)\n",
    "\n",
    "    else:\n",
    "        siamese_net = None\n",
    "\n",
    "    #\n",
    "    # DEFINE AND TRAIN SPECTRALNET\n",
    "    #\n",
    "\n",
    "    spectral_net = networks.SpectralNet(inputs, params['arch'],\n",
    "            params.get('spec_reg'), y_true, y_train_labeled_onehot,\n",
    "            params['n_clusters'], params['affinity'], params['scale_nbr'],\n",
    "            params['n_nbrs'], batch_sizes, siamese_net, x_train, len(x_train_labeled))\n",
    "\n",
    "    spectral_net.train(\n",
    "            x_train_unlabeled, x_train_labeled, x_val_unlabeled,\n",
    "            params['spec_lr'], params['spec_drop'], params['spec_patience'],\n",
    "            1)\n",
    "\n",
    "    spectral_net.net.load_weights(specWeightsPath, by_name=True)\n",
    "\n",
    "    print(\"finished training\")\n",
    "\n",
    "    #\n",
    "    # EVALUATE\n",
    "    #\n",
    "\n",
    "    #get final embeddings\n",
    "    x_spectralnet = spectral_net.predict(x)\n",
    "\n",
    "    #get accuracy and nmi\n",
    "    kmeans_assignments, km = get_cluster_sols(x_spectralnet, ClusterClass=KMeans, n_clusters=params['n_clusters'], init_args={'n_init':10})\n",
    "    \n",
    "    kmeans_assignments = km.predict(x_spectralnet)\n",
    "    \n",
    "    y_spectralnet, _ = get_y_preds(kmeans_assignments, y, params['n_clusters'])\n",
    "    print_accuracy(kmeans_assignments, y, params['n_clusters'])\n",
    "\n",
    "    return km, siamese_net, spectral_net #,x_spectralnet, y_spectralnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = defaultdict(lambda: None)\n",
    "#change to dset  = mnist and codespace = True\n",
    "general_params = {\n",
    "        'dset': 'new',                  # dataset: reuters / mnist\n",
    "        'val_set_fraction': 0.1,            # fraction of training set to use as validation\n",
    "        'precomputedKNNPath': '',           # path for precomputed nearest neighbors (with indices and saved as a pickle or h5py file)\n",
    "        'siam_batch_size': 128,             # minibatch size for siamese net\n",
    "        }\n",
    "params.update(general_params)\n",
    "#         'train_labeled_fraction':True,\n",
    "#         'val_labeled_fraction':True,\n",
    "my_params = {\n",
    "\n",
    "        'n_clusters': 26,                   # number of clusters in data\n",
    "        'use_code_space': False,             # enable / disable code space embedding\n",
    "        'affinity': 'siamese',              # affinity type: siamese / knn\n",
    "        'n_nbrs': 10,                        # number of nonzero entries (neighbors) to use for graph Laplacian affinity matrix\n",
    "        'scale_nbr': 2,                     # neighbor used to determine scale of gaussian graph Laplacian; calculated by\n",
    "                                            # taking median distance of the (scale_nbr)th neighbor, over a set of size batch_size\n",
    "                                            # sampled from the datset\n",
    "\n",
    "        'siam_k': 2,                        # threshold where, for all k <= siam_k closest neighbors to x_i, (x_i, k) is considered\n",
    "                                            # a 'positive' pair by siamese net\n",
    "\n",
    "        'siam_ne': 50,                     # number of training epochs for siamese net\n",
    "        'spec_ne': 150,                     # number of training epochs for spectral net\n",
    "        'siam_lr': 1e-3,                    # initial learning rate for siamese net\n",
    "        'spec_lr': 1e-3,                    # initial learning rate for spectral net #hardcoded in network.py?\n",
    "        'siam_patience': 10,                # early stopping patience for siamese net\n",
    "        'spec_patience': 20,                # early stopping patience for spectral net\n",
    "        'siam_drop': 0.1,                   # learning rate scheduler decay for siamese net\n",
    "        'spec_drop': 0.1,                   # learning rate scheduler decay for spectral net\n",
    "        'batch_size': 1024,                 # batch size for spectral net\n",
    "        'siam_reg': None,                   # regularization parameter for siamese net\n",
    "        'spec_reg': None,                   # regularization parameter for spectral net\n",
    "        'siam_n': None,                     # subset of the dataset used to construct training pairs for siamese net\n",
    "        'siamese_tot_pairs': 600000,        # total number of pairs for siamese net\n",
    "        'arch': [                           # network architecture. if different architectures are desired for siamese net and\n",
    "                                            #   spectral net, 'siam_arch' and 'spec_arch' keys can be used\n",
    "            {'type': 'relu', 'size': 1024},\n",
    "            {'type': 'relu', 'size': 1024},\n",
    "            {'type': 'relu', 'size': 512},\n",
    "            {'type': 'relu', 'size': 10},\n",
    "            ],\n",
    "        'use_approx': False,                # enable / disable approximate nearest neighbors\n",
    "        'use_all_data': False,               # enable to use all data for training (no test set)\n",
    "        }\n",
    "params.update(my_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save(outName= 'data/testModel', dataIn='data/refData.npz'):\n",
    "    #testData for training\n",
    "    # tf.test.is_gpu_available()\n",
    "    rr = np.load(dataIn, allow_pickle=True)\n",
    "    #X_train data is the feature for spectral clustering(Midpoint distance + dihedral angles between helices)\n",
    "    #16 total, eight each\n",
    "    #y_name is the name of the protein\n",
    "    #y_ is the assigned cluster labels from real spectral clustering\n",
    "    y_start , y_name_start, X_train_start, featNames  = [rr[f] for f in rr.files]\n",
    "\n",
    "    X_train_start = X_train_start[:,:-8] #remove phi values and length\n",
    "\n",
    "    #Warning! test Train splits hard coded here for original data\n",
    "    X_test = X_train_start[22000:,:]\n",
    "    y_test = y_start[22000:]\n",
    "\n",
    "    X_train = X_train_start[:22000,:]\n",
    "    y_train = y_start[:22000]\n",
    "\n",
    "    #run this to organize the data into the approriate dictionary formats\n",
    "    print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "    new_dataset_data = (X_train, X_test, y_train, y_test)\n",
    "    ata = get_data(params,new_dataset_data)\n",
    "    \n",
    "    #train net with this code\n",
    "    siamese_net_model, spectral_net_model, x_spectralnet, y_spectralnet,km = run_net(ata, params)\n",
    "    \n",
    "    #save the weights for loading by spectralNet_FromWeights\n",
    "    spectral_net_model.net.save_weights(f'{outName}_spectral_net.tf')\n",
    "    siamese_net_model.net.save_weights(f'{outName}_siamese_net.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22000, 12) (22000,) (5877, 12) (5877,)\n",
      "WARNING: Using data provided in arguments. Must be tuple or array of format (x_train, x_test, y_train, y_test)\n",
      "computing k=2 nearest neighbors...\n",
      "creating pairs...\n",
      "ks 19800 2 2 2\n",
      "Iter: 0/19800\n",
      "Iter: 10000/19800\n",
      "computing k=2 nearest neighbors...\n",
      "creating pairs...\n",
      "ks 2200 2 2 2\n",
      "Iter: 0/2200\n",
      "Epoch 1/50\n",
      "618/618 [==============================] - 3s 4ms/step - loss: 0.0156 - val_loss: 0.0247\n",
      "Epoch 2/50\n",
      "618/618 [==============================] - 2s 4ms/step - loss: 0.0122 - val_loss: 0.0214\n",
      "Epoch 3/50\n",
      "618/618 [==============================] - 2s 4ms/step - loss: 0.0111 - val_loss: 0.0217\n",
      "Epoch 4/50\n",
      "618/618 [==============================] - 2s 4ms/step - loss: 0.0106 - val_loss: 0.0219\n",
      "Epoch 5/50\n",
      "618/618 [==============================] - 2s 3ms/step - loss: 0.0103 - val_loss: 0.0194\n",
      "Epoch 6/50\n",
      "618/618 [==============================] - 2s 4ms/step - loss: 0.0099 - val_loss: 0.0204\n",
      "Epoch 7/50\n",
      "618/618 [==============================] - 2s 4ms/step - loss: 0.0098 - val_loss: 0.0191\n",
      "Epoch 8/50\n",
      "618/618 [==============================] - 2s 4ms/step - loss: 0.0095 - val_loss: 0.0192\n",
      "Epoch 9/50\n",
      "618/618 [==============================] - 3s 4ms/step - loss: 0.0094 - val_loss: 0.0201\n",
      "Epoch 10/50\n",
      "618/618 [==============================] - 2s 4ms/step - loss: 0.0093 - val_loss: 0.0188\n",
      "Epoch 11/50\n",
      "618/618 [==============================] - 2s 4ms/step - loss: 0.0091 - val_loss: 0.0205\n",
      "Epoch 12/50\n",
      "618/618 [==============================] - 2s 4ms/step - loss: 0.0091 - val_loss: 0.0200\n",
      "Epoch 13/50\n",
      "618/618 [==============================] - 2s 4ms/step - loss: 0.0091 - val_loss: 0.0212\n",
      "Epoch 14/50\n",
      "618/618 [==============================] - 2s 4ms/step - loss: 0.0090 - val_loss: 0.0194\n",
      "Epoch 15/50\n",
      "618/618 [==============================] - 2s 4ms/step - loss: 0.0091 - val_loss: 0.0197\n",
      "Epoch 16/50\n",
      "618/618 [==============================] - 2s 4ms/step - loss: 0.0089 - val_loss: 0.0214\n",
      "Epoch 17/50\n",
      "618/618 [==============================] - 2s 4ms/step - loss: 0.0089 - val_loss: 0.0207\n",
      "Epoch 18/50\n",
      "618/618 [==============================] - 2s 4ms/step - loss: 0.0088 - val_loss: 0.0207\n",
      "Epoch 19/50\n",
      "618/618 [==============================] - 2s 4ms/step - loss: 0.0088 - val_loss: 0.0196\n",
      "Epoch 20/50\n",
      "618/618 [==============================] - 2s 4ms/step - loss: 0.0087 - val_loss: 0.0202\n",
      "Epoch 21/50\n",
      "618/618 [==============================] - 2s 3ms/step - loss: 0.0087 - val_loss: 0.0211\n",
      "Epoch 22/50\n",
      "618/618 [==============================] - 2s 3ms/step - loss: 0.0077 - val_loss: 0.0197\n",
      "Epoch 23/50\n",
      "618/618 [==============================] - 2s 4ms/step - loss: 0.0072 - val_loss: 0.0195\n",
      "Epoch 24/50\n",
      "618/618 [==============================] - 2s 4ms/step - loss: 0.0070 - val_loss: 0.0195\n",
      "Epoch 25/50\n",
      "618/618 [==============================] - 2s 4ms/step - loss: 0.0070 - val_loss: 0.0198\n",
      "Epoch 26/50\n",
      "618/618 [==============================] - 2s 4ms/step - loss: 0.0068 - val_loss: 0.0195\n",
      "Epoch 27/50\n",
      "618/618 [==============================] - 2s 3ms/step - loss: 0.0068 - val_loss: 0.0197\n",
      "Epoch 28/50\n",
      "618/618 [==============================] - 2s 3ms/step - loss: 0.0068 - val_loss: 0.0199\n",
      "Epoch 29/50\n",
      "618/618 [==============================] - 2s 3ms/step - loss: 0.0066 - val_loss: 0.0199\n",
      "Epoch 30/50\n",
      "618/618 [==============================] - 2s 3ms/step - loss: 0.0067 - val_loss: 0.0206\n",
      "Epoch 31/50\n",
      "618/618 [==============================] - 2s 4ms/step - loss: 0.0066 - val_loss: 0.0206\n",
      "Epoch 32/50\n",
      "618/618 [==============================] - 2s 4ms/step - loss: 0.0066 - val_loss: 0.0204\n",
      "Epoch 33/50\n",
      "618/618 [==============================] - 2s 4ms/step - loss: 0.0062 - val_loss: 0.0201\n",
      "Epoch 34/50\n",
      "618/618 [==============================] - 2s 4ms/step - loss: 0.0062 - val_loss: 0.0202\n",
      "Epoch 35/50\n",
      "618/618 [==============================] - 2s 4ms/step - loss: 0.0063 - val_loss: 0.0202\n",
      "Epoch 36/50\n",
      "618/618 [==============================] - 2s 4ms/step - loss: 0.0062 - val_loss: 0.0202\n",
      "Epoch 37/50\n",
      "618/618 [==============================] - 2s 4ms/step - loss: 0.0062 - val_loss: 0.0202\n",
      "Epoch 38/50\n",
      "618/618 [==============================] - 2s 4ms/step - loss: 0.0061 - val_loss: 0.0202\n",
      "Epoch 39/50\n",
      "618/618 [==============================] - 2s 4ms/step - loss: 0.0062 - val_loss: 0.0203\n",
      "Epoch 40/50\n",
      "618/618 [==============================] - 2s 4ms/step - loss: 0.0062 - val_loss: 0.0203\n",
      "Epoch 41/50\n",
      "618/618 [==============================] - 2s 3ms/step - loss: 0.0062 - val_loss: 0.0203\n",
      "Epoch 42/50\n",
      "618/618 [==============================] - 2s 3ms/step - loss: 0.0062 - val_loss: 0.0203\n",
      "Epoch 43/50\n",
      "618/618 [==============================] - 2s 4ms/step - loss: 0.0061 - val_loss: 0.0203\n",
      "Epoch 44/50\n",
      "618/618 [==============================] - 2s 4ms/step - loss: 0.0061 - val_loss: 0.0203\n",
      "Epoch 45/50\n",
      "618/618 [==============================] - 2s 4ms/step - loss: 0.0061 - val_loss: 0.0203\n",
      "Epoch 46/50\n",
      "618/618 [==============================] - 2s 4ms/step - loss: 0.0061 - val_loss: 0.0203\n",
      "Epoch 47/50\n",
      "618/618 [==============================] - 2s 4ms/step - loss: 0.0061 - val_loss: 0.0203\n",
      "Epoch 48/50\n",
      "618/618 [==============================] - 2s 4ms/step - loss: 0.0062 - val_loss: 0.0203\n",
      "Epoch 49/50\n",
      "618/618 [==============================] - 2s 4ms/step - loss: 0.0061 - val_loss: 0.0203\n",
      "Epoch 50/50\n",
      "618/618 [==============================] - 2s 4ms/step - loss: 0.0062 - val_loss: 0.0203\n",
      "Epoch: 0, loss=387.535047, val_loss=1.869559\n",
      "Epoch: 1, loss=35.080703, val_loss=0.385769\n",
      "Epoch: 2, loss=6.610832, val_loss=0.074796\n",
      "Epoch: 3, loss=2.332759, val_loss=0.005220\n",
      "Epoch: 4, loss=2.328766, val_loss=0.132491\n",
      "Epoch: 5, loss=1.575624, val_loss=0.035892\n",
      "Epoch: 6, loss=2.997575, val_loss=0.005833\n",
      "Epoch: 7, loss=1.180974, val_loss=0.004478\n",
      "Epoch: 8, loss=0.200774, val_loss=0.006092\n",
      "Epoch: 9, loss=0.469035, val_loss=0.000891\n",
      "Epoch: 10, loss=0.369887, val_loss=0.005702\n",
      "Epoch: 11, loss=0.212025, val_loss=0.000753\n",
      "Epoch: 12, loss=0.109456, val_loss=0.000302\n",
      "Epoch: 13, loss=0.090139, val_loss=0.000115\n",
      "Epoch: 14, loss=0.619002, val_loss=0.000110\n",
      "Epoch: 15, loss=0.040684, val_loss=0.000010\n",
      "Epoch: 16, loss=0.713042, val_loss=0.002370\n",
      "Epoch: 17, loss=0.176644, val_loss=0.000053\n",
      "Epoch: 18, loss=0.080550, val_loss=0.000135\n",
      "Epoch: 19, loss=0.150026, val_loss=0.002303\n",
      "Epoch: 20, loss=2.929994, val_loss=0.048548\n",
      "Epoch: 21, loss=0.182803, val_loss=0.001832\n",
      "Epoch: 22, loss=0.046030, val_loss=0.000022\n",
      "Epoch: 23, loss=0.027211, val_loss=0.000436\n",
      "Epoch: 24, loss=0.043391, val_loss=0.011003\n",
      "Epoch: 25, loss=0.070552, val_loss=0.001399\n",
      "Epoch: 26, loss=0.061056, val_loss=0.002987\n",
      "Epoch: 27, loss=0.208045, val_loss=0.000048\n",
      "Epoch: 28, loss=0.104339, val_loss=0.000024\n",
      "Epoch: 29, loss=0.037077, val_loss=0.000021\n",
      "Epoch: 30, loss=0.078916, val_loss=0.074434\n",
      "Epoch: 31, loss=0.071785, val_loss=0.002270\n",
      "Epoch: 32, loss=0.691435, val_loss=0.061940\n",
      "Epoch: 33, loss=0.050481, val_loss=0.059337\n",
      "Epoch: 34, loss=0.047676, val_loss=0.019128\n",
      "Epoch: 35, loss=0.060718, val_loss=0.084093\n",
      "Epoch: 36, loss=0.024388, val_loss=0.162910\n",
      "Epoch: 37, loss=0.022503, val_loss=0.103088\n",
      "Epoch: 38, loss=0.015545, val_loss=0.293773\n",
      "Epoch: 39, loss=0.027643, val_loss=0.114082\n",
      "Epoch: 40, loss=0.021096, val_loss=0.130997\n",
      "Epoch: 41, loss=0.036906, val_loss=0.193611\n",
      "Epoch: 42, loss=0.035226, val_loss=0.136406\n",
      "Epoch: 43, loss=0.019939, val_loss=0.115749\n",
      "Epoch: 44, loss=0.017910, val_loss=0.112408\n",
      "Epoch: 45, loss=0.019862, val_loss=0.115531\n",
      "Epoch: 46, loss=0.030447, val_loss=0.125064\n",
      "Epoch: 47, loss=0.019110, val_loss=0.159251\n",
      "Epoch: 48, loss=0.024472, val_loss=0.092819\n",
      "Epoch: 49, loss=0.022965, val_loss=0.177838\n",
      "Epoch: 50, loss=0.027994, val_loss=0.118180\n",
      "Epoch: 51, loss=0.021711, val_loss=0.145098\n",
      "Epoch: 52, loss=0.030651, val_loss=0.147329\n",
      "Epoch: 53, loss=0.031799, val_loss=0.239985\n",
      "Epoch: 54, loss=0.033810, val_loss=0.115780\n",
      "Epoch: 55, loss=0.027047, val_loss=0.117788\n",
      "Epoch: 56, loss=0.022831, val_loss=0.288443\n",
      "Epoch: 57, loss=0.019331, val_loss=0.105485\n",
      "Epoch: 58, loss=0.023799, val_loss=0.133802\n",
      "Epoch: 59, loss=0.035927, val_loss=0.141073\n",
      "Epoch: 60, loss=0.018234, val_loss=0.121231\n",
      "Epoch: 61, loss=0.024164, val_loss=0.311031\n",
      "Epoch: 62, loss=0.020722, val_loss=0.122995\n",
      "Epoch: 63, loss=0.039378, val_loss=0.126225\n",
      "Epoch: 64, loss=0.021315, val_loss=0.109213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 65, loss=0.027122, val_loss=0.113087\n",
      "Epoch: 66, loss=0.015474, val_loss=0.097391\n",
      "Epoch: 67, loss=0.025710, val_loss=0.106015\n",
      "Epoch: 68, loss=0.033795, val_loss=0.150148\n",
      "Epoch: 69, loss=0.017013, val_loss=0.145170\n",
      "Epoch: 70, loss=0.034197, val_loss=0.122890\n",
      "Epoch: 71, loss=0.017697, val_loss=0.140515\n",
      "Epoch: 72, loss=0.013563, val_loss=0.141752\n",
      "Epoch: 73, loss=0.018711, val_loss=0.173549\n",
      "Epoch: 74, loss=0.023249, val_loss=0.110035\n",
      "Epoch: 75, loss=0.028166, val_loss=0.098514\n",
      "Epoch: 76, loss=0.026381, val_loss=0.127882\n",
      "Epoch: 77, loss=0.028938, val_loss=0.161364\n",
      "Epoch: 78, loss=0.024517, val_loss=0.165915\n",
      "Epoch: 79, loss=0.036589, val_loss=0.095407\n",
      "Epoch: 80, loss=0.024128, val_loss=0.123871\n",
      "Epoch: 81, loss=0.017409, val_loss=0.144919\n",
      "Epoch: 82, loss=0.030628, val_loss=0.134340\n",
      "Epoch: 83, loss=0.025710, val_loss=0.132847\n",
      "Epoch: 84, loss=0.020977, val_loss=0.096508\n",
      "Epoch: 85, loss=0.028765, val_loss=0.124472\n",
      "Epoch: 86, loss=0.020990, val_loss=0.096492\n",
      "Epoch: 87, loss=0.017053, val_loss=0.159050\n",
      "Epoch: 88, loss=0.038590, val_loss=0.146337\n",
      "Epoch: 89, loss=0.024932, val_loss=0.199331\n",
      "Epoch: 90, loss=0.014641, val_loss=0.126034\n",
      "Epoch: 91, loss=0.017024, val_loss=0.113311\n",
      "Epoch: 92, loss=0.033657, val_loss=0.162380\n",
      "Epoch: 93, loss=0.026861, val_loss=0.109972\n",
      "Epoch: 94, loss=0.014681, val_loss=0.120790\n",
      "Epoch: 95, loss=0.019326, val_loss=0.103889\n",
      "Epoch: 96, loss=0.018767, val_loss=0.140860\n",
      "Epoch: 97, loss=0.020035, val_loss=0.096096\n",
      "Epoch: 98, loss=0.022341, val_loss=0.117440\n",
      "Epoch: 99, loss=0.016494, val_loss=0.136095\n",
      "Epoch: 100, loss=0.024446, val_loss=0.136480\n",
      "Epoch: 101, loss=0.023835, val_loss=0.109359\n",
      "Epoch: 102, loss=0.026666, val_loss=0.109963\n",
      "Epoch: 103, loss=0.021525, val_loss=0.097927\n",
      "Epoch: 104, loss=0.021580, val_loss=0.102207\n",
      "Epoch: 105, loss=0.021936, val_loss=0.104692\n",
      "Epoch: 106, loss=0.033011, val_loss=0.098678\n",
      "Epoch: 107, loss=0.015854, val_loss=0.124419\n",
      "Epoch: 108, loss=0.027115, val_loss=0.134706\n",
      "Epoch: 109, loss=0.028198, val_loss=0.113173\n",
      "Epoch: 110, loss=0.021482, val_loss=0.086945\n",
      "Epoch: 111, loss=0.033214, val_loss=0.096880\n",
      "Epoch: 112, loss=0.029875, val_loss=0.120080\n",
      "Epoch: 113, loss=0.028881, val_loss=0.131359\n",
      "Epoch: 114, loss=0.022792, val_loss=0.097740\n",
      "Epoch: 115, loss=0.029666, val_loss=0.113433\n",
      "Epoch: 116, loss=0.024848, val_loss=0.095853\n",
      "Epoch: 117, loss=0.020192, val_loss=0.099120\n",
      "Epoch: 118, loss=0.024571, val_loss=0.104607\n",
      "Epoch: 119, loss=0.027064, val_loss=0.120071\n",
      "STOPPING EARLY\n",
      "finished training\n",
      "confusion matrix: \n",
      "[[   0    0    0    0    0    0    0    0    0    0    0    0 2345    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    2    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0  371    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0 1625    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0 2103    2    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0  186    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0  536    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "   472    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    1    0    0    0 1453    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0 1190    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0 1495    0    0    0    0    1    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    4    0    0    0    0    4    0 1016    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     1    0    0    0    0    0    0    0    1    0    0  473    0    0]\n",
      " [   0    0    0    0    0    0    0    0    8    0    0    0    0    0\n",
      "     0  312    0    0    0    0    1    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0  531    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [ 705    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    3    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0  475    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    1    1    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0  698    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0  233    0    0    0    0    0    0    0    0    0    0    0\n",
      "     1    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    1    0    0    0    0    0    0    0    1    0    0  759\n",
      "     0    0    1    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0 4160    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0  955    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0 2149    0    0    0    0    0    0    6    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0  165    0    0    0    0    0    0    0    0    0\n",
      "     0    0  338    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    1    0    0    0  426    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    1    0    0    2 1159    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0  148    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0  141    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0  187    0    0    0    0    0\n",
      "     0    0    0    1    0    0    1    0    0  803    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0  222    0    0    0    0    0]]\n",
      "spectralNet accuracy: 0.905\n",
      "NMI: 0.965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Crimson_King\\anaconda3\\envs\\specNetGPU\\lib\\site-packages\\sklearn\\metrics\\cluster\\supervised.py:844: FutureWarning: The behavior of NMI will change in version 0.22. To match the behavior of 'v_measure_score', NMI will use average_method='arithmetic' by default.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "train_and_save(outName= 'testData/testNet', dataIn='testData/test_clusterBcov.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22000, 12) (22000,) (5877, 12) (5877,)\n",
      "WARNING: Using data provided in arguments. Must be tuple or array of format (x_train, x_test, y_train, y_test)\n",
      "computing k=2 nearest neighbors...\n",
      "creating pairs...\n",
      "ks 19800 2 2 2\n",
      "Iter: 0/19800\n",
      "Iter: 10000/19800\n",
      "computing k=2 nearest neighbors...\n",
      "creating pairs...\n",
      "ks 2200 2 2 2\n",
      "Iter: 0/2200\n",
      "Epoch 1/1\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.0150 - val_loss: 0.0328\n",
      "Epoch: 0, loss=419.167578, val_loss=2.478963\n",
      "finished training\n",
      "confusion matrix: \n",
      "[[2345    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    2    0    0  371    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0 1625    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    2    0    0 2103    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0  186    0    0    0    0    0]\n",
      " [ 536    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "   472    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    1    0    0    0    0    0    0 1453    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0 1190    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0 1495    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    1    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    4    0 1016    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    4    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0  473\n",
      "     1    0    0    0    0    0    0    0    1    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    8    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0  312    0    0    0    1    0    0]\n",
      " [   0    0    0    0    0    0  531    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0  705    0    0    3    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0  475    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    1    0    0    0    0    0  698    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    1    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     1    0    0    0    0    0    0    0    0  233    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0  759    0    0    1    1    0    0    1    0    0    0    0]\n",
      " [   0 4160    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0  955    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0 2149    0    0    6    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0  338    0    0    0  165    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    1    0    0    0  426    0    0    0    0    0    0    0    0]\n",
      " [   0    0 1159    0    0    0    0    0    0    0    0    0    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    2    0    0]\n",
      " [   0    0    0  148    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0  141    0    0]\n",
      " [   0    0    0    0    0    0    0    0  990    0    0    0    1    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    1    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0  222    0    0    0    0    0]]\n",
      "spectralNet accuracy: 0.905\n"
     ]
    }
   ],
   "source": [
    "def load_and_predict(out_name='specNet_predicted',refData='data/refData.npz', specWeightsPath='data/SpecNet_bCov4H_cluster26_NN10_weights_jul1.tf', \n",
    "                siamWeightsPath='data/Siamese_SpecNet_bCov4H_cluster26_NN10_weights_jul1.tf'):\n",
    "    #testData used in the training, needs to be reloaded for prediction of clusters,for re-predicting\n",
    "    #original clusters to maintain consistent cluster numbers\n",
    "    #Warning! test Train splits hard coded here for original data\n",
    "    \n",
    "    # tf.test.is_gpu_available()\n",
    "    rr = np.load(refData, allow_pickle=True)\n",
    "    #X_train data is the feature for spectral clustering(Midpoint distance + dihedral angles between helices)\n",
    "    #16 total, eight each\n",
    "    #y_name is the name of the protein\n",
    "    #y_ is the assigned cluster labels from real spectral clustering\n",
    "    y_start , y_name_start, X_train_start, featNames  = [rr[f] for f in rr.files]\n",
    "\n",
    "    X_train_start = X_train_start[:,:-8] #remove phi values and length\n",
    "\n",
    "\n",
    "    X_test = X_train_start[22000:,:]\n",
    "    y_test = y_start[22000:]\n",
    "\n",
    "    X_train = X_train_start[:22000,:]\n",
    "    y_train = y_start[:22000]\n",
    "\n",
    "    #run this to organize the data into the approriate dictionary formats\n",
    "    print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "    new_dataset_data = (X_train, X_test, y_train, y_test)\n",
    "    ata = get_data(params,new_dataset_data)\n",
    "    #due to the special layers and old format, remake the network per the regular training code\n",
    "    #and then load the weights after one epoch of training\n",
    "    km, siamese_net_model, spectral_net_model = spectralNet_FromWeights(ata, params,siamWeightsPath,\n",
    "                                                                       specWeightsPath)\n",
    "    \n",
    "    #load data to predict\n",
    "    direc = 'data/'\n",
    "    name = 'to_predict'\n",
    "    rr = np.load(f'{direc}{name}.npz', allow_pickle=True)\n",
    "    data = [rr[f] for f in rr.files]\n",
    "    \n",
    "    #predict and assign clusters for new data\n",
    "    x_spec = spectral_net_model.predict(data[0])\n",
    "    clusters_assignments = km.predict(x_spec)\n",
    "    \n",
    "    #save the data to give back to clustering class (new python environment easier to use)\n",
    "    np.savez_compressed(f'{direc}{out_name}.npz',data = clusters_assignments)\n",
    "    \n",
    "    #repredict the inital data to get consistent cluster numbers \n",
    "    #original dataset assignments\n",
    "\n",
    "    x_spec_orig = spectral_net_model.predict(X_train_start)\n",
    "    cluster_assignments_orig = km.predict(x_spec_orig)\n",
    "\n",
    "    np.savez_compressed(f'{direc}{out_name}_original_clusters.npz', data=cluster_assignments_orig)\n",
    "\n",
    "\n",
    "load_and_predict(refData='testData/test_clusterBcov.npz',siamWeightsPath='testData/testNet_siamese_net.tf',\n",
    "                specWeightsPath='testData/testNet_spectral_net.tf')\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the data is feed to siamese net to predict neighbors\n",
    "#prediction is then used by spectral net to predict graph laplacian embeddings?\n",
    "#kmeans organize the data in the graph laplacian space for ideal clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22000, 12) (22000,) (5877, 12) (5877,)\n",
      "WARNING: Using data provided in arguments. Must be tuple or array of format (x_train, x_test, y_train, y_test)\n",
      "computing k=2 nearest neighbors...\n",
      "creating pairs...\n",
      "ks 19800 2 2 2\n",
      "Iter: 0/19800\n",
      "Iter: 10000/19800\n",
      "computing k=2 nearest neighbors...\n",
      "creating pairs...\n",
      "ks 2200 2 2 2\n",
      "Iter: 0/2200\n"
     ]
    }
   ],
   "source": [
    "#testData for training\n",
    "# tf.test.is_gpu_available()\n",
    "rr = np.load('data/refData.npz', allow_pickle=True)\n",
    "#X_train data is the feature for spectral clustering(Midpoint distance + dihedral angles between helices)\n",
    "#16 total, eight each\n",
    "#y_name is the name of the protein\n",
    "#y_ is the assigned cluster labels from real spectral clustering\n",
    "y_start , y_name_start, X_train_start, featNames  = [rr[f] for f in rr.files]\n",
    "\n",
    "X_train_start = X_train_start[:,:-8] #remove phi values and length\n",
    "\n",
    "\n",
    "X_test = X_train_start[22000:,:]\n",
    "y_test = y_start[22000:]\n",
    "\n",
    "X_train = X_train_start[:22000,:]\n",
    "y_train = y_start[:22000]\n",
    "\n",
    "#run this to organize the data into the approriate dictionary formats\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "new_dataset_data = (X_train, X_test, y_train, y_test)\n",
    "ata = get_data(params,new_dataset_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train net with this code\n",
    "#siamese_net_model, spectral_net_model, x_spectralnet, y_spectralnet,km = run_net(ata, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the weights for loading by spectralNet_FromWeights\n",
    "# spectral_net_model.net.save_weights('data/SpecNet_bCov4H_cluster26_NN10_weights_jul1.tf')\n",
    "# siamese_net_model.net.save_weights('data/Siamese_SpecNet_bCov4H_cluster26_NN10_weights_jul1.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0813 00:27:16.303197 10756 deprecation_wrapper.py:119] From C:\\Users\\Crimson_King\\anaconda3\\envs\\specNetGPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:488: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0813 00:27:16.305198 10756 deprecation_wrapper.py:119] From C:\\Users\\Crimson_King\\anaconda3\\envs\\specNetGPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:63: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0813 00:27:16.306198 10756 deprecation_wrapper.py:119] From C:\\Users\\Crimson_King\\anaconda3\\envs\\specNetGPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3626: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0813 00:27:16.344207 10756 deprecation.py:506] From C:\\Users\\Crimson_King\\anaconda3\\envs\\specNetGPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1238: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "W0813 00:27:16.349208 10756 deprecation_wrapper.py:119] From C:\\Users\\Crimson_King\\anaconda3\\envs\\specNetGPU\\lib\\site-packages\\keras\\optimizers.py:711: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0813 00:27:16.405760 10756 deprecation.py:323] From C:\\Users\\Crimson_King\\anaconda3\\envs\\specNetGPU\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0813 00:27:16.540569 10756 deprecation.py:506] From C:\\Users\\Crimson_King\\anaconda3\\envs\\specNetGPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:671: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0813 00:27:16.554572 10756 deprecation_wrapper.py:119] From C:\\Users\\Crimson_King\\anaconda3\\envs\\specNetGPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:949: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0813 00:27:16.557573 10756 deprecation_wrapper.py:119] From C:\\Users\\Crimson_King\\anaconda3\\envs\\specNetGPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:936: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "618/618 [==============================] - 3s 5ms/step - loss: 0.0153 - val_loss: 0.0238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0813 00:27:19.672891 10756 deprecation_wrapper.py:119] From SpectralNet-master/src\\core\\networks.py:137: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
      "\n",
      "W0813 00:27:19.766862 10756 deprecation.py:506] From C:\\Users\\Crimson_King\\anaconda3\\envs\\specNetGPU\\lib\\site-packages\\tensorflow\\python\\training\\rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss=444.169343, val_loss=2.219822\n",
      "finished training\n",
      "confusion matrix: \n",
      "[[   0 2345    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    1    0  471    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0  536    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    2    0    0    0    0    0 2103    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0  955    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0 1625    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    1 1453    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0 1190    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0 4159    0    0    0    0    0    1    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    1    0    0\n",
      "     0    0    0    0    0    0  312    0    0    0    8    0]\n",
      " [   0    0    0    0 1495    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0  698    0    0    0    1    0    0    0    0\n",
      "     0    0    0    0    1    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    3  705    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0  186    0\n",
      "     0    0    0    0    0    0    0    0    0    1    0    0]\n",
      " [1016    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    4    0    0    0    4    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "   531    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    2    0  371    0    0    0    0    0    0    0    0]\n",
      " [   0    0    1    0    0    0    0    0    0    0    0    2    0 1159\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0  759    0    0    0    0    0    0\n",
      "     0    0    1    0    1    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0 2149    0    0    0    0    1    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    5    0]\n",
      " [   0    0    1    0    0    0    0    0    0    0    0  804    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0  187    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0  503    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    1    0\n",
      "     0    0    0    0    0    0    0  473    0  222    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    2    0\n",
      "     0    1  660    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0  141    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0  148]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0  475    0    0    0    0    0    0    0    0    0    0]]\n",
      "spectralNet accuracy: 0.953\n"
     ]
    }
   ],
   "source": [
    "#due to the special layers and old format, remake the network per the regular training code\n",
    "#\n",
    "specWeightsPath='data/SpecNet_bCov4H_cluster26_NN10_weights_jul1.tf'\n",
    "siamWeightsPath='data/Siamese_SpecNet_bCov4H_cluster26_NN10_weights_jul1.tf'\n",
    "\n",
    "km, siamese_net_model, spectral_net_model = spectralNet_FromWeights(ata, params,siamWeightsPath,\n",
    "                                                                   specWeightsPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data to predict\n",
    "direc = 'data/'\n",
    "name = 'to_predict'\n",
    "rr = np.load(f'{direc}{name}.npz', allow_pickle=True)\n",
    "data = [rr[f] for f in rr.files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict and assign clusters for new data\n",
    "x_spec = spectral_net_model.predict(data[0])\n",
    "clusters_assignments = km.predict(x_spec)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the data to give back to clustering class (new python environment easier to use)\n",
    "name='clusters_specNet'\n",
    "np.savez_compressed(f'{direc}{name}.npz',data = clusters_assignments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#repredict the inital data to get consistent cluster numbers \n",
    "#original dataset assignments\n",
    "\n",
    "x_spec_orig = spectral_net_model.predict(X_train_start)\n",
    "cluster_assignments_orig = km.predict(x_spec_orig)\n",
    "\n",
    "name_orig = 'original_clusters'\n",
    "\n",
    "np.savez_compressed(f'{direc}{name_orig}.npz', data=cluster_assignments_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9555895865237366"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confirm the accuracy of the loaded\n",
    "x_spectralnet = spectral_net_model.predict(X_test)\n",
    "kmeans_assignments = km.predict(x_spectralnet)\n",
    "y_spectralnet, _ = get_y_preds(kmeans_assignments, y_test, params['n_clusters'])\n",
    "\n",
    "y_pred, confusion_matrix = get_y_preds(kmeans_assignments, y_test,params['n_clusters'])\n",
    "# calculate the accuracy\n",
    "np.mean(y_pred == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "specNetGPU",
   "language": "python",
   "name": "specnetgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
