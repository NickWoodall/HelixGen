{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "032ba86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "#clean up these imports for unused later\n",
    "from math import cos,sin,tan,asin,acos,radians,sqrt,degrees,atan,atan2,copysign\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "import scipy\n",
    "from scipy.stats import norm\n",
    "import random\n",
    "import time\n",
    "import timeit\n",
    "import math\n",
    "import localization as lx\n",
    "import gzip\n",
    "\n",
    "import util.npose_util as nu\n",
    "import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "\n",
    "import joblib\n",
    "from sklearn.manifold import MDS\n",
    "import argparse\n",
    "from functools import partial\n",
    "from itertools import starmap,repeat,permutations,combinations\n",
    "\n",
    "from pymol import cmd, stored, selector\n",
    "\n",
    "import GenerateEndpoints as ge\n",
    "import HelixFit as hf\n",
    "import FitTransform as ft\n",
    "\n",
    "import seaborn as sns\n",
    "import util.RotationMethods as rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9443162e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device name CPU\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "if tf.config.list_physical_devices('GPU'):\n",
    "    device_name = tf.test.gpu_device_name()\n",
    "else:\n",
    "    device_name = 'CPU'\n",
    "rate=0.05\n",
    "# if ~devtype.__eq__(device_name):\n",
    "# device_name = 'CPU'\n",
    "print(f'device name {device_name}')\n",
    "\n",
    "gen=\"data/BestGenerator\"\n",
    "\n",
    "\n",
    "\n",
    "with tf.device(device_name):\n",
    "    gen_obj = ge.BatchRecon(gen)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=rate)\n",
    "output1=gen_obj.generate(z=12,batch_size=12) #example generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccc36150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_generated_to_starting_ep(gen_ep, orig_ep, target_mp=None):\n",
    "    \"\"\"Uses Kabsh to align generated endpoints onto original endpoints. Orig_Ep on origin with oneRef.\"\"\"\n",
    "    #Thanks to below for this code; modified to batch form\n",
    "    #moves gen_\n",
    "    #https://pymolwiki.org/index.php/Kabsch\n",
    "\n",
    "    #only center on first four points [first two helices]\n",
    "    gen_ep_4 = gen_ep[:,:4,:].copy()\n",
    "    orig_ep_4 = orig_ep[:,:4,:].copy()\n",
    "    \n",
    "    \n",
    "    #centering to prevent affine transformaiton\n",
    "    COM_orig = np.expand_dims(np.sum(orig_ep_4, axis=1)/orig_ep_4.shape[1]   ,axis=1)\n",
    "    COM_gen =  np.expand_dims(np.sum(gen_ep_4,  axis=1)/gen_ep_4.shape[1]     ,axis=1)\n",
    "    \n",
    "    gen_ep_4_cen = gen_ep_4 - COM_gen\n",
    "    orig_ep_4_cen = orig_ep_4 - COM_orig\n",
    "    \n",
    "    #initial error estimate\n",
    "    #E0 = np.sum( np.sum(np.square(gen_ep_4),axis=1),axis=1) + np.sum( np.sum(np.square(orig_ep_4),axis=1),axis=1)\n",
    "\n",
    "    # This beautiful step provides the answer.  V and Wt are the orthonormal\n",
    "    # bases that when multiplied by each other give us the rotation matrix, U.\n",
    "    # S, (Sigma, from SVD) provides us with the error!  Isn't SVD great!                                            #2                      #1\n",
    "    V, S, Wt = np.linalg.svd( np.matmul(np.transpose(gen_ep_4_cen, axes=[0,2,1]), orig_ep_4_cen))\n",
    "\n",
    "    # we already have our solution, in the results from SVD.\n",
    "    # we just need to check for reflections and then produce\n",
    "    # the rotation.  V and Wt are orthonormal, so their det's\n",
    "    # are +/-1.\n",
    "    reflect = np.linalg.det(V) * np.linalg.det(Wt)\n",
    "    #original solution, I will take both reflections\n",
    "    #multiples by 1 or -1 depending if relfect is negative (reflection)\n",
    "    proper_reflection = ((reflect>0).astype(np.int32)*-2+1) \n",
    "    S[:,-1] = S[:,-1]*proper_reflection\n",
    "    V[:,:,-1] = -V[:,:,-1]*proper_reflection.reshape((-1,1))\n",
    "\n",
    "    V_reflect = V.copy()\n",
    "    V_reflect[:,:,-1] = -V_reflect[:,:,-1]\n",
    "    #Error\n",
    "#     RMSD = E0 - (2.0 * np.sum(S,axis=1))\n",
    "#     RMSD = np.sqrt(np.abs(RMSD / L))\n",
    "    \n",
    "    #generate rotation matrices\n",
    "    U = np.matmul(V, Wt)\n",
    "    U_reflect = np.matmul(V_reflect, Wt)\n",
    "\n",
    "#    return target_mp,COM_gen,COM_orig\n",
    "    if target_mp:\n",
    "        final_target_midpoint = np.matmul(np.expand_dims((target_mp-COM_gen.squeeze(axis=1)),axis=1), U).squeeze() + COM_orig.squeeze(axis=1)\n",
    "    final_ep_full         = np.matmul((gen_ep-COM_gen), U) + COM_orig\n",
    "    final_ep_full_reflect = np.matmul((gen_ep-COM_gen), U_reflect) + COM_orig\n",
    "    \n",
    "    if target_mp:\n",
    "        return final_ep_full, final_ep_full_reflect, final_target_midpoint\n",
    "    else:\n",
    "        return final_ep_full, final_ep_full_reflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c996cf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dmap = output1\n",
    "batch_size = dmap.shape[0]\n",
    "\n",
    "#alternately\n",
    "#for x in combinations(8,3):\n",
    "base_tri_ind = np.zeros((56,3),dtype=np.int32) #8 choose 3 size\n",
    "ind = 0 \n",
    "for x in combinations(range(8),3):\n",
    "    base_tri_ind[ind] = np.array(x)\n",
    "    ind+=1 \n",
    "    \n",
    "base_tri_ind = np.zeros((6,3),dtype=np.int32) # test size\n",
    "for x in range(9-3):\n",
    "    base_tri_ind[x] = np.array(range(x,x+3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d2da679",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = np.zeros((base_tri_ind.shape[0],5),np.int32) # 8-3 = 5 points not base triangle\n",
    "for i,x in enumerate(base_tri_ind):\n",
    "    tp[i] = np.array(list(set.difference(set(range(8)),set(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9e7b6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_dindex_batch(target_points,base_tri_ind,num_helices = 4):\n",
    "    \n",
    "    num_ep = num_helices*2\n",
    "    mat_ind = np.array(range((num_ep)**2)).reshape((num_ep,num_ep))\n",
    "    iu1 = np.triu_indices(num_ep, 1)\n",
    "    \n",
    "    di =  []\n",
    "    for i,tps in enumerate(target_points):\n",
    "        di.append([])\n",
    "        for j, tpst in enumerate(tps):\n",
    "            if tpst<base_tri_ind[i][0]:\n",
    "                di[i].append(mat_ind[tpst, base_tri_ind[i]])\n",
    "            else:\n",
    "                di[i].append(mat_ind[base_tri_ind[i], tpst])\n",
    "\n",
    "    di = np.array(di)\n",
    "    \n",
    "    out_ind = []\n",
    "    for i in range(len(di)):\n",
    "        out_ind.append([])\n",
    "        for j in di[i].flatten():\n",
    "            out_ind[i].append(np.nonzero(mat_ind[iu1]==j))\n",
    "            \n",
    "            \n",
    "    base_tri = []\n",
    "    for i,c in enumerate(base_tri_ind):\n",
    "        base_tri.append([])\n",
    "        base_tri[i] = [mat_ind[c[0],c[1]], mat_ind[c[1],c[2]], mat_ind[c[0],c[2]]]\n",
    "\n",
    "\n",
    "    out_ind = np.array(out_ind)\n",
    "    return out_ind.reshape((base_tri_ind.shape[0],-1,base_tri_ind.shape[1])), np.array(base_tri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09cc258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dMat_to_iu1_index(indices_in, num_helices = 4):\n",
    "    \"\"\"Converts indices on flattened distance index to iu1 single indices\"\"\"\n",
    "    \n",
    "    \n",
    "    conv_array = np.array(indices_in).flatten()\n",
    "    \n",
    "    num_ep = num_helices*2\n",
    "    mat_ind = np.array(range((num_ep)**2)).reshape((num_ep,num_ep))\n",
    "    iu1 = np.triu_indices(num_ep, 1)\n",
    "    \n",
    "    #convert to generator indices (indices of iu1 array)\n",
    "    out_ind = []\n",
    "    for x in conv_array:\n",
    "        if len(np.nonzero(mat_ind[iu1]==x)[0])>0:\n",
    "            out_ind.append(np.nonzero(mat_ind[iu1]==x))\n",
    "        else:\n",
    "            print(x)\n",
    "            \n",
    "    out_ind = np.array(out_ind)\n",
    "        \n",
    "    return out_ind.reshape(conv_array.shape)\n",
    "\n",
    "def point_dindex_batch(target_points, ref, num_helices = 4):\n",
    "    \n",
    "    num_ep = num_helices*2\n",
    "    mat_ind = np.array(range((num_ep)**2)).reshape((num_ep,num_ep))\n",
    "    iu1 = np.triu_indices(num_ep, 1)\n",
    "    \n",
    "    dindex = []\n",
    "    \n",
    "    for b,tgp in enumerate(target_points):\n",
    "        dindex.append([])\n",
    "        for tp in tgp:\n",
    "                if ref[b]<tp:\n",
    "                    dindex[b].append(mat_ind[ref[b],tp])   #indices for distances to target point\n",
    "                else:\n",
    "                    dindex[b].append( mat_ind[tp,ref[b]])\n",
    "                    print(mat_ind[tp,ref[b]],tp,ref[b])\n",
    "    \n",
    "    dindex = np.array(dindex)\n",
    "    \n",
    "    out_ind = []\n",
    "    for x in dindex.flatten():\n",
    "        out_ind.append(np.nonzero(mat_ind[iu1]==x))\n",
    "        \n",
    "    out_ind = np.array(out_ind)\n",
    "    \n",
    "    return out_ind.reshape((len(target_points),-1))\n",
    "\n",
    "def prep_base_triangle_trilateriation_batch(dindex, base_tri, distance_map):\n",
    "    \"\"\"Return x,y,z coords on z-plane of base triangle of tetrahedron from a distance map.\"\"\"\n",
    "    desired_dm = distance_map[:, base_tri]\n",
    "    dvar_index = tf.convert_to_tensor(0 ,dtype=tf.int32)\n",
    "    s2_index = tf.convert_to_tensor(2 ,dtype=tf.int32) # we would like the angle across from side 2\n",
    "    s3_index = tf.convert_to_tensor(1 ,dtype=tf.int32)\n",
    "    \n",
    "    dvar = tf.reshape(tf.gather(desired_dm, dvar_index,axis=2),(-1,base_tri.shape[0],1)) #side 1\n",
    "    s2 = tf.reshape(tf.gather(desired_dm,   s2_index,axis=2),(-1,base_tri.shape[0],1))\n",
    "    s3 = tf.reshape(tf.gather(desired_dm,   s3_index,axis=2),(-1,base_tri.shape[0],1))\n",
    "    \n",
    "    #calculate the opposite angle of the the third side of base triangle using law of cosines\n",
    "    s1sq = tf.square(dvar)\n",
    "    s2sq = tf.square(s2)\n",
    "    s3sq = tf.square(s3)\n",
    "    ang3 = np.arccos((-s3sq+s2sq+s1sq)/(2*dvar*s2))\n",
    "    \n",
    "    #take third point of base triangle via distance * vector\n",
    "    v13 = tf.concat([tf.cos(ang3), tf.sin(ang3), tf.zeros_like(ang3)], axis=2)\n",
    "    p3 = s2*v13\n",
    "    #center points of 3rd sphere\n",
    "    ivar = tf.reshape(p3[...,0],(p3.shape[0],-1,1))\n",
    "    jvar = tf.reshape(p3[...,1],(p3.shape[0],-1,1))\n",
    "\n",
    "\n",
    "    #convert all to float32 to match generator output\n",
    "    #expand to dindex size \n",
    "\n",
    "    dvar = tf.cast(tf.repeat(dvar,dindex.shape[1],axis=2),dtype=tf.float32)\n",
    "    ivar = tf.cast(tf.repeat(ivar,dindex.shape[1],axis=2),dtype=tf.float32)\n",
    "    jvar = tf.cast(tf.repeat(jvar,dindex.shape[1],axis=2),dtype=tf.float32)\n",
    "    return dvar,ivar,jvar\n",
    "\n",
    "def pointCalc_batch(g1, dvar, ivar, jvar, dindex, pindex, batch_size):\n",
    "    \"\"\"Loss function to move output of two generated helices to target midpoint\"\"\"\n",
    "    #now using dindex gather the desired indices for tetrahedron calcs\n",
    "    #radius of the spheres, aka the distances to unmasked endpoints\n",
    "    g2 = tf.gather(g1,dindex,axis=1)\n",
    "\n",
    "    #transpose lets you easily grab all distances with gather/axis \n",
    "    conv_dist_squared = tf.transpose(tf.square(g2),perm=[0, 1, 3, 2]) \n",
    "\n",
    "    r1_sq = tf.gather(conv_dist_squared, 0, axis=2) \n",
    "    r2_sq =  tf.gather(conv_dist_squared,1, axis=2) \n",
    "    r3_sq = tf.gather(conv_dist_squared, 2, axis=2)\n",
    "    \n",
    "    #calculate coordinates of spherial intersect\n",
    "    x = tf.divide(tf.add(tf.subtract(r1_sq,r2_sq),tf.square(dvar)),tf.multiply(2.0,dvar))\n",
    "    y1 = tf.divide(tf.add(tf.add(tf.subtract(r1_sq,r3_sq), tf.square(ivar)), tf.square(jvar)),tf.multiply(2.0,jvar))\n",
    "    y = tf.subtract(y1,tf.multiply(tf.divide(ivar,jvar),x))\n",
    "\n",
    "    pre_z = tf.subtract(tf.subtract(r1_sq,tf.square(x)),tf.square(y))\n",
    "    fixed_z = tf.clip_by_value(pre_z, 1e-10, 100)\n",
    "\n",
    "    #adds  to negative values to 0 for sqrt,\n",
    "    #I think is okay as zero z will imply lengthening of distances to match a non-zero target midpoint,\n",
    "    #pushing the network in the desired direction?\n",
    "\n",
    "    z = tf.sqrt(fixed_z) #assume positive solution\n",
    "    z_neg = tf.multiply(z,-1) #assume negative soluation\n",
    "\n",
    "    #new points, with both assumptions\n",
    "    nwp = tf.concat((tf.reshape(x,(batch_size,dindex.shape[0],-1,1)),\n",
    "                    tf.reshape(y,(batch_size,dindex.shape[0],-1,1)),\n",
    "                    tf.reshape(z,(batch_size,dindex.shape[0],-1,1))), axis=3)  #\n",
    "\n",
    "    nwp_negz = tf.concat((tf.reshape(x,(batch_size,dindex.shape[0],-1,1)),\n",
    "                    tf.reshape(y,(batch_size,dindex.shape[0],-1,1)),\n",
    "                    tf.reshape(z_neg,(batch_size,dindex.shape[0],-1,1))), axis=3)  #\n",
    "    #distance calc all points, 5 points, 1 ref\n",
    "    nwp_p =  tf.expand_dims(nwp,axis=2) - tf.expand_dims(nwp,axis=3)\n",
    "    nwp_n =  tf.expand_dims(nwp,axis=2) - tf.expand_dims(nwp_negz,axis=3)\n",
    "\n",
    "    nwp_dist_pz = tf.reshape(tf.sqrt(tf.reduce_sum(tf.square(nwp_p), 4)),(batch_size,-1,dindex.shape[1],dindex.shape[1])) #distance calc \n",
    "    nwp_dist_nz = tf.reshape(tf.sqrt(tf.reduce_sum(tf.square(nwp_n), 4)),(batch_size,-1,dindex.shape[1],dindex.shape[1]))  #distance calc\n",
    "    \n",
    "    nwp_dist_pz_c = tf.squeeze(tf.gather(tf.gather(nwp_dist_pz, [0], axis=2), [1,2,3,4], axis=3))\n",
    "    nwp_dist_nz_c  = tf.squeeze(tf.gather(tf.gather(nwp_dist_nz, [0], axis=2), [1,2,3,4], axis=3))\n",
    "    \n",
    "    nwp_dist_pz_c = tf.expand_dims(nwp_dist_pz_c,axis=3)\n",
    "    nwp_dist_nz_c = tf.expand_dims(nwp_dist_nz_c,axis=3)\n",
    "    #take the distances to compare \n",
    "    z_pn_dist = tf.expand_dims(tf.gather(g1,pindex,axis=1),axis=3)\n",
    "    correct_z_assum = tf.abs(z_pn_dist - nwp_dist_nz_c) < tf.abs(z_pn_dist - nwp_dist_pz_c)\n",
    "    #use true false to turn into -1, 1 for multiplying z\n",
    "    cz = tf.multiply(tf.cast(correct_z_assum,tf.int32),-2)\n",
    "    z_reflect_tensor = tf.ones_like(nwp, dtype=tf.int32)\n",
    "    \n",
    "    #awkwardly stack back into same size vector\n",
    "    ze = tf.repeat(tf.zeros_like(cz),2,axis=3)\n",
    "    cz2 = tf.concat((ze,cz),axis=3)\n",
    "    \n",
    "    zztop = tf.expand_dims(tf.zeros_like(z_reflect_tensor)[:,:,0,:],axis=2)\n",
    "    cz_fin = tf.concat((zztop,cz2),axis=2)\n",
    "    \n",
    "    #multiply to get correct z, by distance\n",
    "    nwp_mult = tf.cast(tf.add(z_reflect_tensor,cz_fin),dtype=tf.float32)\n",
    "    nwp_final = tf.multiply(nwp_mult,nwp)\n",
    "    \n",
    "    return nwp_final\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fb0935a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dmap = output1\n",
    "batch_size = dmap.shape[0]\n",
    "\n",
    "#establish indices for distances to reference\n",
    "#prep base triangle, \n",
    "#one ref is [1,2,3] base tri indexing, zero ref is [0,1,2] base tri indexing for trilateration\n",
    "dindex, base_tri = target_dindex_batch(tp, base_tri_ind, num_helices = 4)\n",
    "base_tri = convert_dMat_to_iu1_index(base_tri).reshape((base_tri.shape[0],-1))\n",
    "pindex = point_dindex_batch(tp[:,1:], ref=tp[:,0], num_helices = 4)\n",
    "\n",
    "\n",
    "#base_tri.shape = number of desired base triangles / 3 distances define the triangle\n",
    "#dindex.shape   = number of desired base triangles, 5 possible end tetrahedron vertices per base triangle (4helix assum)\n",
    "#                 number  distances from the base triangle to the desired endpoints (3)\n",
    "#pindex.shape   = number of desired base triangle, distances from a reference point to all other points\n",
    "#                 for determining if +/-z for trilateration\n",
    "dvar, ivar, jvar = prep_base_triangle_trilateriation_batch(dindex, base_tri, dmap)\n",
    "dvar = dvar.numpy()\n",
    "ivar = ivar.numpy()\n",
    "jvar = jvar.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8662c9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_new = pointCalc_batch(output1,dvar,ivar, jvar, dindex, pindex, batch_size)\n",
    "final_new = final_new.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cc39924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 6, 5, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f360cf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = np.zeros((batch_size,base_tri_ind.shape[0],8,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98b266e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 6, 8, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27e0c1af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [1, 2, 3],\n",
       "       [2, 3, 4],\n",
       "       [3, 4, 5],\n",
       "       [4, 5, 6],\n",
       "       [5, 6, 7]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_tri_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a2776ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#index array size = input value size for dvar, ivar, jvar \n",
    "#these have already been expanded for batch, last dimension is repeat\n",
    "batch_array = np.repeat(np.arange(dvar.shape[0]),dvar.shape[1])\n",
    "base_tri_array = np.repeat(np.arange(dvar.shape[1])[np.newaxis,...], batch_size,axis=0).flatten()\n",
    "\n",
    "ep_array_dvar = np.repeat(base_tri_ind[:,1][np.newaxis,...], batch_size,axis=0).flatten()\n",
    "ep_array_ijvar = np.repeat(base_tri_ind[:,2][np.newaxis,...], batch_size,axis=0).flatten()\n",
    "\n",
    "zero_array = np.zeros(dvar.shape[0]*dvar.shape[1],dtype=np.int32)\n",
    "one_array = np.ones_like(zero_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9396e399",
   "metadata": {},
   "outputs": [],
   "source": [
    "#index array size = input value size for tp, \n",
    "batch_array_tp = np.repeat(np.arange(batch_size),tp.shape[1]*tp.shape[0])\n",
    "base_tri_tp_array = np.repeat(np.arange(tp.shape[0])[np.newaxis,...], batch_size*tp.shape[1],axis=0).flatten()\n",
    "\n",
    "ep_array_tp = np.repeat(tp[np.newaxis,...],batch_size,axis=0).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ac9567f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final[batch_array_tp,base_tri_tp_array,ep_array_tp] = final_new.reshape((-1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "619607b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 6, 8, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a284765e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(final[0])):\n",
    "    hf.HelicalProtein.makePointPDB(final[0][x],f'ref{x}.pdb', outDirec='output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18045235",
   "metadata": {},
   "outputs": [],
   "source": [
    "ep2=gen_obj.MDS_reconstruct_()\n",
    "ep2=np.array(ep2)\n",
    "outep = align_generated_to_starting_ep(final[:,3], final[:,2])[0]\n",
    "outep2 = align_generated_to_starting_ep(ep2, final[:,2])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2961b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf.HelicalProtein.makePointPDB(final[:,0][0],'ref.pdb', outDirec='output/')\n",
    "hf.HelicalProtein.makePointPDB(outep[0],'align.pdb', outDirec='output/')\n",
    "hf.HelicalProtein.makePointPDB(outep2[0],'align_MDS.pdb', outDirec='output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094e83f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15daf25a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "76f97295",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2b3ce1a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5abe412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bb6280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fe28e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "99a7935d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf.HelicalProtein.makePointPDB(outep[0],'align.pdb', outDirec='output/')\n",
    "hf.HelicalProtein.makePointPDB(zero_ref_points[0],'align2.pdb', outDirec='output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e7ad5a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ep2=gen_obj.MDS_reconstruct_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "571f3414",
   "metadata": {},
   "outputs": [],
   "source": [
    "ep2=np.array(ep2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "aaac8f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "outep2 = align_generated_to_starting_ep(ep2, zero_ref_points)[0]\n",
    "hf.HelicalProtein.makePointPDB(outep2[0],'align3.pdb', outDirec='output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "212c86e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_midpoint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-8c48eb67b86a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdX\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#+ 1e-6) #this dataset is good\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmp_01\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_midpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhelices_desired\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mmp_23\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_midpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhelices_desired\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#mp distance map\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_midpoint' is not defined"
     ]
    }
   ],
   "source": [
    "rr = np.load(f'data/ep_for_X.npz', allow_pickle=True)\n",
    "X = [rr[f] for f in rr.files][0]\n",
    "\n",
    "dX = np.expand_dims(X,axis=1) - np.expand_dims(X,axis=2)\n",
    "dist = np.sqrt(np.sum(dX**2, 3))  #+ 1e-6) #this dataset is good \n",
    "dist = dist.reshape((dist.shape[0],-1))\n",
    "mp_01 = get_midpoint(X,helices_desired=[0,1])\n",
    "mp_23 = get_midpoint(X,helices_desired=[2,3])\n",
    "#mp distance map\n",
    "ep_mp = np.hstack((X.reshape((-1,24)),mp_01,mp_23)).reshape(-1,10,3) #helix12mp=8  helix34mp=9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7288e629",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ep = [4,5,6,7,0]\n",
    "target_ep0 = [4,5,6,7,3]\n",
    "#batch_size = output1.shape[0]\n",
    "\n",
    "#establish indices for distances to reference\n",
    "#prep base triangle, convert distances from minmax to regular\n",
    "dindex, base_tri = target_dindex_pc(target_ep, oneRef = True, num_helices = 4)\n",
    "base_tri = convert_dMat_to_iu1_index(base_tri) #dirty\n",
    "pindex = point_dindex(target_ep[1:], ref=[target_ep[0]], num_helices = 4)\n",
    "\n",
    "#reference 0 [0,1,2] base tri of tetrahedron\n",
    "dindex0, base_tri0 = target_dindex_pc(target_ep0, oneRef = False, num_helices = 4)\n",
    "base_tri0 = convert_dMat_to_iu1_index(base_tri0) #dirty\n",
    "pindex0 = point_dindex(target_ep0[1:], ref=[target_ep0[0]], num_helices = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63d4f667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dindex.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "831e6ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "iu1 = np.triu_indices(8, 1)\n",
    "dist = dist.reshape((-1,8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3b88ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist0 = dist[0][iu1][np.newaxis,...]\n",
    "dist1 = dist[1][iu1][np.newaxis,...]\n",
    "\n",
    "tdest = np.concatenate((dist0,dist1),axis=0,dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf4da489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 28)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tdest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64f494ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dvar, ivar, jvar = prep_base_triangle_trilateriation(dindex, base_tri, tdest)\n",
    "dvar0, ivar0, jvar0 = prep_base_triangle_trilateriation(dindex0, base_tri0, tdest)\n",
    "\n",
    "dvar = dvar.numpy()\n",
    "ivar = ivar.numpy()\n",
    "jvar = jvar.numpy()\n",
    "dvar0 = dvar0.numpy()\n",
    "ivar0 = ivar0.numpy()\n",
    "jvar0 = jvar0.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c2b621f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.397503, 10.397503, 10.397503, 10.397503, 10.397503],\n",
       "       [11.03398 , 11.03398 , 11.03398 , 11.03398 , 11.03398 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa10affd",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_r_innerInd = np.repeat(tf.convert_to_tensor([[[1,2],[2,2],[3,2],[4,2]]]),tdest.shape[0],axis=0) #select all distances z for target ep\n",
    "#batch index\n",
    "zfi_bi =np.expand_dims(np.array(range(tdest.shape[0])).reshape((-1,1)).repeat(z_r_innerInd.shape[1],axis=1),axis=2)\n",
    "z_reflect_ind = np.concatenate((zfi_bi,z_r_innerInd),axis=2)\n",
    "zr_ind = z_reflect_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e57fbab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zr_ind.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64f1755a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEXT PLACE TO TROUBLESHOOT\n",
    "#one ref broken..... why?\n",
    "\n",
    "orp =  point_calc(tdest, 4, dvar, ivar, jvar, dindex, pindex, tdest.shape[0], z_reflect_ind)\n",
    "zrp =  point_calc(tdest, 4, dvar0, ivar0, jvar0, dindex0, pindex0, tdest.shape[0], z_reflect_ind)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "one_ref_points = np.zeros((tdest.shape[0],8,3))\n",
    "zero_ref_points = np.zeros((tdest.shape[0],8,3))\n",
    "\n",
    "# (0,0,0) (dvar,0,0) (ivar,jvar,0)\n",
    "one_ref_points[:,2,0] = dvar[:,0]\n",
    "zero_ref_points[:,1,0] = dvar0[:,0]\n",
    "\n",
    "one_ref_points[:,3,0] = ivar[:,0]\n",
    "zero_ref_points[:,2,0] = ivar0[:,0]\n",
    "\n",
    "one_ref_points[:,3,1] = jvar[:,0]\n",
    "zero_ref_points[:,2,1] = jvar0[:,0]\n",
    "\n",
    "one_ref_points[:,4:,:] = orp[:,:4,:]\n",
    "zero_ref_points[:,4:,:] = zrp[:,:4,:]\n",
    "\n",
    "one_ref_points[:,0,:] = orp[:,-1,:]\n",
    "zero_ref_points[:,3,:] = zrp[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2337ed79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf.HelicalProtein.makePointPDB(one_ref_points[1],'align.pdb', outDirec='output/')\n",
    "# hf.HelicalProtein.makePointPDB(zero_ref_points[1],'align2.pdb', outDirec='output/')\n",
    "hf.HelicalProtein.makePointPDB(X[1],'align3.pdb', outDirec='output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08b951b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "outep, outepr  = align_generated_to_starting_ep(one_ref_points, X[:2])\n",
    "\n",
    "hf.HelicalProtein.makePointPDB(outep[1],'alignO.pdb', outDirec='output/')\n",
    "hf.HelicalProtein.makePointPDB(outepr[1],'alignR.pdb', outDirec='output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21eae216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.397503, 10.397503, 10.397503, 10.397503, 10.397503],\n",
       "       [11.03398 , 11.03398 , 11.03398 , 11.03398 , 11.03398 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "856789ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 0\n",
    "\n",
    "outep = align_generated_to_starting_ep(one_ref_points, zero_ref_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "32c191de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = 0\n",
    "\n",
    "outep, outepr = align_generated_to_starting_ep(one_ref_points, zero_ref_points)\n",
    "\n",
    "hf.HelicalProtein.makePointPDB(outep[0],'align.pdb', outDirec='output/')\n",
    "hf.HelicalProtein.makePointPDB(outepr[0],'alignR.pdb', outDirec='output/')\n",
    "\n",
    "outep2 = align_generated_to_starting_ep(X[:2], zero_ref_points)[0]\n",
    "hf.HelicalProtein.makePointPDB(outep2[0],'align3.pdb', outDirec='output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "dedb7632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 8, 3)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outep2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a089928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_calc(g1, target, dvar, ivar, jvar, dindex, pindex, batch_size, zr_ind):\n",
    "    \"\"\"Loss function to move output of two generated helices to target midpoint\"\"\"\n",
    "\n",
    "\n",
    "    #now using dindex gather the desired indices for tetrahedron calcs\n",
    "\n",
    "    #radius of the spheres, aka the distances to unmasked endpoints\n",
    "    g2 = tf.gather(g1,dindex,axis=1)\n",
    "\n",
    "    #see https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler\n",
    "    #inspect .scale_\n",
    "    conv_dist = g2\n",
    "    #transpose lets you easily grab all distances with gather/axis \n",
    "    conv_dist_squared = tf.transpose(tf.square(conv_dist),perm=[0, 2, 1]) \n",
    "\n",
    "    r1_sq = tf.gather(conv_dist_squared, 0, axis=1) \n",
    "    r2_sq =  tf.gather(conv_dist_squared,1, axis=1) \n",
    "    r3_sq = tf.gather(conv_dist_squared, 2, axis=1)\n",
    "\n",
    "    #calculate coordinates of spherial intersect\n",
    "    x = tf.divide(tf.add(tf.subtract(r1_sq,r2_sq),tf.square(dvar)),tf.multiply(2.0,dvar))\n",
    "    y1 = tf.divide(tf.add(tf.add(tf.subtract(r1_sq,r3_sq), tf.square(ivar)), tf.square(jvar)),tf.multiply(2.0,jvar))\n",
    "    y = tf.subtract(y1,tf.multiply(tf.divide(ivar,jvar),x))\n",
    "\n",
    "    pre_z = tf.subtract(tf.subtract(r1_sq,tf.square(x)),tf.square(y))\n",
    "    fixed_z = tf.clip_by_value(pre_z, 1e-10, 100)\n",
    "\n",
    "    #adds  to negative values to 0 for sqrt,\n",
    "    #I think is okay as zero z will imply lengthening of distances to match a non-zero target midpoint,\n",
    "    #pushing the network in the desired direction?\n",
    "\n",
    "    z = tf.sqrt(fixed_z) #assume positive solution\n",
    "    z_neg = tf.multiply(z,-1) #assume negative soluation\n",
    "\n",
    "    #new points, with both assumptions\n",
    "    nwp = tf.concat((tf.reshape(x,(batch_size,-1,1)),\n",
    "                    tf.reshape(y,(batch_size,-1,1)),\n",
    "                    tf.reshape(z,(batch_size,-1,1))), axis=2)  #\n",
    "\n",
    "    nwp_negz = tf.concat((tf.reshape(x,(batch_size,-1,1)),\n",
    "                    tf.reshape(y,(batch_size,-1,1)),\n",
    "                    tf.reshape(z_neg,(batch_size,-1,1))), axis=2)  #\n",
    "\n",
    "    #some positive solutions assumptions,\n",
    "    # assume first [i4] is actual positive use remaining distances of i4 to (i5,i6,i7) to determine z sign\n",
    "    # closest to matching distance is used\n",
    "\n",
    "\n",
    "    #let's start by calculating all i4 to (i5,i6,i7) distances\n",
    "\n",
    "    #stop the gradients since these are used to index gather and scatter\n",
    "    #unsqueeze at two different dimensionsq to broadcast into matrix MX1 by 1XN to MXN \n",
    "    nwp_p =  tf.stop_gradient(tf.expand_dims(nwp,axis=1) - tf.expand_dims(nwp,axis=2))\n",
    "    nwp_n =  tf.stop_gradient(tf.expand_dims(nwp,axis=1) - tf.expand_dims(nwp_negz,axis=2))\n",
    "\n",
    "    nwp_dist_pz = tf.reshape(tf.sqrt(tf.reduce_sum(tf.square(nwp_p), 3)),(-1,5,5)) #distance calc +1e6?\n",
    "    nwp_dist_nz = tf.reshape(tf.sqrt(tf.reduce_sum(tf.square(nwp_n), 3)),(-1,5,5))  #distance calc\n",
    "\n",
    "    z_pn_dist = tf.gather(g1,pindex,axis=1)\n",
    "\n",
    "    #index p4 to p5,p6,p7\n",
    "    #rewrite as non-slice version of this\n",
    "\n",
    "    nwp_dist_pz_c = tf.squeeze(tf.gather(tf.gather(nwp_dist_pz, [0], axis=1), [1,2,3,4], axis=2))\n",
    "    nwp_dist_nz_c = tf.squeeze(tf.gather(tf.gather(nwp_dist_nz, [0], axis=1), [1,2,3,4], axis=2))\n",
    "\n",
    "    nwp_dist_pz_c = tf.expand_dims(nwp_dist_pz_c,axis=2)\n",
    "    nwp_dist_nz_c = tf.expand_dims(nwp_dist_nz_c,axis=2)\n",
    "\n",
    "    # #using a single distance decide the z assumption and apply\n",
    "    correct_z_assum = tf.abs(z_pn_dist - nwp_dist_nz_c) < tf.abs(z_pn_dist - nwp_dist_pz_c)\n",
    "    cz = tf.squeeze(tf.multiply(tf.cast(correct_z_assum,tf.int32),-2))\n",
    "\n",
    "    z_reflect_tensor = tf.ones_like(nwp, dtype=tf.int32)\n",
    "\n",
    "    nwp_mult = tf.cast(tf.tensor_scatter_nd_add(z_reflect_tensor, zr_ind, cz),dtype=tf.float32)\n",
    "    nwp_final = tf.multiply(nwp_mult,nwp)\n",
    "    return nwp_final # means squared loss to desired midpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f3ab996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_base_triangle_trilateriation(dindex, base_tri, distance_map):\n",
    "    \"\"\"Return x,y,z coords on z-plane of base triangle of tetrahedron from a distance map.\"\"\"\n",
    "    \n",
    "#     dindex, base_tri = target_dindex(targ_dind, oneRef = oneRef, num_helices = num_helices)\n",
    "# #     print(dindex)\n",
    "# #     print(base_tri)\n",
    "    \n",
    "    #test case input data: prep base triangles for trilateration at zplane, (0,0,0) (dvar,0,0) (ivar,jvar,0)\n",
    "    desired_dm = distance_map[:, base_tri] #base tri from dindex\n",
    "\n",
    "    dvar_index = tf.convert_to_tensor(0 ,dtype=tf.int32)\n",
    "    s2_index = tf.convert_to_tensor(2 ,dtype=tf.int32) # we would like the angle across from side 2\n",
    "    s3_index = tf.convert_to_tensor(1 ,dtype=tf.int32)\n",
    "\n",
    "    #x value representing center of 2nd sphere at (dvar,0,0) aka s1\n",
    "    dvar = tf.reshape(tf.gather(desired_dm, dvar_index,axis=1),(-1,1)) #side 1\n",
    "    s2 = tf.reshape(tf.gather(desired_dm,   s2_index,axis=1),(-1,1))\n",
    "    s3 = tf.reshape(tf.gather(desired_dm,   s3_index,axis=1),(-1,1))\n",
    "\n",
    "    #calculate the opposite angle of the the third side of base triangle using law of cosines\n",
    "    s1sq = tf.square(dvar)\n",
    "    s2sq = tf.square(s2)\n",
    "    s3sq = tf.square(s3)\n",
    "    ang3 = np.arccos((-s3sq+s2sq+s1sq)/(2*dvar*s2))\n",
    "\n",
    "    #take third point of base triangle via distance * vector\n",
    "    v13 = tf.concat([tf.cos(ang3), tf.sin(ang3), tf.zeros_like(ang3)], axis=1)\n",
    "    p3 = s2*v13\n",
    "    #center points of 3rd sphere\n",
    "    ivar = tf.reshape(p3[:,0],(-1,1))\n",
    "    jvar = tf.reshape(p3[:,1],(-1,1))\n",
    "\n",
    "\n",
    "    #convert all to float32 to match generator output\n",
    "    #expand to dindex size \n",
    "\n",
    "    dvar = tf.cast(tf.repeat(dvar,dindex.shape[0],axis=1),dtype=tf.float32)\n",
    "    ivar = tf.cast(tf.repeat(ivar,dindex.shape[0],axis=1),dtype=tf.float32)\n",
    "    jvar = tf.cast(tf.repeat(jvar,dindex.shape[0],axis=1),dtype=tf.float32)\n",
    "    \n",
    "    return dvar, ivar, jvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6812600f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ab31c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device name CPU\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "\n",
    "#load distance maps and endpoints dataset for initializing start\n",
    "def load_distance_map(name, dm_file='data/Fits_4H_dm_phi.npz'):\n",
    "    rr = np.load(dm_file, allow_pickle=True)\n",
    "    X_train, y_train , featNames = [rr[f] for f in rr.files]\n",
    "    \n",
    "    \n",
    "    return X_train[y_train==name][:,:-4]\n",
    "\n",
    "def index_helix_ep(ep_in,helices_desired=[0,1],num_helices=4):\n",
    "    \n",
    "    num_ep = num_helices*2\n",
    "    hi = np.array(helices_desired,dtype=int)\n",
    "    h_ep = np.array(range(num_ep)).reshape((-1,2)) #generate helix to endpoint mapping\n",
    "    \n",
    "    #alternate example for indexing batch of X \n",
    "    #X.reshape((X.shape[0],-1))[:,indexarray]\n",
    "    \n",
    "    #select desired endpoints from  batch of endpoints\n",
    "    return ep_in[np.ix_(np.array(range(ep_in.shape[0])),h_ep[hi].flatten(), np.array(range(ep_in.shape[2])))]\n",
    "    \n",
    "def get_midpoint(ep_in,helices_desired=[0,1],num_helices=4):\n",
    "    \n",
    "    num_ep = num_helices*2\n",
    "    \n",
    "    ind_ep = index_helix_ep(ep_in, helices_desired=helices_desired, num_helices=4)\n",
    "    \n",
    "    #calculate midpoint\n",
    "    midpoint = ind_ep.sum(axis=1)/np.repeat(ind_ep.shape[1], ind_ep.shape[2])\n",
    "    \n",
    "    return midpoint\n",
    "\n",
    "def get_stubs_from_points(ep_in,index=[0,1,2]):\n",
    "#def get_stubs_from_n_ca_c(n, ca, c):\n",
    "    \"\"\"Modified from Brian's npose code  get_stubs_from_n_ca_c, index references 3 points, to define plane.\n",
    "    \"\"\"\n",
    "    e1 = ep_in[:,index[1]]-ep_in[:,index[0]]\n",
    "    e1 = np.divide( e1, np.linalg.norm(e1, axis=1)[..., None] )\n",
    "\n",
    "    e3 = np.cross( e1, ep_in[:,index[2]]-ep_in[:,index[0]], axis=1 )\n",
    "    e3 = np.divide( e3, np.linalg.norm(e3, axis=1)[..., None] )\n",
    "\n",
    "    e2 = np.cross( e3, e1, axis=1 )\n",
    "\n",
    "    stub = np.zeros((len(ep_in), 4, 4))\n",
    "    stub[...,:3,0] = e1\n",
    "    stub[...,:3,1] = e2\n",
    "    stub[...,:3,2] = e3\n",
    "    stub[...,:3,3] = ep_in[:,index[1]]\n",
    "    stub[...,3,3] = 1.0\n",
    "\n",
    "    return stub\n",
    "\n",
    "def xform_npose_2batch(xform, npose):\n",
    "    #single batch code  util.npose_util as xform_npose\n",
    "    return np.matmul(np.repeat(xform[:,np.newaxis,...],npose.shape[1],axis=1),npose[...,None]).squeeze(-1)\n",
    "\n",
    "def xform_to_z_plane(mobile, index_mobile=[0,1,2]):\n",
    "    \"\"\"rotate points into the z-plane for trilaterization. needs additional translation/reflection\"\"\"\n",
    "\n",
    "    mobile_stub = get_stubs_from_points(mobile, index=index_mobile)\n",
    "    mobile_stub_inv = np.linalg.inv(mobile_stub)\n",
    "    \n",
    "    z_plane_ref = np.repeat(np.array([[[0,0,0],[1,0,0],[1,1,0]]]), mobile.shape[0],axis=0)\n",
    "\n",
    "    ref_stub = get_stubs_from_points(z_plane_ref, index=[0,1,2])\n",
    "\n",
    "    xform = ref_stub @ mobile_stub_inv\n",
    "\n",
    "    return xform\n",
    "\n",
    "\n",
    "def rotate_base_tri_Zplane(endpoint_midpoints, target_point=4, index_mobile=[1,2,3], returnRotMat=False):\n",
    "    \"\"\"rotate points into the z-plane for trilaterization. Target point ensures that point is positive in Z\"\"\"\n",
    "    tp = target_point #target point\n",
    "    zplanexform = xform_to_z_plane(endpoint_midpoints,index_mobile=index_mobile) #one index start base triangle, default\n",
    "    #add one for npose rot calc\n",
    "    npose = np.concatenate((endpoint_midpoints, np.ones((endpoint_midpoints.shape[0],\n",
    "                                                         endpoint_midpoints.shape[1],1))),axis=2) \n",
    "    rot = xform_npose_2batch(zplanexform,npose) # double batch matrix multiplication, see npose, for one batch\n",
    "\n",
    "    #translate X domain to place first index of \"index_mobile\" to 0,0,0\n",
    "    rot[:,:,0] = rot[:,:,0]-np.expand_dims(rot[:,index_mobile[0],0],axis=1)\n",
    "    #based on target point guaranteed to be positive\n",
    "    #reflect new points across the z axis to positive if negative to match just choosing positive solutions\n",
    "    rot[...,2][rot[:,tp,2]<0] = -rot[...,2][rot[:,tp,2]<0]\n",
    "    \n",
    "    if not returnRotMat:\n",
    "        return rot[...,:3] #remove npose rotate dimension\n",
    "    else:\n",
    "        return rot[...,:3], zplanexform\n",
    "\n",
    "\n",
    "\n",
    "#-----------------methods to index needed indices from generator------------------------\n",
    "\n",
    "def helix_dindex(helices_to_keep, num_helices=4, intraHelixDist=True):\n",
    "    \"\"\"Get index values for parts of the distance map\"\"\"\n",
    "    \n",
    "    #prep indices for distance map\n",
    "    num_ep = num_helices*2\n",
    "    mat_ind = np.array(range((num_ep)**2)).reshape((num_ep,num_ep))\n",
    "    iu1 = np.triu_indices(num_ep, 1)\n",
    "    \n",
    "    helix_used = np.array(helices_to_keep,dtype=int)\n",
    "    \n",
    "    h_ep = np.array(range(num_ep)).reshape((-1,2)) #generate helix to endpoint mapping\n",
    "    \n",
    "    tot_ind = []\n",
    "    \n",
    "    if intraHelixDist:\n",
    "        #get indices of distance map that correspond to each helix, overlap is distances between specified endpoints\n",
    "        for x in helix_used:\n",
    "            new_ind = np.intersect1d(mat_ind[h_ep[x]], mat_ind.T[h_ep[x]])\n",
    "            tot_ind.extend(new_ind)\n",
    "    \n",
    "    \n",
    "    for x in permutations(helix_used,2):\n",
    "        new_ind = np.intersect1d(mat_ind[h_ep[x[0]]], mat_ind.T[h_ep[x[1]]])\n",
    "        tot_ind.extend(new_ind)\n",
    "    \n",
    "    #convert to generator indices (indices of iu1 array)\n",
    "    out_ind = []\n",
    "    for x in tot_ind:\n",
    "        if len(np.nonzero(mat_ind[iu1]==x)[0])>0:\n",
    "            out_ind.append(np.nonzero(mat_ind[iu1]==x))\n",
    "\n",
    "    return np.sort(np.array(out_ind).flatten())\n",
    "\n",
    "\n",
    "def point_dindex(target_points, ref=[4], num_helices = 4):\n",
    "    \n",
    "    num_ep = num_helices*2\n",
    "    mat_ind = np.array(range((num_ep)**2)).reshape((num_ep,num_ep))\n",
    "    iu1 = np.triu_indices(num_ep, 1)\n",
    "    \n",
    "    dindex = []\n",
    "    \n",
    "    for tp in target_points:\n",
    "        for ref_ind in ref:\n",
    "            if ref_ind<tp:\n",
    "                dindex.append(mat_ind[ref_ind,tp]) #indices for distances to target point\n",
    "            else:\n",
    "                dindex.append(mat_ind[tp,ref_ind])\n",
    "    \n",
    "    dindex = np.array(dindex)\n",
    "    \n",
    "    out_ind = []\n",
    "    for x in dindex.flatten():\n",
    "        out_ind.append(np.nonzero(mat_ind[iu1]==x))\n",
    "        \n",
    "    out_ind = np.array(out_ind)\n",
    "    \n",
    "    return out_ind.reshape((len(target_points),-1))\n",
    "\n",
    "def target_dindex_pc(target_points, oneRef = True, num_helices = 5, baseTri_out=True):\n",
    "    \"\"\"Distance map indices for base triangle and output distance map\"\"\"\n",
    "    \n",
    "    num_ep = num_helices*2\n",
    "    mat_ind = np.array(range((num_ep)**2)).reshape((num_ep,num_ep))\n",
    "    iu1 = np.triu_indices(num_ep, 1)\n",
    "\n",
    "    if oneRef:\n",
    "        ref = [1,2,3]\n",
    "        base_tri = [mat_ind[1][2],mat_ind[2][3],mat_ind[1][3]] #p1 to p2, p2 to p3, p1 to p3\n",
    "        \n",
    "    else:\n",
    "        ref = [0,1,2]\n",
    "        base_tri = [mat_ind[0][1],mat_ind[1][2],mat_ind[0][3]] #p0 to p1, p1 to p2, p0 to p3\n",
    "    \n",
    "    dindex = []\n",
    "    \n",
    "    for tp in target_points:\n",
    "        if tp<ref[0]: #ensure upper triangulr\n",
    "            dindex.append(mat_ind[tp,ref]) \n",
    "        else:\n",
    "            dindex.append(mat_ind[ref,tp]) #indices for distances to target point #indices for distances to target point\n",
    "    \n",
    "    dindex = np.array(dindex)\n",
    "    \n",
    "    out_ind = []\n",
    "    for x in dindex.flatten():\n",
    "        out_ind.append(np.nonzero(mat_ind[iu1]==x))\n",
    "        \n",
    "    out_ind = np.array(out_ind)\n",
    "    \n",
    "    return out_ind.reshape((-1,len(base_tri))),base_tri\n",
    "\n",
    "def minMax_indices(distance_index, point_index, minmax_obj):\n",
    "    \n",
    "    #assemble conversions \n",
    "    #converts output from generator back to real distances\n",
    "    dMin_all = tf.convert_to_tensor(minmax_obj.data_min_, dtype=tf.float32)\n",
    "    mScale_all = tf.convert_to_tensor(minmax_obj.scale_, dtype = tf.float32)\n",
    "    mMin = tf.convert_to_tensor(minmax_obj.feature_range[0], dtype = tf.float32)\n",
    "\n",
    "    #index just the distances we need for calculation\n",
    "    dMin = tf.gather(dMin_all, distance_index,axis=0)\n",
    "    mScale = tf.gather(mScale_all, distance_index,axis=0)\n",
    "\n",
    "    #indexes we need to determine the +/- z value of the new points\n",
    "    pindex = point_dindex([5,6,7], ref=[4], num_helices = 4)\n",
    "    dMin_nwp = tf.gather(dMin_all, point_index,axis=0)\n",
    "    mScale_nwp = tf.gather(mScale_all, point_index,axis=0)\n",
    "    \n",
    "    return dMin, mScale, mMin, dMin_nwp,  mScale_nwp \n",
    "\n",
    "def ref_distmap_index(distances, num_helices = 4):\n",
    "    \n",
    "    num_ep = num_helices*2\n",
    "    mat_ind = np.array(range((num_ep)**2)).reshape((num_ep,num_ep))\n",
    "    iu1 = np.triu_indices(num_ep, 1)\n",
    "    iu1_flat = iu1[0]*num_ep+iu1[1]\n",
    "    \n",
    "    return distances[np.ix_(range(distances.shape[0]),iu1_flat)]\n",
    "\n",
    "def convert_dMat_to_iu1_index(indices_in, num_helices = 4):\n",
    "    \"\"\"Converts indices on flattened distance index to iu1 single indices\"\"\"\n",
    "    \n",
    "    \n",
    "    conv_array = np.array(indices_in).flatten()\n",
    "    \n",
    "    num_ep = num_helices*2\n",
    "    mat_ind = np.array(range((num_ep)**2)).reshape((num_ep,num_ep))\n",
    "    iu1 = np.triu_indices(num_ep, 1)\n",
    "    \n",
    "    #convert to generator indices (indices of iu1 array)\n",
    "    out_ind = []\n",
    "    for x in conv_array:\n",
    "        if len(np.nonzero(mat_ind[iu1]==x)[0])>0:\n",
    "            out_ind.append(np.nonzero(mat_ind[iu1]==x))\n",
    "            \n",
    "    out_ind = np.array(out_ind)\n",
    "        \n",
    "    return out_ind.reshape(conv_array.shape)\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "def prep_base_triangle_trilateriation(dindex, base_tri, distance_map):\n",
    "    \"\"\"Return x,y,z coords on z-plane of base triangle of tetrahedron from a distance map.\"\"\"\n",
    "    \n",
    "#     dindex, base_tri = target_dindex(targ_dind, oneRef = oneRef, num_helices = num_helices)\n",
    "# #     print(dindex)\n",
    "# #     print(base_tri)\n",
    "    \n",
    "    #test case input data: prep base triangles for trilateration at zplane, (0,0,0) (dvar,0,0) (ivar,jvar,0)\n",
    "    desired_dm = distance_map[:, base_tri] #base tri from dindex\n",
    "\n",
    "    dvar_index = tf.convert_to_tensor(0 ,dtype=tf.int32)\n",
    "    s2_index = tf.convert_to_tensor(2 ,dtype=tf.int32) # we would like the angle across from side 2\n",
    "    s3_index = tf.convert_to_tensor(1 ,dtype=tf.int32)\n",
    "\n",
    "    #x value representing center of 2nd sphere at (dvar,0,0) aka s1\n",
    "    dvar = tf.reshape(tf.gather(desired_dm, dvar_index,axis=1),(-1,1)) #side 1\n",
    "    s2 = tf.reshape(tf.gather(desired_dm,   s2_index,axis=1),(-1,1))\n",
    "    s3 = tf.reshape(tf.gather(desired_dm,   s3_index,axis=1),(-1,1))\n",
    "\n",
    "    #calculate the opposite angle of the the third side of base triangle using law of cosines\n",
    "    s1sq = tf.square(dvar)\n",
    "    s2sq = tf.square(s2)\n",
    "    s3sq = tf.square(s3)\n",
    "    ang3 = np.arccos((-s3sq+s2sq+s1sq)/(2*dvar*s2))\n",
    "\n",
    "    #take third point of base triangle via distance * vector\n",
    "    v13 = tf.concat([tf.cos(ang3), tf.sin(ang3), tf.zeros_like(ang3)], axis=1)\n",
    "    p3 = s2*v13\n",
    "    #center points of 3rd sphere\n",
    "    ivar = tf.reshape(p3[:,0],(-1,1))\n",
    "    jvar = tf.reshape(p3[:,1],(-1,1))\n",
    "\n",
    "\n",
    "    #convert all to float32 to match generator output\n",
    "    #expand to dindex size \n",
    "\n",
    "    dvar = tf.cast(tf.repeat(dvar,dindex.shape[0],axis=1),dtype=tf.float32)\n",
    "    ivar = tf.cast(tf.repeat(ivar,dindex.shape[0],axis=1),dtype=tf.float32)\n",
    "    jvar = tf.cast(tf.repeat(jvar,dindex.shape[0],axis=1),dtype=tf.float32)\n",
    "    \n",
    "    return dvar, ivar, jvar\n",
    "\n",
    "\n",
    "#functions for back propagation\n",
    "\n",
    "\n",
    "@tf.function \n",
    "def train_step(input_z_var,ref_map_, helix_keep_mask_,\n",
    "               target_mp_, dvar_, ivar_, jvar_,\n",
    "                dMin_, mScale_, mMin_, dMin_nwp_, mScale_nwp_, \n",
    "                dindex_, pindex_, batch_,z_reflect_ind_, scale_):\n",
    "\n",
    "    with tf.GradientTape() as g_tape:\n",
    "        g_tape.watch(input_z_var)\n",
    "        g_o = gen_obj.g(input_z_var)\n",
    "        masked_loss = maskLoss(ref_map_, g_o, helix_keep_mask_)\n",
    "        mp_loss  = tf.divide(midpoints_loss(g_o, target_mp_, \n",
    "                    dvar_, ivar_, jvar_,\n",
    "                    dMin_, mScale_, mMin_, dMin_nwp_, mScale_nwp_, \n",
    "                    dindex_, pindex_, batch_,z_reflect_ind_), scale_)\n",
    "\n",
    "        loss = tf.reduce_sum(mp_loss,axis=1) + tf.reduce_sum(masked_loss,axis=1)\n",
    "\n",
    "    g_grads = g_tape.gradient(loss, input_z_var)\n",
    "    optimizer.apply_gradients(zip([g_grads],[input_z_var]))\n",
    "\n",
    "    return input_z_var, masked_loss, mp_loss\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def maskLoss(y_actual, y_pred,mask):\n",
    "    \"\"\"Loss Function for mantaing shape of input helices\"\"\"\n",
    "    custom_loss_val = tf.multiply(mask,tf.square(y_actual-y_pred))\n",
    "    return custom_loss_val\n",
    "\n",
    "@tf.function\n",
    "def midpoints_loss(g1, target, \n",
    "                   dvar, ivar, jvar,\n",
    "                   dMin, mScale, mMin, dMin_nwp, mScale_nwp, \n",
    "                   dindex, pindex, batch_size, zr_ind):\n",
    "    \"\"\"Loss function to move output of two generated helices to target midpoint\"\"\"\n",
    "\n",
    "\n",
    "    #now using dindex gather the desired indices for tetrahedron calcs\n",
    "\n",
    "    #radius of the spheres, aka the distances to unmasked endpoints\n",
    "    g2 = tf.gather(g1,dindex,axis=1)\n",
    "\n",
    "    #see https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler\n",
    "    #inspect .scale_\n",
    "    conv_dist = tf.add(tf.divide(tf.subtract(g2, mMin), mScale),dMin)\n",
    "    #transpose lets you easily grab all distances with gather/axis \n",
    "    conv_dist_squared = tf.transpose(tf.square(conv_dist),perm=[0, 2, 1]) \n",
    "\n",
    "    r1_sq = tf.gather(conv_dist_squared, 0, axis=1) \n",
    "    r2_sq =  tf.gather(conv_dist_squared,1, axis=1) \n",
    "    r3_sq = tf.gather(conv_dist_squared, 2, axis=1)\n",
    "\n",
    "    #calculate coordinates of spherial intersect\n",
    "    x = tf.divide(tf.add(tf.subtract(r1_sq,r2_sq),tf.square(dvar)),tf.multiply(2.0,dvar))\n",
    "    y1 = tf.divide(tf.add(tf.add(tf.subtract(r1_sq,r3_sq), tf.square(ivar)), tf.square(jvar)),tf.multiply(2.0,jvar))\n",
    "    y = tf.subtract(y1,tf.multiply(tf.divide(ivar,jvar),x))\n",
    "\n",
    "    pre_z = tf.subtract(tf.subtract(r1_sq,tf.square(x)),tf.square(y))\n",
    "    fixed_z = tf.clip_by_value(pre_z, 1e-10, 100)\n",
    "\n",
    "    #adds  to negative values to 0 for sqrt,\n",
    "    #I think is okay as zero z will imply lengthening of distances to match a non-zero target midpoint,\n",
    "    #pushing the network in the desired direction?\n",
    "\n",
    "    z = tf.sqrt(fixed_z) #assume positive solution\n",
    "    z_neg = tf.multiply(z,-1) #assume negative soluation\n",
    "\n",
    "    #new points, with both assumptions\n",
    "    nwp = tf.concat((tf.reshape(x,(batch_size,-1,1)),\n",
    "                    tf.reshape(y,(batch_size,-1,1)),\n",
    "                    tf.reshape(z,(batch_size,-1,1))), axis=2)  #\n",
    "\n",
    "    nwp_negz = tf.concat((tf.reshape(x,(batch_size,-1,1)),\n",
    "                    tf.reshape(y,(batch_size,-1,1)),\n",
    "                    tf.reshape(z_neg,(batch_size,-1,1))), axis=2)  #\n",
    "\n",
    "    #some positive solutions assumptions,\n",
    "    # assume first [i4] is actual positive use remaining distances of i4 to (i5,i6,i7) to determine z sign\n",
    "    # closest to matching distance is used\n",
    "\n",
    "\n",
    "    #let's start by calculating all i4 to (i5,i6,i7) distances\n",
    "\n",
    "    #stop the gradients since these are used to index gather and scatter\n",
    "    #unsqueeze at two different dimensionsq to broadcast into matrix MX1 by 1XN to MXN \n",
    "    nwp_p =  tf.stop_gradient(tf.expand_dims(nwp,axis=1) - tf.expand_dims(nwp,axis=2))\n",
    "    nwp_n =  tf.stop_gradient(tf.expand_dims(nwp,axis=1) - tf.expand_dims(nwp_negz,axis=2))\n",
    "\n",
    "    nwp_dist_pz = tf.reshape(tf.sqrt(tf.reduce_sum(tf.square(nwp_p), 3)),(-1,4,4)) #distance calc +1e6?\n",
    "    nwp_dist_nz = tf.reshape(tf.sqrt(tf.reduce_sum(tf.square(nwp_n), 3)),(-1,4,4))  #distance calc\n",
    "\n",
    "    z_pn_dist_pre_con = tf.gather(g1,pindex,axis=1)\n",
    "    z_pn_dist = tf.add(tf.divide(tf.subtract(z_pn_dist_pre_con, mMin), mScale_nwp),dMin_nwp)\n",
    "\n",
    "    #index p4 to p5,p6,p7\n",
    "    #rewrite as non-slice version of this\n",
    "\n",
    "    nwp_dist_pz_c = tf.squeeze(tf.gather(tf.gather(nwp_dist_pz, [0], axis=1), [1,2,3], axis=2))\n",
    "    nwp_dist_nz_c = tf.squeeze(tf.gather(tf.gather(nwp_dist_nz, [0], axis=1), [1,2,3], axis=2))\n",
    "\n",
    "    nwp_dist_pz_c = tf.expand_dims(nwp_dist_pz_c,axis=2)\n",
    "    nwp_dist_nz_c = tf.expand_dims(nwp_dist_nz_c,axis=2)\n",
    "\n",
    "    # #using a single distance decide the z assumption and apply\n",
    "    correct_z_assum = tf.abs(z_pn_dist - nwp_dist_nz_c) < tf.abs(z_pn_dist - nwp_dist_pz_c)\n",
    "    cz = tf.squeeze(tf.multiply(tf.cast(correct_z_assum,tf.int32),-2))\n",
    "\n",
    "    z_reflect_tensor = tf.ones_like(nwp, dtype=tf.int32)\n",
    "\n",
    "    nwp_mult = tf.cast(tf.tensor_scatter_nd_add(z_reflect_tensor, zr_ind, cz),dtype=tf.float32)\n",
    "    nwp_final = tf.multiply(nwp_mult,nwp)\n",
    "\n",
    "    midpoint = tf.reduce_mean(nwp_final,axis=1)\n",
    "    return tf.square(tf.subtract(midpoint,target)) # means squared loss to desired midpoint\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "def fullBUTT_GPU(gen_obj, ref_map, target_mp_in, batch_size=32,cycles=100, input_z=None, \n",
    "                          rate=0.05, target_ep=[4,5,6,7], num_helices=4, oneRef=True,\n",
    "                          scale=5.0, z_size=12, print_loss=False):\n",
    "    \n",
    "    batch_indices = np.repeat(np.array(range(ref_map.shape[0])),batch_size)\n",
    "    batch = batch_indices.shape[0]\n",
    "    target_mp = tf.convert_to_tensor(np.repeat(target_mp_in, batch_size,axis=0),dtype=tf.float32)\n",
    "    ref_map = np.repeat(ref_map, batch_size, axis=0)\n",
    "    \n",
    "    #establish indices for distances to reference\n",
    "    #prep base triangle, convert distances from minmax to regular\n",
    "    dindex, base_tri = target_dindex(target_ep, oneRef = True, num_helices = num_helices)\n",
    "    base_tri = convert_dMat_to_iu1_index(base_tri) #dirty\n",
    "    pindex = point_dindex(target_ep[1:], ref=[target_ep[0]], num_helices = num_helices)\n",
    "\n",
    "    #convert generator output to 'real distances'\n",
    "    #dMin, mScale, mMin, dMin_nwp,  mScale_nwp = minMax_indices(dindex, pindex, brec.mm)\n",
    "    mmTuple = minMax_indices(dindex, pindex, gen_obj.mm)\n",
    "\n",
    "    # prepare base triangle for trilateriation (z plane , p1 at origin, p2 positive x)\n",
    "    #dvar, ivar, jvar = prep_base_triangle_trilateriation(dist[:batch], targ_dind = [4,5,6,7], oneRef = True, num_helices=4)\n",
    "    #baseTuple = prep_base_triangle_trilateriation(dist[:batch], targ_dind = target_ep, oneRef = True, num_helices=num_helices)\n",
    "\n",
    "    baseTuple = prep_base_triangle_trilateriation(dindex, base_tri, ref_map)\n",
    "    \n",
    "    dMin, mScale, mMin, dMin_nwp,  mScale_nwp = mmTuple\n",
    "    dvar,ivar,jvar = baseTuple\n",
    "    \n",
    "    #mask for keeping buttress helices in same orientation\n",
    "    h_index = helix_dindex([0,1], num_helices=4, intraHelixDist=True)\n",
    "    helix_keep_mask = np.zeros((ref_map.shape[1],),dtype=np.int32)\n",
    "    helix_keep_mask[h_index] = 1\n",
    "    helix_keep_mask = tf.convert_to_tensor(helix_keep_mask,dtype=tf.float32)\n",
    "\n",
    "    #input to generator (determinstic output)\n",
    "    \n",
    "    ref_map_conv = gen_obj.mm.transform(ref_map) #map to keep helices same as input\n",
    "    #controlling z reflection during trilaterization\n",
    "    z_r_innerInd = np.repeat(tf.convert_to_tensor([[[1,2],[2,2],[3,2]]]),batch,axis=0)\n",
    "    #batch index\n",
    "    zfi_bi =np.expand_dims(np.array(range(batch)).reshape((-1,1)).repeat(3,axis=1),axis=2)\n",
    "    z_reflect_ind = np.concatenate((zfi_bi,z_r_innerInd),axis=2)\n",
    "    \n",
    "    \n",
    "    if input_z is None:\n",
    "        input_z = tf.random.uniform(shape=(batch, z_size), minval=-1, maxval=1)\n",
    "        \n",
    "    with tf.device(device_name):\n",
    "        input_z_var = tf.Variable(input_z)\n",
    "        ref_map_ = tf.constant(ref_map_conv,dtype=tf.float32)\n",
    "        scale_, z_reflect_ind_ = tf.constant(scale), tf.constant(z_reflect_ind)\n",
    "        target_mp_, batch_ = tf.constant(target_mp),  tf.constant(batch)\n",
    "        dindex_, pindex_ = tf.constant(dindex), tf.constant(pindex)\n",
    "        dMin_, mScale_, mMin_ = tf.constant(dMin), tf.constant(mScale), tf.constant(mMin),\n",
    "        dMin_nwp_,  mScale_nwp_ =  tf.constant(dMin_nwp),  tf.constant(mScale_nwp)\n",
    "        dvar_, ivar_, jvar_ = tf.constant(dvar), tf.constant(ivar), tf.constant(jvar)\n",
    "        helix_keep_mask_ = tf.constant(helix_keep_mask, dtype=tf.float32)\n",
    "\n",
    "    #store grads and inputs as we backpropagate\n",
    "    z=[]\n",
    "    loss_mask = []\n",
    "    loss_mp = []\n",
    "    grads = []\n",
    "    \n",
    "\n",
    "    if print_loss:\n",
    "        g_o = gen_obj.g(input_z_var)\n",
    "        masked_loss = maskLoss(ref_map_, g_o, helix_keep_mask_)\n",
    "\n",
    "        mp_loss  = tf.divide(midpoints_loss(g_o, target_mp_, \n",
    "                            dvar_, ivar_, jvar_,\n",
    "                            dMin_, mScale_, mMin_, dMin_nwp_, mScale_nwp_, \n",
    "                            dindex_, pindex_, batch_, z_reflect_ind_), scale_)\n",
    "\n",
    "        print('start_masked',np.round(np.sum(masked_loss),2))\n",
    "        print('start_mp',np.round(np.sum(mp_loss),2))\n",
    "    \n",
    "    for t in range(1,cycles):\n",
    "        \n",
    "        in_z, mask_l, mp_l = train_step(input_z_var,ref_map_, helix_keep_mask_,\n",
    "                                        target_mp_, dvar_, ivar_, jvar_,\n",
    "                                        dMin_, mScale_, mMin_, dMin_nwp_, mScale_nwp_, \n",
    "                                        dindex_, pindex_, batch_, z_reflect_ind_, scale_)\n",
    "        \n",
    "        z.append(in_z.numpy())\n",
    "        loss_mask.append(mask_l.numpy())\n",
    "        loss_mp.append(mp_l.numpy())\n",
    "\n",
    "    if print_loss:\n",
    "        print('end_masked', np.round(np.sum(loss_mask[-1]),2))\n",
    "        print('end_mp', np.round(np.sum(loss_mp[-1]),2))\n",
    "    \n",
    "    return z, loss_mask, loss_mp, batch_indices\n",
    "    \n",
    "#give backpropagated generator output, find the best outputs based on loss\n",
    "def buttress_ep_from_z(gen_obj, gen_z, starting_ep , loss_midpoint, loss_masked, batchIndices,\n",
    "                       max_loss_mp = 0.001, max_loss_mask = 0.001):\n",
    "    \n",
    "    \n",
    "    best_mp = np.sum(loss_midpoint<max_loss_mp,axis=1)>2 # 3 total mp loss outputs (x,y,z of midpoint to target)\n",
    "    best_mask = np.sum(loss_masked<max_loss_mask,axis=1)>27 # 28 total mask loss point (2 helices)\n",
    "\n",
    "    mask_mp_bool = np.logical_and(best_mp, best_mask)       \n",
    "\n",
    "    identified_z = gen_z[mask_mp_bool]\n",
    "    print(f'Outputs passing filters: {len(identified_z)}')\n",
    "    print(f'Total Outputs: {len(gen_z)}')\n",
    "    uInd = batchIndices[mask_mp_bool]\n",
    "    \n",
    "    orig_ep = starting_ep[uInd]\n",
    "    \n",
    "    \n",
    "    gen_obj.generate(z=12, input_z = identified_z, batch_size=identified_z.shape[0])\n",
    "    gen_obj.MDS_reconstruct_()\n",
    "    \n",
    "    out_ep = np.array(gen_obj.reconsMDS_)\n",
    "    \n",
    "    return align_generated_to_starting_ep(out_ep, orig_ep)\n",
    "\n",
    "def buttress_ep_from_z_mask_only(gen_obj, gen_z,loss_masked, batchIndices,\n",
    "                                 max_loss_mask = 0.002, max_out=100, print_stats= False):\n",
    "    \n",
    "    \n",
    "    sm = np.sum(loss_masked,axis=1)\n",
    "    smi = np.argsort(sm)\n",
    "    sm_sort = sm[smi]\n",
    "    best_mask = sm_sort < max_loss_mask\n",
    "    \n",
    "    ind2 = np.array(range(len(sm)))\n",
    "    #uInd = batchIndices[smi][best_mask]\n",
    "    uiInd = ind2[smi][best_mask]\n",
    "    uInd = batchIndices[uiInd]\n",
    "    \n",
    "    if print_stats:\n",
    "        print('Input Size: ',      len(sm))\n",
    "        print('Passing Filters: ', len(uInd))\n",
    "    \n",
    "    \n",
    "    if len(uInd)>max_out:\n",
    "        uInd = uInd[:max_out]\n",
    "        uiInd = uiInd[:max_out]\n",
    "    \n",
    "    identified_z = gen_z[uiInd]\n",
    "    \n",
    "    gen_obj.generate(z=12, input_z = identified_z, batch_size=identified_z.shape[0])\n",
    "    gen_obj.MDS_reconstruct_()\n",
    "    \n",
    "    out_ep = np.array(gen_obj.reconsMDS_)\n",
    "    \n",
    "    return out_ep, uInd\n",
    "\n",
    "def buttress_ep_from_z_mask_mp(gen_obj, gen_z,loss_masked, loss_mp_ed, batchIndices, max_mp_loss = 1e-3,\n",
    "                                 max_loss_mask = 0.002, max_out=200, print_stats= False, mask_first=True):\n",
    "    \n",
    "    if mask_first:\n",
    "        #make sure the input two helices are maintained in generator output\n",
    "        sm = np.sum(loss_masked,axis=1)\n",
    "        smi = np.argsort(sm) #get sorted indices of lowest mask loss\n",
    "        sm_sort = sm[smi]\n",
    "        smi_o = smi[sm_sort < max_loss_mask]\n",
    "        smp = np.sum(loss_mp_ed, axis=1)\n",
    "\n",
    "        ind2 = np.array(range(len(sm)))\n",
    "        uInd = batchIndices[smi_o][smp[smi_o]<max_mp_loss]\n",
    "        uiInd = ind2[smi_o][smp[smi_o]<max_mp_loss]\n",
    "    else:\n",
    "        #make sure the input two helices are maintained in generator output\n",
    "        sm = np.sum(loss_mp_ed,axis=1)\n",
    "        smi = np.argsort(sm) #get sorted indices of lowest mask loss\n",
    "        sm_sort = sm[smi]\n",
    "        smi_o = smi[sm_sort < max_mp_loss]\n",
    "        smp = np.sum(loss_mp_ed, axis=1)\n",
    "\n",
    "        ind2 = np.array(range(len(sm)))\n",
    "        uInd = batchIndices[smi_o][smp[smi_o]<max_loss_mask]\n",
    "        uiInd = ind2[smi_o][smp[smi_o]<max_loss_mask]\n",
    "        \n",
    "\n",
    "    if len(uInd) > max_out:\n",
    "        uInd = uInd[:max_out]\n",
    "        uiInd = uiInd[:max_out]\n",
    "    \n",
    "    if print_stats:\n",
    "        print('Input Size: ',      len(sm))\n",
    "        print('Passing Filters: ', len(uInd))\n",
    "        \n",
    "    if len(uInd) < 1:\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    \n",
    "    \n",
    "    identified_z = gen_z[uiInd]\n",
    "    \n",
    "    gen_obj.generate(z=12, input_z = identified_z, batch_size=identified_z.shape[0])\n",
    "    gen_obj.MDS_reconstruct_()\n",
    "    \n",
    "    out_ep = np.array(gen_obj.reconsMDS_)\n",
    "    \n",
    "    return out_ep, uInd\n",
    "    \n",
    "\n",
    "#the output of generator will not perfectly match the desired input, get best fit via Kabsch\n",
    "def align_generated_to_starting_ep(gen_ep, orig_ep, target_mp=None):\n",
    "    \"\"\"Uses Kabsh to align generated endpoints onto original endpoints. Orig_Ep on origin with oneRef.\"\"\"\n",
    "    #Thanks to below for this code; modified to batch form\n",
    "    #moves gen_\n",
    "    #https://pymolwiki.org/index.php/Kabsch\n",
    "\n",
    "    #only center on first four points [first two helices]\n",
    "    gen_ep_4 = gen_ep[:,:4,:].copy()\n",
    "    orig_ep_4 = orig_ep[:,:4,:].copy()\n",
    "    \n",
    "    \n",
    "    #centering to prevent affine transformaiton\n",
    "    COM_orig = np.expand_dims(np.sum(orig_ep_4, axis=1)/orig_ep_4.shape[1]   ,axis=1)\n",
    "    COM_gen =  np.expand_dims(np.sum(gen_ep_4,  axis=1)/gen_ep_4.shape[1]     ,axis=1)\n",
    "    \n",
    "    gen_ep_4_cen = gen_ep_4 - COM_gen\n",
    "    orig_ep_4_cen = orig_ep_4 - COM_orig\n",
    "    \n",
    "    #initial error estimate\n",
    "    #E0 = np.sum( np.sum(np.square(gen_ep_4),axis=1),axis=1) + np.sum( np.sum(np.square(orig_ep_4),axis=1),axis=1)\n",
    "\n",
    "    # This beautiful step provides the answer.  V and Wt are the orthonormal\n",
    "    # bases that when multiplied by each other give us the rotation matrix, U.\n",
    "    # S, (Sigma, from SVD) provides us with the error!  Isn't SVD great!                                            #2                      #1\n",
    "    V, S, Wt = np.linalg.svd( np.matmul(np.transpose(gen_ep_4_cen, axes=[0,2,1]), orig_ep_4_cen))\n",
    "\n",
    "    # we already have our solution, in the results from SVD.\n",
    "    # we just need to check for reflections and then produce\n",
    "    # the rotation.  V and Wt are orthonormal, so their det's\n",
    "    # are +/-1.\n",
    "    reflect = np.linalg.det(V) * np.linalg.det(Wt)\n",
    "    #original solution, I will take both reflections\n",
    "    #multiples by 1 or -1 depending if relfect is negative (reflection)\n",
    "    proper_reflection = ((reflect>0).astype(np.int32)*-2+1) \n",
    "    S[:,-1] = S[:,-1]*proper_reflection\n",
    "    V[:,:,-1] = -V[:,:,-1]*proper_reflection.reshape((-1,1))\n",
    "\n",
    "    V_reflect = V.copy()\n",
    "    V_reflect[:,:,-1] = -V_reflect[:,:,-1]\n",
    "    #Error\n",
    "#     RMSD = E0 - (2.0 * np.sum(S,axis=1))\n",
    "#     RMSD = np.sqrt(np.abs(RMSD / L))\n",
    "    \n",
    "    #generate rotation matrices\n",
    "    U = np.matmul(V, Wt)\n",
    "    U_reflect = np.matmul(V_reflect, Wt)\n",
    "\n",
    "#    return target_mp,COM_gen,COM_orig\n",
    "    if target_mp:\n",
    "        final_target_midpoint = np.matmul(np.expand_dims((target_mp-COM_gen.squeeze(axis=1)),axis=1), U).squeeze() + COM_orig.squeeze(axis=1)\n",
    "    final_ep_full         = np.matmul((gen_ep-COM_gen), U) + COM_orig\n",
    "    final_ep_full_reflect = np.matmul((gen_ep-COM_gen), U_reflect) + COM_orig\n",
    "    \n",
    "    if target_mp:\n",
    "        return final_ep_full, final_ep_full_reflect, final_target_midpoint\n",
    "    else:\n",
    "        return final_ep_full, final_ep_full_reflect\n",
    "\n",
    "def guess_reflection(p, p_reflect, des_mp, invert=False):\n",
    "    p_mp = get_midpoint(p, helices_desired=[2,3], num_helices=4)\n",
    "    p_reflect_mp = get_midpoint(p_reflect, helices_desired=[2,3], num_helices=4)\n",
    "    \n",
    "    final_ep = np.zeros_like(p)\n",
    "\n",
    "    measure1 = np.linalg.norm(p_mp - des_mp,axis=1)\n",
    "    measure2 =  np.linalg.norm(p_reflect_mp - des_mp,axis=1)\n",
    "\n",
    "    if not invert:\n",
    "        final_ep[np.nonzero((measure1<measure2))] = p[np.nonzero((measure1<measure2))]\n",
    "        final_ep[np.nonzero((measure1>measure2))] = p_reflect[np.nonzero((measure1>measure2))]\n",
    "    else:\n",
    "        final_ep[np.nonzero((measure1>measure2))] = p[np.nonzero((measure1>measure2))]\n",
    "        final_ep[np.nonzero((measure1<measure2))] = p_reflect[np.nonzero((measure1<measure2))]\n",
    "        \n",
    "    \n",
    "    return final_ep\n",
    "\n",
    "def determine_reflection(input_points, input_points_reflect, reference_targets):\n",
    "    \n",
    "    i_mp = get_midpoint(input_points, helices_desired=[2,3]) \n",
    "    i_reflect_mp = get_midpoint(input_points_reflect, helices_desired=[2,3]) \n",
    "\n",
    "\n",
    "\n",
    "    #get a point on the desired line on mp dist away\n",
    "    dmp = np.linalg.norm(reference_targets-np.expand_dims(i_mp,axis=1),axis=2)\n",
    "    dmp_r = np.linalg.norm(reference_targets-np.expand_dims(i_reflect_mp,axis=1),axis=2)\n",
    "\n",
    "    am = np.argmin(np.abs(dmp),axis=1)\n",
    "    am_r = np.argmin(np.abs(dmp_r),axis=1)\n",
    "    #desired reflection is the further along the target point array, highest index\n",
    "    proper_reflect = am_r>am\n",
    "    \n",
    "\n",
    "    final = input_points.copy()\n",
    "    final[proper_reflect] = input_points_reflect[proper_reflect]\n",
    "    \n",
    "    return final\n",
    "\n",
    "def align_points_to_XYplane(input_points, keep_orig_trans=False):\n",
    "    \"\"\"Align batch of points to XY Plane.\"\"\"\n",
    "    \n",
    "    #example data set\n",
    "    #modified below to align set of points to the xy plane\n",
    "    #https://www.mathworks.com/matlabcentral/answers/255998-how-do-i-move-a-set-of-x-y-z-points-so-they-align-with-the-x-y-plane\n",
    "    \n",
    "    COM_start  = np.sum(input_points, axis=1)/input_points.shape[1]\n",
    "    input_points_ori = input_points - np.expand_dims(COM_start,axis=1)\n",
    "\n",
    "    #sp_ori = (start_points - np.expand_dims(COM_start,axis=1))[0]\n",
    "    V, S, Wt= np.linalg.svd(input_points_ori,full_matrices=False)\n",
    "    \n",
    "    #this indexing is opposite of the matlab output\n",
    "    axis= np.cross(Wt[:,2,:],[[0,0,1]]);\n",
    "    angle = -np.arctan2(np.linalg.norm(axis,axis=1), Wt[:,2,2])\n",
    "    \n",
    "    xf = []\n",
    "    for x in range(len(axis)):\n",
    "        xf.append(nu.xform_from_axis_angle_rad(axis[x], angle[x]))\n",
    "\n",
    "\n",
    "    xformR = np.array(xf)\n",
    "    R = xformR[:,0:3,0:3]\n",
    "    if keep_orig_trans:\n",
    "        A_rot = np.matmul(input_points_ori, R) + np.expand_dims(COM_start, axis=1)\n",
    "    else:\n",
    "        A_rot = np.matmul(input_points_ori, R) \n",
    "    \n",
    "    return A_rot\n",
    "\n",
    "def view_ep(ep_in, name='test', max_out=10):\n",
    "    \n",
    "    outDirec='output/'\n",
    "    \n",
    "    if len(ep_in)>max_out:\n",
    "        it = max_out\n",
    "    else:\n",
    "        it = len(ep_in)\n",
    "    \n",
    "    for i in range(it):\n",
    "        hf.HelicalProtein.makePointPDB(ep_in[i], f'{name}{i}.pdb', outDirec='output/')\n",
    "        \n",
    "    cmd.delete(\"all\")\n",
    "    for i in range(it):\n",
    "        cmd.load(f'{outDirec}/{name}{i}.pdb')\n",
    "        \n",
    "    \n",
    "    cmd.save(f'{outDirec}/viewEP_{name}.pse')\n",
    "    \n",
    "def vp(ep_in, guide_in,  name='test', max_out=10):\n",
    "    \n",
    "    outDirec='output/'\n",
    "    \n",
    "    if len(ep_in)>max_out:\n",
    "        it = max_out\n",
    "    else:\n",
    "        it = len(ep_in)\n",
    "    \n",
    "    for i in range(it):\n",
    "        hf.HelicalProtein.makePointPDB(ep_in[i], f'{name}{i}.pdb', outDirec='output/')\n",
    "        hf.HelicalProtein.makePointPDB(ep_in[i], f'{name}{i}.pdb', outDirec='output/')\n",
    "        \n",
    "        \n",
    "    hf.HelicalProtein.makePointPDB(guide_in,f'{name}_guide.pdb',outDirec='output/')\n",
    "        \n",
    "    cmd.delete(\"all\")\n",
    "    cmd.load(f'{outDirec}/{name}_guide.pdb')\n",
    "    for i in range(it):\n",
    "        cmd.load(f'{outDirec}/{name}{i}.pdb')\n",
    "        \n",
    "    \n",
    "    cmd.save(f'{outDirec}/viewEP_{name}.pse')\n",
    "\n",
    "#generate test set of points to align to a plane \n",
    "#qr decomposition generates a set of orthoganol vectors from the input matrix\n",
    "#we can use this to matrix mulplication to offset a plane on the z axis\n",
    "# Q,R=np.linalg.qr(np.random.randn(3,3))\n",
    "\n",
    "# # [Q,~] = qr(randn(3));\n",
    "# n = 500;\n",
    "# sigma = 0.02;\n",
    "# #grid of points\n",
    "# gp = (np.random.rand(n,3)*[1, 1, 0] + sigma*np.random.randn(n,3))\n",
    "# XYZ = gp@Q;\n",
    "# X = XYZ[:,0];\n",
    "# Y = XYZ[:,1];\n",
    "# Z = XYZ[:,2];\n",
    "\n",
    "\n",
    "def build_protein_on_guide_alt(start_helices, guide_points, batch=200, \n",
    "                           next_mp_dist = 9, mp_deviation_limit = 5, maxOut=1000, nd=12, change=200):\n",
    "    \n",
    "    clash_thresh = 2.85\n",
    "    maxClash_num = 10\n",
    "    \n",
    "    #this orientation promotes most likely growth to [0,0,1] (see distribution of reference set)\n",
    "    sh_xy = align_points_to_XYplane(start_helices, keep_orig_trans=False)\n",
    "    ci = np.zeros(start_helices.shape[0],dtype=np.int32)\n",
    "    master_ep = sh_xy[:,:4,...]# if there are more than 4ep (2 helices) just take the first two\n",
    "    \n",
    "    roomToGrow = True\n",
    "    output_ep_list = []\n",
    "\n",
    "    while roomToGrow:\n",
    "        \n",
    "\n",
    "        #second set of added points unused except for maintaining distance maps indexing from gen\n",
    "        #(based around 4 helices)\n",
    "        current_quad_prez = np.concatenate((master_ep[:,-4:,:], master_ep[:,-4:,:] ), axis=1)\n",
    "\n",
    "        #get a point on the desired line on mp dist away\n",
    "        mp_start = get_midpoint(current_quad_prez,helices_desired=[0,1])\n",
    "\n",
    "        #guide_start = gp[ci]\n",
    "        vg = np.repeat( np.expand_dims(guide_points, axis=0) , current_quad_prez.shape[0],axis=0)\n",
    "        #make that move backwards much larger than next_mp_dist\n",
    "        boo = np.repeat(np.expand_dims(np.arange(vg.shape[1]),axis=0), vg.shape[0],axis=0)\n",
    "        boo2 = (boo<np.expand_dims(np.expand_dims(ci,axis=1),axis=1))[:,0]\n",
    "        vg[boo2] = -1e6\n",
    "\n",
    "\n",
    "        dmp = np.linalg.norm(vg - np.expand_dims(mp_start,axis=1),axis=2)\n",
    "        am = np.argmin(np.abs(dmp - next_mp_dist),axis=1)\n",
    "        print('max next indices',max(am))\n",
    "        if np.max(am)>90:\n",
    "            next_mp_dist = 11\n",
    "        \n",
    "        tmp = vg[np.ix_(np.array(range(vg.shape[0])), am, np.array(range(3)))][0]\n",
    "\n",
    "        cqpz_dmp = np.concatenate((current_quad_prez, np.expand_dims(tmp,axis=1)), axis=1)\n",
    "        print(current_quad_prez.shape)\n",
    "\n",
    "        #rotate points and desired midpoint into trilaterization place\n",
    "        current_quad_tmp = rotate_base_tri_Zplane(cqpz_dmp,  target_point=4, index_mobile=[1,2,3])\n",
    "\n",
    "        target_midpoint = current_quad_tmp[:, 8, :]\n",
    "        current_quad = current_quad_tmp[:, :8, :]\n",
    "        #create distance map for generator\n",
    "        start_dist = np.expand_dims(current_quad,axis=1) - np.expand_dims(current_quad,axis=2)\n",
    "        dist = np.sqrt(np.sum(start_dist**2, 3))\n",
    "        dist = dist.reshape((dist.shape[0],-1))\n",
    "\n",
    "        #indices for reference map\n",
    "        ref_map_base = ref_distmap_index(dist, num_helices=4)\n",
    "\n",
    "        #GPU ##33s  with 500,000 samples with 200 cycles (average of 7 runs)\n",
    "        #CPU ##39s\n",
    "        #maybe there is something I can do to make this more effecient, not pipeline bottleneck so okay\n",
    "        #for small models like this tensor flow says gpu may not be more effecient\n",
    "        output_z, loss_mask, loss_mp, batchInd = fullBUTT_GPU(gen_obj, ref_map_base , target_midpoint, \n",
    "                                                            batch_size=100, cycles=200, input_z=None, \n",
    "                                                            rate=0.05, target_ep=[4,5,6,7], num_helices=4, \n",
    "                                                            oneRef=True, scale=100.0, z_size=12)\n",
    "\n",
    "        out_ep, uInd = buttress_ep_from_z_mask_mp(gen_obj, output_z[-1], loss_mask[-1], loss_mp[-1], \n",
    "                                                  batchInd, max_mp_loss = 1e-3, max_loss_mask = 0.002, \n",
    "                                                  max_out=maxOut, print_stats= True, mask_first=True)\n",
    "        \n",
    "        if len(uInd) < 1:\n",
    "            print(f'Failed at helices length {output_ep_list[0].shape[1]/2}')\n",
    "            return output_ep_list\n",
    "\n",
    "        fa, fa_reflect = align_generated_to_starting_ep(out_ep, current_quad_prez[uInd])\n",
    "\n",
    "        #use the reflection that promotes the most movement along the guide points\n",
    "        final_dr = determine_reflection(fa, fa_reflect, vg[uInd])\n",
    "\n",
    "        #ensure that midpoint is close enough\n",
    "        mpf = get_midpoint(final_dr,helices_desired=[2,3])\n",
    "        mpfdb = np.linalg.norm(tmp[uInd].squeeze()-mpf,axis=1) < mp_deviation_limit\n",
    "\n",
    "        final = final_dr[mpfdb]\n",
    "        \n",
    "        #remove steric clashes\n",
    "        build_epr = ge.EP_Recon(master_ep[uInd,:,:][mpfdb][:,-2:,:])\n",
    "        query_epr = ge.EP_Recon(final[:,4:,:])\n",
    "        build = build_epr.to_npose()\n",
    "        query = query_epr.to_npose()\n",
    "        cc = np.zeros((len(build),),dtype=np.int32)\n",
    "        for x in range(len(build)) :\n",
    "            axa = scipy.spatial.distance.cdist(build[x],query[x])\n",
    "            cc[x] = np.sum(axa<clash_thresh)\n",
    "            \n",
    "        remClash = cc<maxClash_num\n",
    "\n",
    "        \n",
    "        print(f'clash keep', np.sum(remClash))\n",
    "        \n",
    "        master_ep = np.concatenate((master_ep[uInd,:,:][mpfdb][remClash], final[:,4:,:][remClash]), axis=1)\n",
    "        print(f'final pass filter' ,master_ep.shape[0])\n",
    "        if master_ep.shape[0]<2:\n",
    "            break\n",
    "\n",
    "        #remake ci here with new indices\n",
    "        final_mp = get_midpoint(final[remClash], helices_desired=[2,3])\n",
    "        dmp = np.linalg.norm(guide_points - np.expand_dims(final_mp,axis=1),axis=2)\n",
    "        ci = np.argmin(np.abs(dmp),axis=1)\n",
    "\n",
    "        end_dist = np.linalg.norm(guide_points[ci]-guide_points[-1],axis=1)\n",
    "        outOfRoom = end_dist<next_mp_dist\n",
    "\n",
    "        getOut = master_ep[outOfRoom]\n",
    "        master_ep = master_ep[~outOfRoom]\n",
    "        ci = ci[~outOfRoom]\n",
    "\n",
    "        if len(getOut)>1:\n",
    "            output_ep_list.append(getOut)\n",
    "\n",
    "        if len(master_ep) < 2:\n",
    "             roomToGrow = False\n",
    "                \n",
    "    return output_ep_list\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def get_reference_input(batch=1000):\n",
    "    rr = np.load(f'data/ep_for_X.npz', allow_pickle=True)\n",
    "    X = [rr[f] for f in rr.files][0]\n",
    "\n",
    "    dX = np.expand_dims(X,axis=1) - np.expand_dims(X,axis=2)\n",
    "    dist = np.sqrt(np.sum(dX**2, 3))  #+ 1e-6) #this dataset is good \n",
    "    dist = dist.reshape((dist.shape[0],-1))\n",
    "    mp_01 = get_midpoint(X,helices_desired=[0,1])\n",
    "    mp_23 = get_midpoint(X,helices_desired=[2,3])\n",
    "    #mp distance map\n",
    "    ep_mp = np.hstack((X.reshape((-1,24)),mp_01,mp_23)).reshape(-1,10,3) #helix12mp=8  helix34mp=9\n",
    "\n",
    "\n",
    "    #initiate array to hold endpoints\n",
    "     #mp deviation from guide points\n",
    "\n",
    "    #random sample starting endpoints to buttress\n",
    "    refi_all = list(range(ep_mp.shape[0]))\n",
    "    ref_ind = np.array(random.sample(refi_all , batch))\n",
    "    start_hel = ep_mp[ref_ind ,:4,...]\n",
    "    \n",
    "    return start_hel\n",
    "\n",
    "#ahhh recode this without the global nonsense at some point\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    device_name = tf.test.gpu_device_name()\n",
    "else:\n",
    "    device_name = 'CPU'\n",
    "rate=0.05\n",
    "# if ~devtype.__eq__(device_name):\n",
    "# device_name = 'CPU'\n",
    "print(f'device name {device_name}')\n",
    "\n",
    "gen=\"data/BestGenerator\"\n",
    "\n",
    "\n",
    "\n",
    "with tf.device(device_name):\n",
    "    gen_obj = ge.BatchRecon(gen)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=rate)\n",
    "output1=gen_obj.generate(z=12,batch_size=12) #example generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "76aaa6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pos_circle(start, stop, h=30, k=0, r=30, num_points= 100):\n",
    "    \"\"\"Generate guide points originating a the origin for a circle, positive values only. \"\"\"\n",
    "    #(x-h)^2 + (y-k)^2 = r^(2)\n",
    "    # y = sqrt(r^2-(x-h^2))+k\n",
    "    x = np.linspace(start,stop,num=num_points)\n",
    "    z = np.sqrt(np.square(r)-np.square(x-h))+k\n",
    "    y = np.zeros_like(x)\n",
    "    return np.concatenate((x.reshape((-1,1)),y.reshape((-1,1)),z.reshape((-1,1))),axis=1)\n",
    "\n",
    "def add_xline(inputP, line_length, num=60):\n",
    "    line = np.zeros((num,3))\n",
    "    line[:,0] = np.linspace(0, line_length, num=num)\n",
    "    cline = inputP[-1]+line\n",
    "    ci_line = np.concatenate((inputP,cline),axis=0)\n",
    "    \n",
    "    return ci_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "296c2fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "c341a418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Guided_Midpoint_Buttress as gmp\n",
    "start_hel = gmp.get_reference_input(batch=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "05146189",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_deviation_limit = 6\n",
    "\n",
    "circ_gp = generate_pos_circle(0,60,h=60,k=0,r=60, num_points= 200)\n",
    "guide_points = circ_gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "2b227749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max next indices 2\n",
      "(3000, 8, 3)\n",
      "Input Size:  300000\n",
      "Passing Filters:  2000\n",
      "clash keep 1770\n",
      "final pass filter 1770\n",
      "max next indices 12\n",
      "(1770, 8, 3)\n",
      "Input Size:  177000\n",
      "Passing Filters:  1911\n",
      "clash keep 1294\n",
      "final pass filter 1294\n",
      "max next indices 27\n",
      "(1294, 8, 3)\n",
      "Input Size:  129400\n",
      "Passing Filters:  2000\n",
      "clash keep 1108\n",
      "final pass filter 1108\n",
      "max next indices 46\n",
      "(1108, 8, 3)\n",
      "Input Size:  110800\n",
      "Passing Filters:  2000\n",
      "clash keep 940\n",
      "final pass filter 940\n",
      "max next indices 69\n",
      "(940, 8, 3)\n",
      "Input Size:  94000\n",
      "Passing Filters:  1643\n",
      "clash keep 673\n",
      "final pass filter 673\n",
      "max next indices 96\n",
      "(673, 8, 3)\n",
      "Input Size:  67300\n",
      "Passing Filters:  1018\n",
      "clash keep 356\n",
      "final pass filter 356\n",
      "max next indices 131\n",
      "(356, 8, 3)\n",
      "Input Size:  35600\n",
      "Passing Filters:  415\n",
      "clash keep 139\n",
      "final pass filter 139\n",
      "max next indices 162\n",
      "(139, 8, 3)\n",
      "Input Size:  13900\n",
      "Passing Filters:  176\n",
      "clash keep 59\n",
      "final pass filter 59\n",
      "max next indices 181\n",
      "(59, 8, 3)\n",
      "Input Size:  5900\n",
      "Passing Filters:  101\n",
      "clash keep 26\n",
      "final pass filter 26\n",
      "max next indices 186\n",
      "(26, 8, 3)\n",
      "Input Size:  2600\n",
      "Passing Filters:  20\n",
      "clash keep 0\n",
      "final pass filter 0\n",
      "Wall time: 12min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "out_ep = build_protein_on_guide_alt(start_hel, circ_gp, batch=500, \n",
    "                                next_mp_dist=9, mp_deviation_limit = 5,maxOut=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "04fdc78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\n"
     ]
    }
   ],
   "source": [
    "sumL = 0\n",
    "\n",
    "for i,x in enumerate(out_ep):\n",
    "    sumL += len(x)\n",
    "    print(i, len(x))\n",
    "print('total',sumL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "035fc310",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this orientation promotes most likely growth to [0,0,1] (see distribution of reference set)\n",
    "sh_xy = align_points_to_XYplane(start_hel, keep_orig_trans=False)\n",
    "ci = np.zeros(start_hel.shape[0],dtype=np.int32)\n",
    "master_ep = sh_xy[:,:4,...]# if there are more than 4ep (2 helices) just take the first two\n",
    "\n",
    "roomToGrow = True\n",
    "output_ep_list = []\n",
    "\n",
    "circ_gp = generate_pos_circle(0,60,h=60,k=0,r=60, num_points= 200)\n",
    "guide_points = circ_gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "7ac52bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmp.vp(out_ep[1],circ_gp,max_out=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fcf8befb",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_mp_dist=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1b65f504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max next indices 3\n",
      "(200, 8, 3)\n",
      "Input Size:  20000\n",
      "Passing Filters:  200\n"
     ]
    }
   ],
   "source": [
    "#second set of added points unused except for maintaining distance maps indexing from gen\n",
    "#(based around 4 helices)\n",
    "current_quad_prez = np.concatenate((master_ep[:,-4:,:], master_ep[:,-4:,:] ), axis=1)\n",
    "\n",
    "#get a point on the desired line on mp dist away\n",
    "mp_start = get_midpoint(current_quad_prez,helices_desired=[0,1])\n",
    "\n",
    "#guide_start = gp[ci]\n",
    "vg = np.repeat( np.expand_dims(guide_points, axis=0) , current_quad_prez.shape[0],axis=0)\n",
    "#make that move backwards much larger than next_mp_dist\n",
    "boo = np.repeat(np.expand_dims(np.arange(vg.shape[1]),axis=0), vg.shape[0],axis=0)\n",
    "boo2 = (boo<np.expand_dims(np.expand_dims(ci,axis=1),axis=1))[:,0]\n",
    "vg[boo2] = -1e6\n",
    "\n",
    "\n",
    "dmp = np.linalg.norm(vg - np.expand_dims(mp_start,axis=1),axis=2)\n",
    "am = np.argmin(np.abs(dmp - next_mp_dist),axis=1)\n",
    "print('max next indices',max(am))\n",
    "\n",
    "\n",
    "tmp = vg[np.ix_(np.array(range(vg.shape[0])), am, np.array(range(3)))][0]\n",
    "\n",
    "cqpz_dmp = np.concatenate((current_quad_prez, np.expand_dims(tmp,axis=1)), axis=1)\n",
    "print(current_quad_prez.shape)\n",
    "\n",
    "#rotate points and desired midpoint into trilaterization place\n",
    "current_quad_tmp = rotate_base_tri_Zplane(cqpz_dmp,  target_point=8, index_mobile=[1,2,3])\n",
    "\n",
    "target_midpoint = current_quad_tmp[:, 8, :]\n",
    "current_quad = current_quad_tmp[:, :8, :]\n",
    "#create distance map for generator\n",
    "start_dist = np.expand_dims(current_quad,axis=1) - np.expand_dims(current_quad,axis=2)\n",
    "dist = np.sqrt(np.sum(start_dist**2, 3))\n",
    "dist = dist.reshape((dist.shape[0],-1))\n",
    "\n",
    "#indices for reference map\n",
    "ref_map_base = ref_distmap_index(dist, num_helices=4)\n",
    "\n",
    "maxOut = 200\n",
    "#GPU ##33s  with 500,000 samples with 200 cycles (average of 7 runs)\n",
    "#CPU ##39s\n",
    "#maybe there is something I can do to make this more effecient, not pipeline bottleneck so okay\n",
    "#for small models like this tensor flow says gpu may not be more effecient\n",
    "output_z, loss_mask, loss_mp, batchInd = fullBUTT_GPU(gen_obj, ref_map_base , target_midpoint, \n",
    "                                                    batch_size=100, cycles=200, input_z=None, \n",
    "                                                    rate=0.05, target_ep=[4,5,6,7], num_helices=4, \n",
    "                                                    oneRef=True, scale=100.0, z_size=12)\n",
    "\n",
    "out_ep, uInd = buttress_ep_from_z_mask_mp(gen_obj, output_z[-1], loss_mask[-1], loss_mp[-1], \n",
    "                                          batchInd, max_mp_loss = 1e-3, max_loss_mask = 0.002, \n",
    "                                          max_out=maxOut, print_stats= True, mask_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c961a7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fa, fa_reflect = align_generated_to_starting_ep(out_ep, current_quad_prez[uInd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9f27a847",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the reflection that promotes the most movement along the guide points\n",
    "final_dr = determine_reflection(fa, fa_reflect, vg[uInd])\n",
    "\n",
    "#ensure that midpoint is close enough\n",
    "mpf = get_midpoint(final_dr,helices_desired=[2,3])\n",
    "mpfdb = np.linalg.norm(tmp[uInd].squeeze()-mpf,axis=1) < mp_deviation_limit\n",
    "\n",
    "final = final_dr[mpfdb]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d8675d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(193, 8, 3)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e1185678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final pass filter 200\n"
     ]
    }
   ],
   "source": [
    "master_ep2 = np.concatenate((master_ep[uInd,:,:][mpfdb], final[:,4:,:]), axis=1)\n",
    "print(f'final pass filter' ,master_ep.shape[0])\n",
    "\n",
    "#remake ci here with new indices\n",
    "final_mp = get_midpoint(final, helices_desired=[2,3])\n",
    "dmp = np.linalg.norm(guide_points - np.expand_dims(final_mp,axis=1),axis=2)\n",
    "ci = np.argmin(np.abs(dmp),axis=1)\n",
    "\n",
    "end_dist = np.linalg.norm(guide_points[ci]-guide_points[-1],axis=1)\n",
    "outOfRoom = end_dist<next_mp_dist\n",
    "\n",
    "getOut = master_ep2[outOfRoom]\n",
    "master_ep = master_ep2[~outOfRoom]\n",
    "ci = ci[~outOfRoom]\n",
    "\n",
    "if len(getOut)>1:\n",
    "    output_ep_list.append(getOut)\n",
    "\n",
    "if len(master_ep2) < 2:\n",
    "     roomToGrow = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b54dd061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_ep2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "05eecd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmp.view_ep(master_ep2)\n",
    "#hf.HelicalProtein.makePointPDB(circ_gp,'cirgp.pdb',outDirec='output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9da87e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "59d58509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max next indices 14\n",
      "(193, 8, 3)\n",
      "Input Size:  19300\n",
      "Passing Filters:  200\n"
     ]
    }
   ],
   "source": [
    "#second set of added points unused except for maintaining distance maps indexing from gen\n",
    "#(based around 4 helices)\n",
    "current_quad_prez = np.concatenate((master_ep2[:,-4:,:], master_ep2[:,-4:,:] ), axis=1)\n",
    "\n",
    "#get a point on the desired line on mp dist away\n",
    "mp_start = get_midpoint(current_quad_prez,helices_desired=[0,1])\n",
    "\n",
    "#guide_start = gp[ci]\n",
    "vg = np.repeat( np.expand_dims(guide_points, axis=0) , current_quad_prez.shape[0],axis=0)\n",
    "#make that move backwards much larger than next_mp_dist\n",
    "boo = np.repeat(np.expand_dims(np.arange(vg.shape[1]),axis=0), vg.shape[0],axis=0)\n",
    "boo2 = (boo<np.expand_dims(np.expand_dims(ci,axis=1),axis=1))[:,0]\n",
    "vg[boo2] = -1e6\n",
    "\n",
    "\n",
    "dmp = np.linalg.norm(vg - np.expand_dims(mp_start,axis=1),axis=2)\n",
    "am = np.argmin(np.abs(dmp - next_mp_dist),axis=1)\n",
    "print('max next indices',max(am))\n",
    "\n",
    "\n",
    "tmp = vg[np.ix_(np.array(range(vg.shape[0])), am, np.array(range(3)))][0]\n",
    "\n",
    "cqpz_dmp = np.concatenate((current_quad_prez, np.expand_dims(tmp,axis=1)), axis=1)\n",
    "print(current_quad_prez.shape)\n",
    "\n",
    "#rotate points and desired midpoint into trilaterization place\n",
    "current_quad_tmp = rotate_base_tri_Zplane(cqpz_dmp,  target_point=8, index_mobile=[1,2,3])\n",
    "\n",
    "target_midpoint = current_quad_tmp[:, 8, :]\n",
    "current_quad = current_quad_tmp[:, :8, :]\n",
    "#create distance map for generator\n",
    "start_dist = np.expand_dims(current_quad,axis=1) - np.expand_dims(current_quad,axis=2)\n",
    "dist = np.sqrt(np.sum(start_dist**2, 3))\n",
    "dist = dist.reshape((dist.shape[0],-1))\n",
    "\n",
    "#indices for reference map\n",
    "ref_map_base = ref_distmap_index(dist, num_helices=4)\n",
    "\n",
    "maxOut = 200\n",
    "#GPU ##33s  with 500,000 samples with 200 cycles (average of 7 runs)\n",
    "#CPU ##39s\n",
    "#maybe there is something I can do to make this more effecient, not pipeline bottleneck so okay\n",
    "#for small models like this tensor flow says gpu may not be more effecient\n",
    "output_z, loss_mask, loss_mp, batchInd = fullBUTT_GPU(gen_obj, ref_map_base , target_midpoint, \n",
    "                                                    batch_size=100, cycles=200, input_z=None, \n",
    "                                                    rate=0.05, target_ep=[4,5,6,7], num_helices=4, \n",
    "                                                    oneRef=True, scale=100.0, z_size=12)\n",
    "\n",
    "out_ep, uInd = buttress_ep_from_z_mask_mp(gen_obj, output_z[-1], loss_mask[-1], loss_mp[-1], \n",
    "                                          batchInd, max_mp_loss = 1e-3, max_loss_mask = 0.002, \n",
    "                                          max_out=maxOut, print_stats= True, mask_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d0ba8070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac5eada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ff43e7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fa, fa_reflect = align_generated_to_starting_ep(out_ep, current_quad_prez[uInd])\n",
    "#use the reflection that promotes the most movement along the guide points\n",
    "final_dr = determine_reflection(fa, fa_reflect, vg[uInd])\n",
    "\n",
    "#ensure that midpoint is close enough\n",
    "mpf = get_midpoint(final_dr,helices_desired=[2,3])\n",
    "mpfdb = np.linalg.norm(tmp[uInd].squeeze()-mpf,axis=1) < mp_deviation_limit\n",
    "\n",
    "final = final_dr[mpfdb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "feb8a279",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmp.view_ep(current_quad_prez[uInd][mpfdb], name='cqp')\n",
    "gmp.view_ep(final, name='fin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "175452d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final pass filter 141\n"
     ]
    }
   ],
   "source": [
    "master_ep3 = np.concatenate((master_ep2[uInd,:,:][mpfdb], final[:,4:,:]), axis=1)\n",
    "print(f'final pass filter' ,master_ep3.shape[0])\n",
    "\n",
    "#remake ci here with new indices\n",
    "final_mp = get_midpoint(final, helices_desired=[2,3])\n",
    "dmp = np.linalg.norm(guide_points - np.expand_dims(final_mp,axis=1),axis=2)\n",
    "ci = np.argmin(np.abs(dmp),axis=1)\n",
    "\n",
    "end_dist = np.linalg.norm(guide_points[ci]-guide_points[-1],axis=1)\n",
    "outOfRoom = end_dist<next_mp_dist\n",
    "\n",
    "getOut = master_ep3[outOfRoom]\n",
    "master_ep3 = master_ep3[~outOfRoom]\n",
    "ci = ci[~outOfRoom]\n",
    "\n",
    "if len(getOut)>1:\n",
    "    output_ep_list.append(getOut)\n",
    "\n",
    "if len(master_ep2) < 2:\n",
    "     roomToGrow = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e059608a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmp.vp(master_ep3, circ_gp,name='mep3', max_out=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a0a633c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(141, 12, 3)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_ep3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175311ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a5112149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max next indices 28\n",
      "(141, 8, 3)\n",
      "Input Size:  14100\n",
      "Passing Filters:  200\n"
     ]
    }
   ],
   "source": [
    "#second set of added points unused except for maintaining distance maps indexing from gen\n",
    "#(based around 4 helices)\n",
    "current_quad_prez = np.concatenate((master_ep3[:,-4:,:], master_ep3[:,-4:,:] ), axis=1)\n",
    "\n",
    "#get a point on the desired line on mp dist away\n",
    "mp_start = get_midpoint(current_quad_prez,helices_desired=[0,1])\n",
    "\n",
    "#guide_start = gp[ci]\n",
    "vg = np.repeat( np.expand_dims(guide_points, axis=0) , current_quad_prez.shape[0],axis=0)\n",
    "#make that move backwards much larger than next_mp_dist\n",
    "boo = np.repeat(np.expand_dims(np.arange(vg.shape[1]),axis=0), vg.shape[0],axis=0)\n",
    "boo2 = (boo<np.expand_dims(np.expand_dims(ci,axis=1),axis=1))[:,0]\n",
    "vg[boo2] = -1e6\n",
    "\n",
    "\n",
    "dmp = np.linalg.norm(vg - np.expand_dims(mp_start,axis=1),axis=2)\n",
    "am = np.argmin(np.abs(dmp - next_mp_dist),axis=1)\n",
    "print('max next indices',max(am))\n",
    "\n",
    "\n",
    "tmp = vg[np.ix_(np.array(range(vg.shape[0])), am, np.array(range(3)))][0]\n",
    "\n",
    "cqpz_dmp = np.concatenate((current_quad_prez, np.expand_dims(tmp,axis=1)), axis=1)\n",
    "print(current_quad_prez.shape)\n",
    "\n",
    "#rotate points and desired midpoint into trilaterization place\n",
    "current_quad_tmp = rotate_base_tri_Zplane(cqpz_dmp,  target_point=8, index_mobile=[1,2,3])\n",
    "\n",
    "target_midpoint = current_quad_tmp[:, 8, :]\n",
    "current_quad = current_quad_tmp[:, :8, :]\n",
    "#create distance map for generator\n",
    "start_dist = np.expand_dims(current_quad,axis=1) - np.expand_dims(current_quad,axis=2)\n",
    "dist = np.sqrt(np.sum(start_dist**2, 3))\n",
    "dist = dist.reshape((dist.shape[0],-1))\n",
    "\n",
    "#indices for reference map\n",
    "ref_map_base = ref_distmap_index(dist, num_helices=4)\n",
    "\n",
    "maxOut = 200\n",
    "#GPU ##33s  with 500,000 samples with 200 cycles (average of 7 runs)\n",
    "#CPU ##39s\n",
    "#maybe there is something I can do to make this more effecient, not pipeline bottleneck so okay\n",
    "#for small models like this tensor flow says gpu may not be more effecient\n",
    "output_z, loss_mask, loss_mp, batchInd = fullBUTT_GPU(gen_obj, ref_map_base , target_midpoint, \n",
    "                                                    batch_size=100, cycles=200, input_z=None, \n",
    "                                                    rate=0.05, target_ep=[4,5,6,7], num_helices=4, \n",
    "                                                    oneRef=True, scale=100.0, z_size=12)\n",
    "\n",
    "out_ep, uInd = buttress_ep_from_z_mask_mp(gen_obj, output_z[-1], loss_mask[-1], loss_mp[-1], \n",
    "                                          batchInd, max_mp_loss = 1e-3, max_loss_mask = 0.002, \n",
    "                                          max_out=maxOut, print_stats= True, mask_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c07ec34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmp.view_ep([tmp],max_out = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "78d94c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(100):\n",
    "    hf.HelicalProtein.makePointPDB([tmp[x]],f'tmp{x}.pdb',outDirec='output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "34c67612",
   "metadata": {},
   "outputs": [],
   "source": [
    "fa, fa_reflect = align_generated_to_starting_ep(out_ep, current_quad_prez[uInd])\n",
    "#use the reflection that promotes the most movement along the guide points\n",
    "final_dr = determine_reflection(fa, fa_reflect, vg[uInd])\n",
    "\n",
    "#ensure that midpoint is close enough\n",
    "mpf = get_midpoint(final_dr,helices_desired=[2,3])\n",
    "mpfdb = np.linalg.norm(tmp[uInd].squeeze()-mpf,axis=1) < mp_deviation_limit\n",
    "\n",
    "final = final_dr[mpfdb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4207b85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmp.view_ep([tmp],max_out = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5efa6d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmp.view_ep(current_quad_prez[uInd][mpfdb], name='cqp')\n",
    "gmp.view_ep(final, name='fin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "57e083c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import LoopEndpoints as le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3df661f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_clash(build_set, query, threshold=2.85):\n",
    "    \"\"\"Return True if new addition clashes with current set\"\"\"\n",
    "    \n",
    "    query_set = query\n",
    "    seq_buff = 5 # +1 from old clash check, should be fine\n",
    "    if len(query_set) < seq_buff:\n",
    "        seq_buff = len(query_set)\n",
    "    elif len(build_set) < seq_buff:\n",
    "        seq_buff = len(build_set)\n",
    "\n",
    "    axa = scipy.spatial.distance.cdist(build_set,query_set)\n",
    "    for i in range(seq_buff):\n",
    "        for j in range(seq_buff-i):\n",
    "            axa[-(i+1)][j] = threshold + 10 # modded from .1 here\n",
    "            \n",
    "\n",
    "    return np.sum(axa<threshold)\n",
    "#     if np.min(axa) < threshold: # clash condition\n",
    "#         return True\n",
    "\n",
    "#     return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "54ac682d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "83bde749",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_epr = ge.EP_Recon(master_ep3[uInd,:,:][mpfdb][:,-2:,:])\n",
    "query_epr = ge.EP_Recon(final[:,4:,:])\n",
    "build = build_epr.to_npose()\n",
    "query = query_epr.to_npose()\n",
    "cc = np.zeros((len(build),),dtype=np.int32)\n",
    "for x in range(len(build)) :\n",
    "    axa = scipy.spatial.distance.cdist(build[x],query[x])\n",
    "    cc[x] = np.sum(axa<2.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "dcf09388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,  24,   0,   8,   0,   4,  21,   2, 128,   7,   1,  32,   0,\n",
       "         3,   0,  51,   2,   0,   0,  57,   9,   0,  34, 126,   0,  15,\n",
       "         2,   6,   0,  21,   0,   2,   5,   0,   0, 183,  46,   0,   1,\n",
       "         5,   0,   0,   1,   0,   0,   0,   0,  42,   0,   1,   2, 202,\n",
       "         0,  23,   4,   0,   1,   0,  12,  18,  18,   0,  72,   0,   0,\n",
       "         0,   0,  20,   0,   0,   2,   0,   0,   0,   3,  15,   0,   3,\n",
       "         0,   2,  38,   1,   4,   0,   2,   1,   0,   0,   0,   0,  38,\n",
       "         7,   0,  14,   0,   0,   2,   2,   0,   2,   6,   0,   0,   4,\n",
       "         0,   2,   1,   0,  28,  10,   0,  43,   0,   0,   4,   0,  73,\n",
       "         0,   6,   0,   0,   1,   0,   0,   0,   7,   3,   0,  16,   1,\n",
       "         4,   0,   0,   2,   0,   1])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "31425a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "b=check_clash(build[x], query[x], threshold=2.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "eec6e0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "?scipy.spatial.distance.cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6dc1ee66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(build)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad04dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_ep3 = np.concatenate((master_ep2[uInd,:,:][mpfdb], final[:,4:,:]), axis=1)\n",
    "print(f'final pass filter' ,master_ep3.shape[0])\n",
    "\n",
    "#remake ci here with new indices\n",
    "final_mp = get_midpoint(final, helices_desired=[2,3])\n",
    "dmp = np.linalg.norm(guide_points - np.expand_dims(final_mp,axis=1),axis=2)\n",
    "ci = np.argmin(np.abs(dmp),axis=1)\n",
    "\n",
    "end_dist = np.linalg.norm(guide_points[ci]-guide_points[-1],axis=1)\n",
    "outOfRoom = end_dist<next_mp_dist\n",
    "\n",
    "getOut = master_ep3[outOfRoom]\n",
    "master_ep3 = master_ep3[~outOfRoom]\n",
    "ci = ci[~outOfRoom]\n",
    "\n",
    "if len(getOut)>1:\n",
    "    output_ep_list.append(getOut)\n",
    "\n",
    "if len(master_ep2) < 2:\n",
    "     roomToGrow = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fa_tfpy",
   "language": "python",
   "name": "fa_tfpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
