{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e941c40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device name CPU\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    device_name = tf.test.gpu_device_name()\n",
    "else:\n",
    "    device_name = 'CPU'\n",
    "device_name = 'CPU' \n",
    "print(f'device name {device_name}')\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "#clean up these imports for unused later\n",
    "from math import cos,sin,tan,asin,acos,radians,sqrt,degrees,atan,atan2,copysign\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "import scipy\n",
    "from scipy.stats import norm\n",
    "import random\n",
    "import time\n",
    "import timeit\n",
    "import math\n",
    "import localization as lx\n",
    "import gzip\n",
    "\n",
    "import util.npose_util as nu\n",
    "import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "\n",
    "import joblib\n",
    "from sklearn.manifold import MDS\n",
    "import argparse\n",
    "from functools import partial\n",
    "from itertools import starmap,repeat,permutations\n",
    "\n",
    "from pymol import cmd, stored, selector\n",
    "\n",
    "import GenerateEndpoints as ge\n",
    "import HelixFit as hf\n",
    "import FitTransform as ft\n",
    "\n",
    "import seaborn as sns\n",
    "import util.RotationMethods as rm\n",
    "    \n",
    "    \n",
    "    \n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "\n",
    "#load distance maps and endpoints dataset for initializing start\n",
    "def load_distance_map(name, dm_file='data/Fits_4H_dm_phi.npz'):\n",
    "    rr = np.load(dm_file, allow_pickle=True)\n",
    "    X_train, y_train , featNames = [rr[f] for f in rr.files]\n",
    "    \n",
    "    \n",
    "    return X_train[y_train==name][:,:-4]\n",
    "\n",
    "def index_helix_ep(ep_in,helices_desired=[0,1],num_helices=4):\n",
    "    \n",
    "    num_ep = num_helices*2\n",
    "    hi = np.array(helices_desired,dtype=int)\n",
    "    h_ep = np.array(range(num_ep)).reshape((-1,2)) #generate helix to endpoint mapping\n",
    "    \n",
    "    #alternate example for indexing batch of X \n",
    "    #X.reshape((X.shape[0],-1))[:,indexarray]\n",
    "    \n",
    "    #select desired endpoints from  batch of endpoints\n",
    "    return ep_in[np.ix_(np.array(range(ep_in.shape[0])),h_ep[hi].flatten(), np.array(range(ep_in.shape[2])))]\n",
    "    \n",
    "def get_midpoint(ep_in,helices_desired=[0,1],num_helices=4):\n",
    "    \n",
    "    num_ep = num_helices*2\n",
    "    \n",
    "    ind_ep = index_helix_ep(ep_in, helices_desired=helices_desired, num_helices=4)\n",
    "    \n",
    "    #calculate midpoint\n",
    "    midpoint = ind_ep.sum(axis=1)/np.repeat(ind_ep.shape[1], ind_ep.shape[2])\n",
    "    \n",
    "    return midpoint\n",
    "\n",
    "def get_stubs_from_points(ep_in,index=[0,1,2]):\n",
    "#def get_stubs_from_n_ca_c(n, ca, c):\n",
    "    \"\"\"Modified from Brian's npose code  get_stubs_from_n_ca_c, index references 3 points, to define plane.\n",
    "    \"\"\"\n",
    "    e1 = ep_in[:,index[1]]-ep_in[:,index[0]]\n",
    "    e1 = np.divide( e1, np.linalg.norm(e1, axis=1)[..., None] )\n",
    "\n",
    "    e3 = np.cross( e1, ep_in[:,index[2]]-ep_in[:,index[0]], axis=1 )\n",
    "    e3 = np.divide( e3, np.linalg.norm(e3, axis=1)[..., None] )\n",
    "\n",
    "    e2 = np.cross( e3, e1, axis=1 )\n",
    "\n",
    "    stub = np.zeros((len(ep_in), 4, 4))\n",
    "    stub[...,:3,0] = e1\n",
    "    stub[...,:3,1] = e2\n",
    "    stub[...,:3,2] = e3\n",
    "    stub[...,:3,3] = ep_in[:,index[1]]\n",
    "    stub[...,3,3] = 1.0\n",
    "\n",
    "    return stub\n",
    "\n",
    "def xform_npose_2batch(xform, npose):\n",
    "    #single batch code  util.npose_util as xform_npose\n",
    "    return np.matmul(np.repeat(xform[:,np.newaxis,...],npose.shape[1],axis=1),npose[...,None]).squeeze(-1)\n",
    "\n",
    "def xform_to_z_plane(mobile, index_mobile=[0,1,2]):\n",
    "    \"\"\"rotate points into the z-plane for trilaterization. needs additional translation/reflection\"\"\"\n",
    "\n",
    "    mobile_stub = get_stubs_from_points(mobile, index=index_mobile)\n",
    "    mobile_stub_inv = np.linalg.inv(mobile_stub)\n",
    "    \n",
    "    z_plane_ref = np.repeat(np.array([[[0,0,0],[1,0,0],[1,1,0]]]), mobile.shape[0],axis=0)\n",
    "\n",
    "    ref_stub = get_stubs_from_points(z_plane_ref, index=[0,1,2])\n",
    "\n",
    "    xform = ref_stub @ mobile_stub_inv\n",
    "\n",
    "    return xform\n",
    "\n",
    "\n",
    "def rotate_base_tri_Zplane(endpoint_midpoints, target_point=4, index_mobile=[1,2,3], returnRotMat=False):\n",
    "    \"\"\"rotate points into the z-plane for trilaterization. Target point ensures that point is positive in Z\"\"\"\n",
    "    tp = target_point #target point\n",
    "    zplanexform = xform_to_z_plane(endpoint_midpoints,index_mobile=index_mobile) #one index start base triangle, default\n",
    "    #add one for npose rot calc\n",
    "    npose = np.concatenate((endpoint_midpoints, np.ones((endpoint_midpoints.shape[0],\n",
    "                                                         endpoint_midpoints.shape[1],1))),axis=2) \n",
    "    rot = xform_npose_2batch(zplanexform,npose) # double batch matrix multiplication, see npose, for one batch\n",
    "\n",
    "    #translate X domain to place first index of \"index_mobile\" to 0,0,0\n",
    "    rot[:,:,0] = rot[:,:,0]-np.expand_dims(rot[:,index_mobile[0],0],axis=1)\n",
    "    #based on target point guaranteed to be positive\n",
    "    #reflect new points across the z axis to positive if negative to match just choosing positive solutions\n",
    "    rot[...,2][rot[:,tp,2]<0] = -rot[...,2][rot[:,tp,2]<0]\n",
    "    \n",
    "    if not returnRotMat:\n",
    "        return rot[...,:3] #remove npose rotate dimension\n",
    "    else:\n",
    "        return rot[...,:3], zplanexform\n",
    "\n",
    "\n",
    "\n",
    "#-----------------methods to index needed indices from generator------------------------\n",
    "\n",
    "def helix_dindex(helices_to_keep, num_helices=4, intraHelixDist=True):\n",
    "    \"\"\"Get index values for parts of the distance map\"\"\"\n",
    "    \n",
    "    #prep indices for distance map\n",
    "    num_ep = num_helices*2\n",
    "    mat_ind = np.array(range((num_ep)**2)).reshape((num_ep,num_ep))\n",
    "    iu1 = np.triu_indices(num_ep, 1)\n",
    "    \n",
    "    helix_used = np.array(helices_to_keep,dtype=int)\n",
    "    \n",
    "    h_ep = np.array(range(num_ep)).reshape((-1,2)) #generate helix to endpoint mapping\n",
    "    \n",
    "    tot_ind = []\n",
    "    \n",
    "    if intraHelixDist:\n",
    "        #get indices of distance map that correspond to each helix, overlap is distances between specified endpoints\n",
    "        for x in helix_used:\n",
    "            new_ind = np.intersect1d(mat_ind[h_ep[x]], mat_ind.T[h_ep[x]])\n",
    "            tot_ind.extend(new_ind)\n",
    "    \n",
    "    \n",
    "    for x in permutations(helix_used,2):\n",
    "        new_ind = np.intersect1d(mat_ind[h_ep[x[0]]], mat_ind.T[h_ep[x[1]]])\n",
    "        tot_ind.extend(new_ind)\n",
    "    \n",
    "    #convert to generator indices (indices of iu1 array)\n",
    "    out_ind = []\n",
    "    for x in tot_ind:\n",
    "        if len(np.nonzero(mat_ind[iu1]==x)[0])>0:\n",
    "            out_ind.append(np.nonzero(mat_ind[iu1]==x))\n",
    "\n",
    "    return np.sort(np.array(out_ind).flatten())\n",
    "\n",
    "\n",
    "def point_dindex(target_points, ref=[4], num_helices = 4):\n",
    "    \n",
    "    num_ep = num_helices*2\n",
    "    mat_ind = np.array(range((num_ep)**2)).reshape((num_ep,num_ep))\n",
    "    iu1 = np.triu_indices(num_ep, 1)\n",
    "    \n",
    "    dindex = []\n",
    "    \n",
    "    for tp in target_points:\n",
    "        for ref_ind in ref:\n",
    "            dindex.append(mat_ind[ref_ind,tp]) #indices for distances to target point\n",
    "    \n",
    "    dindex = np.array(dindex)\n",
    "    \n",
    "    out_ind = []\n",
    "    for x in dindex.flatten():\n",
    "        out_ind.append(np.nonzero(mat_ind[iu1]==x))\n",
    "        \n",
    "    out_ind = np.array(out_ind)\n",
    "    \n",
    "    return out_ind.reshape((len(target_points),-1))\n",
    "\n",
    "def target_dindex(target_points, oneRef = True, num_helices = 5, baseTri_out=True):\n",
    "    \"\"\"Distance map indices for base triangle and output distance map\"\"\"\n",
    "    \n",
    "    num_ep = num_helices*2\n",
    "    mat_ind = np.array(range((num_ep)**2)).reshape((num_ep,num_ep))\n",
    "    iu1 = np.triu_indices(num_ep, 1)\n",
    "\n",
    "    if oneRef:\n",
    "        ref = [1,2,3]\n",
    "        base_tri = [mat_ind[1][2],mat_ind[2][3],mat_ind[1][3]] #p1 to p2, p2 to p3, p1 to p3\n",
    "        \n",
    "    else:\n",
    "        ref = [0,1,2]\n",
    "        base_tri = [mat_ind[0][1],mat_ind[1][2],mat_ind[0][3]] #p0 to p1, p1 to p2, p0 to p3\n",
    "    \n",
    "    dindex = []\n",
    "    \n",
    "    for tp in target_points:\n",
    "        dindex.append(mat_ind[ref,tp]) #indices for distances to target point\n",
    "    \n",
    "    dindex = np.array(dindex)\n",
    "    \n",
    "    out_ind = []\n",
    "    for x in dindex.flatten():\n",
    "        out_ind.append(np.nonzero(mat_ind[iu1]==x))\n",
    "        \n",
    "    out_ind = np.array(out_ind)\n",
    "    \n",
    "    return out_ind.reshape((-1,len(base_tri))),base_tri\n",
    "\n",
    "def minMax_indices(distance_index, point_index, minmax_obj):\n",
    "    \n",
    "    #assemble conversions \n",
    "    #converts output from generator back to real distances\n",
    "    dMin_all = tf.convert_to_tensor(minmax_obj.data_min_, dtype=tf.float32)\n",
    "    mScale_all = tf.convert_to_tensor(minmax_obj.scale_, dtype = tf.float32)\n",
    "    mMin = tf.convert_to_tensor(minmax_obj.feature_range[0], dtype = tf.float32)\n",
    "\n",
    "    #index just the distances we need for calculation\n",
    "    dMin = tf.gather(dMin_all, distance_index,axis=0)\n",
    "    mScale = tf.gather(mScale_all, distance_index,axis=0)\n",
    "\n",
    "    #indexes we need to determine the +/- z value of the new points\n",
    "    pindex = point_dindex([5,6,7], ref=[4], num_helices = 4)\n",
    "    dMin_nwp = tf.gather(dMin_all, point_index,axis=0)\n",
    "    mScale_nwp = tf.gather(mScale_all, point_index,axis=0)\n",
    "    \n",
    "    return dMin, mScale, mMin, dMin_nwp,  mScale_nwp \n",
    "\n",
    "def ref_distmap_index(distances, num_helices = 4):\n",
    "    \n",
    "    num_ep = num_helices*2\n",
    "    mat_ind = np.array(range((num_ep)**2)).reshape((num_ep,num_ep))\n",
    "    iu1 = np.triu_indices(num_ep, 1)\n",
    "    iu1_flat = iu1[0]*num_ep+iu1[1]\n",
    "    \n",
    "    return distances[np.ix_(range(distances.shape[0]),iu1_flat)]\n",
    "\n",
    "def convert_dMat_to_iu1_index(indices_in, num_helices = 4):\n",
    "    \"\"\"Converts indices on flattened distance index to iu1 single indices\"\"\"\n",
    "    \n",
    "    \n",
    "    conv_array = np.array(indices_in).flatten()\n",
    "    \n",
    "    num_ep = num_helices*2\n",
    "    mat_ind = np.array(range((num_ep)**2)).reshape((num_ep,num_ep))\n",
    "    iu1 = np.triu_indices(num_ep, 1)\n",
    "    \n",
    "    #convert to generator indices (indices of iu1 array)\n",
    "    out_ind = []\n",
    "    for x in conv_array:\n",
    "        if len(np.nonzero(mat_ind[iu1]==x)[0])>0:\n",
    "            out_ind.append(np.nonzero(mat_ind[iu1]==x))\n",
    "            \n",
    "    out_ind = np.array(out_ind)\n",
    "        \n",
    "    return out_ind.reshape(conv_array.shape)\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "def prep_base_triangle_trilateriation(dindex, base_tri, distance_map):\n",
    "    \"\"\"Return x,y,z coords on z-plane of base triangle of tetrahedron from a distance map.\"\"\"\n",
    "    \n",
    "#     dindex, base_tri = target_dindex(targ_dind, oneRef = oneRef, num_helices = num_helices)\n",
    "# #     print(dindex)\n",
    "# #     print(base_tri)\n",
    "    \n",
    "    #test case input data: prep base triangles for trilateration at zplane, (0,0,0) (dvar,0,0) (ivar,jvar,0)\n",
    "    desired_dm = distance_map[:, base_tri] #base tri from dindex\n",
    "\n",
    "    dvar_index = tf.convert_to_tensor(0 ,dtype=tf.int32)\n",
    "    s2_index = tf.convert_to_tensor(2 ,dtype=tf.int32) # we would like the angle across from side 2\n",
    "    s3_index = tf.convert_to_tensor(1 ,dtype=tf.int32)\n",
    "\n",
    "    #x value representing center of 2nd sphere at (dvar,0,0) aka s1\n",
    "    dvar = tf.reshape(tf.gather(desired_dm, dvar_index,axis=1),(-1,1)) #side 1\n",
    "    s2 = tf.reshape(tf.gather(desired_dm,   s2_index,axis=1),(-1,1))\n",
    "    s3 = tf.reshape(tf.gather(desired_dm,   s3_index,axis=1),(-1,1))\n",
    "\n",
    "    #calculate the opposite angle of the the third side of base triangle using law of cosines\n",
    "    s1sq = tf.square(dvar)\n",
    "    s2sq = tf.square(s2)\n",
    "    s3sq = tf.square(s3)\n",
    "    ang3 = np.arccos((-s3sq+s2sq+s1sq)/(2*dvar*s2))\n",
    "\n",
    "    #take third point of base triangle via distance * vector\n",
    "    v13 = tf.concat([tf.cos(ang3), tf.sin(ang3), tf.zeros_like(ang3)], axis=1)\n",
    "    p3 = s2*v13\n",
    "    #center points of 3rd sphere\n",
    "    ivar = tf.reshape(p3[:,0],(-1,1))\n",
    "    jvar = tf.reshape(p3[:,1],(-1,1))\n",
    "\n",
    "\n",
    "    #convert all to float32 to match generator output\n",
    "    #expand to dindex size \n",
    "\n",
    "    dvar = tf.cast(tf.repeat(dvar,dindex.shape[0],axis=1),dtype=tf.float32)\n",
    "    ivar = tf.cast(tf.repeat(ivar,dindex.shape[0],axis=1),dtype=tf.float32)\n",
    "    jvar = tf.cast(tf.repeat(jvar,dindex.shape[0],axis=1),dtype=tf.float32)\n",
    "    \n",
    "    return dvar, ivar, jvar\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7673d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions for back propagation\n",
    "\n",
    "\n",
    "@tf.function \n",
    "def train_step(input_z_var,ref_map_, helix_keep_mask_,\n",
    "               target_mp_, dvar_, ivar_, jvar_,\n",
    "                dMin_, mScale_, mMin_, dMin_nwp_, mScale_nwp_, \n",
    "                dindex_, pindex_, batch_,z_reflect_ind_, scale_):\n",
    "\n",
    "    with tf.GradientTape() as g_tape:\n",
    "        g_tape.watch(input_z_var)\n",
    "        g_o = gen_obj.g(input_z_var)\n",
    "        masked_loss = maskLoss(ref_map_, g_o, helix_keep_mask_)\n",
    "        mp_loss  = tf.divide(midpoints_loss(g_o, target_mp_, \n",
    "                    dvar_, ivar_, jvar_,\n",
    "                    dMin_, mScale_, mMin_, dMin_nwp_, mScale_nwp_, \n",
    "                    dindex_, pindex_, batch_,z_reflect_ind_), scale_)\n",
    "\n",
    "        loss = tf.reduce_sum(mp_loss,axis=1) + tf.reduce_sum(masked_loss,axis=1)\n",
    "\n",
    "    g_grads = g_tape.gradient(loss, input_z_var)\n",
    "    optimizer.apply_gradients(zip([g_grads],[input_z_var]))\n",
    "\n",
    "    return input_z_var, masked_loss, mp_loss\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def maskLoss(y_actual, y_pred,mask):\n",
    "    \"\"\"Loss Function for mantaing shape of input helices\"\"\"\n",
    "    custom_loss_val = tf.multiply(mask,tf.square(y_actual-y_pred))\n",
    "    return custom_loss_val\n",
    "\n",
    "@tf.function\n",
    "def midpoints_loss(g1, target, \n",
    "                   dvar, ivar, jvar,\n",
    "                   dMin, mScale, mMin, dMin_nwp, mScale_nwp, \n",
    "                   dindex, pindex, batch_size, zr_ind):\n",
    "    \"\"\"Loss function to move output of two generated helices to target midpoint\"\"\"\n",
    "\n",
    "\n",
    "    #now using dindex gather the desired indices for tetrahedron calcs\n",
    "\n",
    "    #radius of the spheres, aka the distances to unmasked endpoints\n",
    "    g2 = tf.gather(g1,dindex,axis=1)\n",
    "\n",
    "    #see https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler\n",
    "    #inspect .scale_\n",
    "    conv_dist = tf.add(tf.divide(tf.subtract(g2, mMin), mScale),dMin)\n",
    "    #transpose lets you easily grab all distances with gather/axis \n",
    "    conv_dist_squared = tf.transpose(tf.square(conv_dist),perm=[0, 2, 1]) \n",
    "\n",
    "    r1_sq = tf.gather(conv_dist_squared, 0, axis=1) \n",
    "    r2_sq =  tf.gather(conv_dist_squared,1, axis=1) \n",
    "    r3_sq = tf.gather(conv_dist_squared, 2, axis=1)\n",
    "\n",
    "    #calculate coordinates of spherial intersect\n",
    "    x = tf.divide(tf.add(tf.subtract(r1_sq,r2_sq),tf.square(dvar)),tf.multiply(2.0,dvar))\n",
    "    y1 = tf.divide(tf.add(tf.add(tf.subtract(r1_sq,r3_sq), tf.square(ivar)), tf.square(jvar)),tf.multiply(2.0,jvar))\n",
    "    y = tf.subtract(y1,tf.multiply(tf.divide(ivar,jvar),x))\n",
    "\n",
    "    pre_z = tf.subtract(tf.subtract(r1_sq,tf.square(x)),tf.square(y))\n",
    "    fixed_z = tf.clip_by_value(pre_z, 1e-10, 100)\n",
    "\n",
    "    #adds  to negative values to 0 for sqrt,\n",
    "    #I think is okay as zero z will imply lengthening of distances to match a non-zero target midpoint,\n",
    "    #pushing the network in the desired direction?\n",
    "\n",
    "    z = tf.sqrt(fixed_z) #assume positive solution\n",
    "    z_neg = tf.multiply(z,-1) #assume negative soluation\n",
    "\n",
    "    #new points, with both assumptions\n",
    "    nwp = tf.concat((tf.reshape(x,(batch_size,-1,1)),\n",
    "                    tf.reshape(y,(batch_size,-1,1)),\n",
    "                    tf.reshape(z,(batch_size,-1,1))), axis=2)  #\n",
    "\n",
    "    nwp_negz = tf.concat((tf.reshape(x,(batch_size,-1,1)),\n",
    "                    tf.reshape(y,(batch_size,-1,1)),\n",
    "                    tf.reshape(z_neg,(batch_size,-1,1))), axis=2)  #\n",
    "\n",
    "    #some positive solutions assumptions,\n",
    "    # assume first [i4] is actual positive use remaining distances of i4 to (i5,i6,i7) to determine z sign\n",
    "    # closest to matching distance is used\n",
    "\n",
    "\n",
    "    #let's start by calculating all i4 to (i5,i6,i7) distances\n",
    "\n",
    "    #stop the gradients since these are used to index gather and scatter\n",
    "    #unsqueeze at two different dimensionsq to broadcast into matrix MX1 by 1XN to MXN \n",
    "    nwp_p =  tf.stop_gradient(tf.expand_dims(nwp,axis=1) - tf.expand_dims(nwp,axis=2))\n",
    "    nwp_n =  tf.stop_gradient(tf.expand_dims(nwp,axis=1) - tf.expand_dims(nwp_negz,axis=2))\n",
    "\n",
    "    nwp_dist_pz = tf.reshape(tf.sqrt(tf.reduce_sum(tf.square(nwp_p), 3)),(-1,4,4)) #distance calc +1e6?\n",
    "    nwp_dist_nz = tf.reshape(tf.sqrt(tf.reduce_sum(tf.square(nwp_n), 3)),(-1,4,4))  #distance calc\n",
    "\n",
    "    z_pn_dist_pre_con = tf.gather(g1,pindex,axis=1)\n",
    "    z_pn_dist = tf.add(tf.divide(tf.subtract(z_pn_dist_pre_con, mMin), mScale_nwp),dMin_nwp)\n",
    "\n",
    "    #index p4 to p5,p6,p7\n",
    "    #rewrite as non-slice version of this\n",
    "\n",
    "    nwp_dist_pz_c = tf.squeeze(tf.gather(tf.gather(nwp_dist_pz, [0], axis=1), [1,2,3], axis=2))\n",
    "    nwp_dist_nz_c = tf.squeeze(tf.gather(tf.gather(nwp_dist_nz, [0], axis=1), [1,2,3], axis=2))\n",
    "\n",
    "    nwp_dist_pz_c = tf.expand_dims(nwp_dist_pz_c,axis=2)\n",
    "    nwp_dist_nz_c = tf.expand_dims(nwp_dist_nz_c,axis=2)\n",
    "\n",
    "    # #using a single distance decide the z assumption and apply\n",
    "    correct_z_assum = tf.abs(z_pn_dist - nwp_dist_nz_c) < tf.abs(z_pn_dist - nwp_dist_pz_c)\n",
    "    cz = tf.squeeze(tf.multiply(tf.cast(correct_z_assum,tf.int32),-2))\n",
    "\n",
    "    z_reflect_tensor = tf.ones_like(nwp, dtype=tf.int32)\n",
    "\n",
    "    nwp_mult = tf.cast(tf.tensor_scatter_nd_add(z_reflect_tensor, zr_ind, cz),dtype=tf.float32)\n",
    "    nwp_final = tf.multiply(nwp_mult,nwp)\n",
    "\n",
    "    midpoint = tf.reduce_mean(nwp_final,axis=1)\n",
    "    return tf.square(tf.subtract(midpoint,target)) # means squared loss to desired midpoint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f6ca4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[23]:\n",
    "\n",
    "\n",
    "def fullBUTT_GPU(gen_obj, ref_map, target_mp_in, batch_size=32,cycles=100, input_z=None, \n",
    "                          rate=0.05, target_ep=[4,5,6,7], num_helices=4, oneRef=True,\n",
    "                          scale=5.0, z_size=12, print_loss=False):\n",
    "    \n",
    "    batch_indices = np.repeat(np.array(range(ref_map.shape[0])),batch_size)\n",
    "    batch = batch_indices.shape[0]\n",
    "    target_mp = tf.convert_to_tensor(np.repeat(target_mp_in, batch_size,axis=0),dtype=tf.float32)\n",
    "    ref_map = np.repeat(ref_map, batch_size, axis=0)\n",
    "    \n",
    "    #establish indices for distances to reference\n",
    "    #prep base triangle, convert distances from minmax to regular\n",
    "    dindex, base_tri = target_dindex(target_ep, oneRef = True, num_helices = num_helices)\n",
    "    base_tri = convert_dMat_to_iu1_index(base_tri) #dirty\n",
    "    pindex = point_dindex(target_ep[1:], ref=[target_ep[0]], num_helices = num_helices)\n",
    "\n",
    "    #convert generator output to 'real distances'\n",
    "    #dMin, mScale, mMin, dMin_nwp,  mScale_nwp = minMax_indices(dindex, pindex, brec.mm)\n",
    "    mmTuple = minMax_indices(dindex, pindex, gen_obj.mm)\n",
    "\n",
    "    # prepare base triangle for trilateriation (z plane , p1 at origin, p2 positive x)\n",
    "    #dvar, ivar, jvar = prep_base_triangle_trilateriation(dist[:batch], targ_dind = [4,5,6,7], oneRef = True, num_helices=4)\n",
    "    #baseTuple = prep_base_triangle_trilateriation(dist[:batch], targ_dind = target_ep, oneRef = True, num_helices=num_helices)\n",
    "\n",
    "    baseTuple = prep_base_triangle_trilateriation(dindex, base_tri, ref_map)\n",
    "    \n",
    "    dMin, mScale, mMin, dMin_nwp,  mScale_nwp = mmTuple\n",
    "    dvar,ivar,jvar = baseTuple\n",
    "    \n",
    "    #mask for keeping buttress helices in same orientation\n",
    "    h_index = helix_dindex([0,1], num_helices=4, intraHelixDist=True)\n",
    "    helix_keep_mask = np.zeros((ref_map.shape[1],),dtype=np.int32)\n",
    "    helix_keep_mask[h_index] = 1\n",
    "    helix_keep_mask = tf.convert_to_tensor(helix_keep_mask,dtype=tf.float32)\n",
    "\n",
    "    #input to generator (determinstic output)\n",
    "    \n",
    "    ref_map_conv = gen_obj.mm.transform(ref_map) #map to keep helices same as input\n",
    "    #controlling z reflection during trilaterization\n",
    "    z_r_innerInd = np.repeat(tf.convert_to_tensor([[[1,2],[2,2],[3,2]]]),batch,axis=0)\n",
    "    #batch index\n",
    "    zfi_bi =np.expand_dims(np.array(range(batch)).reshape((-1,1)).repeat(3,axis=1),axis=2)\n",
    "    z_reflect_ind = np.concatenate((zfi_bi,z_r_innerInd),axis=2)\n",
    "    \n",
    "    \n",
    "    if input_z is None:\n",
    "        input_z = tf.random.uniform(shape=(batch, z_size), minval=-1, maxval=1)\n",
    "        \n",
    "    with tf.device(device_name):\n",
    "        input_z_var = tf.Variable(input_z)\n",
    "        ref_map_ = tf.constant(ref_map_conv,dtype=tf.float32)\n",
    "        scale_, z_reflect_ind_ = tf.constant(scale), tf.constant(z_reflect_ind)\n",
    "        target_mp_, batch_ = tf.constant(target_mp),  tf.constant(batch)\n",
    "        dindex_, pindex_ = tf.constant(dindex), tf.constant(pindex)\n",
    "        dMin_, mScale_, mMin_ = tf.constant(dMin), tf.constant(mScale), tf.constant(mMin),\n",
    "        dMin_nwp_,  mScale_nwp_ =  tf.constant(dMin_nwp),  tf.constant(mScale_nwp)\n",
    "        dvar_, ivar_, jvar_ = tf.constant(dvar), tf.constant(ivar), tf.constant(jvar)\n",
    "        helix_keep_mask_ = tf.constant(helix_keep_mask, dtype=tf.float32)\n",
    "\n",
    "    #store grads and inputs as we backpropagate\n",
    "    z=[]\n",
    "    loss_mask = []\n",
    "    loss_mp = []\n",
    "    grads = []\n",
    "    \n",
    "\n",
    "    if print_loss:\n",
    "        g_o = gen_obj.g(input_z_var)\n",
    "        masked_loss = maskLoss(ref_map_, g_o, helix_keep_mask_)\n",
    "\n",
    "        mp_loss  = tf.divide(midpoints_loss(g_o, target_mp_, \n",
    "                            dvar_, ivar_, jvar_,\n",
    "                            dMin_, mScale_, mMin_, dMin_nwp_, mScale_nwp_, \n",
    "                            dindex_, pindex_, batch_, z_reflect_ind_), scale_)\n",
    "\n",
    "        print('start_masked',np.round(np.sum(masked_loss),2))\n",
    "        print('start_mp',np.round(np.sum(mp_loss),2))\n",
    "    \n",
    "    for t in range(1,cycles):\n",
    "        \n",
    "        in_z, mask_l, mp_l = train_step(input_z_var,ref_map_, helix_keep_mask_,\n",
    "                                        target_mp_, dvar_, ivar_, jvar_,\n",
    "                                        dMin_, mScale_, mMin_, dMin_nwp_, mScale_nwp_, \n",
    "                                        dindex_, pindex_, batch_, z_reflect_ind_, scale_)\n",
    "        \n",
    "        z.append(in_z.numpy())\n",
    "        loss_mask.append(mask_l.numpy())\n",
    "        loss_mp.append(mp_l.numpy())\n",
    "\n",
    "    if print_loss:\n",
    "        print('end_masked', np.round(np.sum(loss_mask[-1]),2))\n",
    "        print('end_mp', np.round(np.sum(loss_mp[-1]),2))\n",
    "    \n",
    "    return z, loss_mask, loss_mp, batch_indices\n",
    "    \n",
    "#give backpropagated generator output, find the best outputs based on loss\n",
    "def buttress_ep_from_z(gen_obj, gen_z, starting_ep , loss_midpoint, loss_masked, batchIndices,\n",
    "                       max_loss_mp = 0.001, max_loss_mask = 0.001):\n",
    "    \n",
    "    \n",
    "    best_mp = np.sum(loss_midpoint<max_loss_mp,axis=1)>2 # 3 total mp loss outputs (x,y,z of midpoint to target)\n",
    "    best_mask = np.sum(loss_masked<max_loss_mask,axis=1)>27 # 28 total mask loss point (2 helices)\n",
    "\n",
    "    mask_mp_bool = np.logical_and(best_mp, best_mask)       \n",
    "\n",
    "    identified_z = gen_z[mask_mp_bool]\n",
    "    print(f'Outputs passing filters: {len(identified_z)}')\n",
    "    print(f'Total Outputs: {len(gen_z)}')\n",
    "    uInd = batchIndices[mask_mp_bool]\n",
    "    \n",
    "    orig_ep = starting_ep[uInd]\n",
    "    \n",
    "    \n",
    "    gen_obj.generate(z=12, input_z = identified_z, batch_size=identified_z.shape[0])\n",
    "    gen_obj.MDS_reconstruct_()\n",
    "    \n",
    "    out_ep = np.array(gen_obj.reconsMDS_)\n",
    "    \n",
    "    return align_generated_to_starting_ep(out_ep, orig_ep)\n",
    "\n",
    "def buttress_ep_from_z_mask_only(gen_obj, gen_z,loss_masked, batchIndices,\n",
    "                                 max_loss_mask = 0.002, max_out=100, print_stats= False):\n",
    "    \n",
    "    \n",
    "    sm = np.sum(loss_masked,axis=1)\n",
    "    smi = np.argsort(sm)\n",
    "    sm_sort = sm[smi]\n",
    "    best_mask = sm_sort < max_loss_mask\n",
    "    \n",
    "    ind2 = np.array(range(len(sm)))\n",
    "    #uInd = batchIndices[smi][best_mask]\n",
    "    uiInd = ind2[smi][best_mask]\n",
    "    uInd = batchIndices[uiInd]\n",
    "    \n",
    "    if print_stats:\n",
    "        print('Input Size: ',      len(sm))\n",
    "        print('Passing Filters: ', len(uInd))\n",
    "    \n",
    "    \n",
    "    if len(uInd)>max_out:\n",
    "        uInd = uInd[:max_out]\n",
    "        uiInd = uiInd[:max_out]\n",
    "    \n",
    "    identified_z = gen_z[uiInd]\n",
    "    \n",
    "    gen_obj.generate(z=12, input_z = identified_z, batch_size=identified_z.shape[0])\n",
    "    gen_obj.MDS_reconstruct_()\n",
    "    \n",
    "    out_ep = np.array(gen_obj.reconsMDS_)\n",
    "    \n",
    "    return out_ep, uInd\n",
    "\n",
    "def random_reduce(arrayList, num_to_keep = 20):\n",
    "    \n",
    "    if num_to_keep > arrayList.shape[0]:\n",
    "        num_to_keep =  arrayList.shape[0]\n",
    "    \n",
    "    indexer = np.random.choice(range(arrayList.shape[0]),num_to_keep ,replace=False)\n",
    "    \n",
    "    return indexer\n",
    "\n",
    "    \n",
    "\n",
    "#the output of generator will not perfectly match the desired input, get best fit via Kabsch\n",
    "def align_generated_to_starting_ep(gen_ep, orig_ep, target_mp=None):\n",
    "    \"\"\"Uses Kabsh to align generated endpoints onto original endpoints. Orig_Ep on origin with oneRef.\"\"\"\n",
    "    #Thanks to below for this code; modified to batch form\n",
    "    #moves gen_\n",
    "    #https://pymolwiki.org/index.php/Kabsch\n",
    "\n",
    "    #only center on first four points [first two helices]\n",
    "    gen_ep_4 = gen_ep[:,:4,:].copy()\n",
    "    orig_ep_4 = orig_ep[:,:4,:].copy()\n",
    "    \n",
    "    \n",
    "    #centering to prevent affine transformaiton\n",
    "    COM_orig = np.expand_dims(np.sum(orig_ep_4, axis=1)/orig_ep_4.shape[1]   ,axis=1)\n",
    "    COM_gen =  np.expand_dims(np.sum(gen_ep_4,  axis=1)/gen_ep_4.shape[1]     ,axis=1)\n",
    "    \n",
    "    gen_ep_4_cen = gen_ep_4 - COM_gen\n",
    "    orig_ep_4_cen = orig_ep_4 - COM_orig\n",
    "    \n",
    "    #initial error estimate\n",
    "    #E0 = np.sum( np.sum(np.square(gen_ep_4),axis=1),axis=1) + np.sum( np.sum(np.square(orig_ep_4),axis=1),axis=1)\n",
    "\n",
    "    # This beautiful step provides the answer.  V and Wt are the orthonormal\n",
    "    # bases that when multiplied by each other give us the rotation matrix, U.\n",
    "    # S, (Sigma, from SVD) provides us with the error!  Isn't SVD great!                                            #2                      #1\n",
    "    V, S, Wt = np.linalg.svd( np.matmul(np.transpose(gen_ep_4_cen, axes=[0,2,1]), orig_ep_4_cen))\n",
    "\n",
    "    # we already have our solution, in the results from SVD.\n",
    "    # we just need to check for reflections and then produce\n",
    "    # the rotation.  V and Wt are orthonormal, so their det's\n",
    "    # are +/-1.\n",
    "    reflect = np.linalg.det(V) * np.linalg.det(Wt)\n",
    "    #original solution, I will take both reflections\n",
    "    #multiples by 1 or -1 depending if relfect is negative (reflection)\n",
    "    proper_reflection = ((reflect>0).astype(np.int32)*-2+1) \n",
    "    S[:,-1] = S[:,-1]*proper_reflection\n",
    "    V[:,:,-1] = -V[:,:,-1]*proper_reflection.reshape((-1,1))\n",
    "\n",
    "    V_reflect = V.copy()\n",
    "    V_reflect[:,:,-1] = -V_reflect[:,:,-1]\n",
    "    #Error\n",
    "#     RMSD = E0 - (2.0 * np.sum(S,axis=1))\n",
    "#     RMSD = np.sqrt(np.abs(RMSD / L))\n",
    "    \n",
    "    #generate rotation matrices\n",
    "    U = np.matmul(V, Wt)\n",
    "    U_reflect = np.matmul(V_reflect, Wt)\n",
    "\n",
    "#    return target_mp,COM_gen,COM_orig\n",
    "    if target_mp:\n",
    "        final_target_midpoint = np.matmul(np.expand_dims((target_mp-COM_gen.squeeze(axis=1)),axis=1), U).squeeze() + COM_orig.squeeze(axis=1)\n",
    "    final_ep_full         = np.matmul((gen_ep-COM_gen), U) + COM_orig\n",
    "    final_ep_full_reflect = np.matmul((gen_ep-COM_gen), U_reflect) + COM_orig\n",
    "    \n",
    "    if target_mp:\n",
    "        return final_ep_full, final_ep_full_reflect, final_target_midpoint\n",
    "    else:\n",
    "        return final_ep_full, final_ep_full_reflect\n",
    "\n",
    "def guess_reflection(p, p_reflect, des_mp, invert=False):\n",
    "    p_mp = get_midpoint(p, helices_desired=[2,3], num_helices=4)\n",
    "    p_reflect_mp = get_midpoint(p_reflect, helices_desired=[2,3], num_helices=4)\n",
    "    \n",
    "    final_ep = np.zeros_like(p)\n",
    "\n",
    "    measure1 = np.linalg.norm(p_mp - des_mp,axis=1)\n",
    "    measure2 =  np.linalg.norm(p_reflect_mp - des_mp,axis=1)\n",
    "\n",
    "    if not invert:\n",
    "        final_ep[np.nonzero((measure1<measure2))] = p[np.nonzero((measure1<measure2))]\n",
    "        final_ep[np.nonzero((measure1>measure2))] = p_reflect[np.nonzero((measure1>measure2))]\n",
    "    else:\n",
    "        final_ep[np.nonzero((measure1>measure2))] = p[np.nonzero((measure1>measure2))]\n",
    "        final_ep[np.nonzero((measure1<measure2))] = p_reflect[np.nonzero((measure1<measure2))]\n",
    "        \n",
    "    \n",
    "    return final_ep\n",
    "\n",
    "def determine_reflection(input_points, input_points_reflect, reference_targets):\n",
    "    \n",
    "    i_mp = get_midpoint(input_points, helices_desired=[2,3]) \n",
    "    i_reflect_mp = get_midpoint(input_points_reflect, helices_desired=[2,3]) \n",
    "\n",
    "\n",
    "\n",
    "    #get a point on the desired line on mp dist away\n",
    "    dmp = np.linalg.norm(reference_targets-np.expand_dims(i_mp,axis=1),axis=2)\n",
    "    dmp_r = np.linalg.norm(reference_targets-np.expand_dims(i_reflect_mp,axis=1),axis=2)\n",
    "\n",
    "    am = np.argmin(np.abs(dmp),axis=1)\n",
    "    am_r = np.argmin(np.abs(dmp_r),axis=1)\n",
    "    #desired reflection is the further along the target point array, highest index\n",
    "    proper_reflect = am_r>am\n",
    "    \n",
    "\n",
    "    final = input_points.copy()\n",
    "    final[proper_reflect] = input_points_reflect[proper_reflect]\n",
    "    \n",
    "    return final\n",
    "\n",
    "def align_points_to_XYplane(input_points, keep_orig_trans=False):\n",
    "    \"\"\"Align batch of points to XY Plane.\"\"\"\n",
    "    \n",
    "    #example data set\n",
    "    #modified below to align set of points to the xy plane\n",
    "    #https://www.mathworks.com/matlabcentral/answers/255998-how-do-i-move-a-set-of-x-y-z-points-so-they-align-with-the-x-y-plane\n",
    "    \n",
    "    COM_start  = np.sum(input_points, axis=1)/input_points.shape[1]\n",
    "    input_points_ori = input_points - np.expand_dims(COM_start,axis=1)\n",
    "\n",
    "    #sp_ori = (start_points - np.expand_dims(COM_start,axis=1))[0]\n",
    "    V, S, Wt= np.linalg.svd(input_points_ori,full_matrices=False)\n",
    "    \n",
    "    #this indexing is opposite of the matlab output\n",
    "    axis= np.cross(Wt[:,2,:],[[0,0,1]]);\n",
    "    angle = -np.arctan2(np.linalg.norm(axis,axis=1), Wt[:,2,2])\n",
    "    \n",
    "    xf = []\n",
    "    for x in range(len(axis)):\n",
    "        xf.append(nu.xform_from_axis_angle_rad(axis[x], angle[x]))\n",
    "\n",
    "\n",
    "    xformR = np.array(xf)\n",
    "    R = xformR[:,0:3,0:3]\n",
    "    if keep_orig_trans:\n",
    "        A_rot = np.matmul(input_points_ori, R) + np.expand_dims(COM_start, axis=1)\n",
    "    else:\n",
    "        A_rot = np.matmul(input_points_ori, R) \n",
    "    \n",
    "    return A_rot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb3eb98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc50fec1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cd45ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reference_input(batch=1000):\n",
    "    rr = np.load(f'data/ep_for_X.npz', allow_pickle=True)\n",
    "    X = [rr[f] for f in rr.files][0]\n",
    "\n",
    "    dX = np.expand_dims(X,axis=1) - np.expand_dims(X,axis=2)\n",
    "    dist = np.sqrt(np.sum(dX**2, 3))  #+ 1e-6) #this dataset is good \n",
    "    dist = dist.reshape((dist.shape[0],-1))\n",
    "    mp_01 = get_midpoint(X,helices_desired=[0,1])\n",
    "    mp_23 = get_midpoint(X,helices_desired=[2,3])\n",
    "    #mp distance map\n",
    "    ep_mp = np.hstack((X.reshape((-1,24)),mp_01,mp_23)).reshape(-1,10,3) #helix12mp=8  helix34mp=9\n",
    "\n",
    "\n",
    "    #initiate array to hold endpoints\n",
    "     #mp deviation from guide points\n",
    "\n",
    "    #random sample starting endpoints to buttress\n",
    "    refi_all = list(range(ep_mp.shape[0]))\n",
    "    ref_ind = np.array(random.sample(refi_all , batch))\n",
    "    start_hel = ep_mp[ref_ind ,:4,...]\n",
    "    \n",
    "    return start_hel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a955b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device name /device:GPU:0\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "#ahhh recode this without the global nonsense at some point\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    device_name = tf.test.gpu_device_name()\n",
    "else:\n",
    "    device_name = 'CPU'\n",
    "rate=0.05\n",
    "# if ~devtype.__eq__(device_name):\n",
    "# device_name = 'CPU'\n",
    "print(f'device name {device_name}')\n",
    "\n",
    "gen=\"data/BestGenerator\"\n",
    "\n",
    "\n",
    "\n",
    "with tf.device(device_name):\n",
    "    gen_obj = ge.BatchRecon(gen)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=rate)\n",
    "output1=gen_obj.generate(z=12,batch_size=12) #example generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "080c3a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_hel = get_reference_input(batch=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ddd16948",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate guide points\n",
    "\n",
    "def generate_parabola(start, stop, h=[20,20,20], k=[20,30,40], num_points = 100):\n",
    "    #create a set of parabolas past through ([0,0]) start of helices to vertex ([[10,20],[10,30],[10,40]])\n",
    "    # (x-h)^2 = -4(a)(y-k)\n",
    "    # Solve for 'a' at x=0, y=0\n",
    "    # (0-h)^2 = -4(a)(0-k)\n",
    "    #      a  =  h^2/4k\n",
    "    h = np.array(h).reshape((-1,1))\n",
    "    k = np.array(k).reshape((-1,1))\n",
    "    a = np.square(h)/(4*k)\n",
    "    \n",
    "    #trace x from 0 to 10\n",
    "    # -4ay +4ak = (x-h)^2\n",
    "    # -4ay = (x-h)^2 - 4ak\n",
    "    #    y = ( (x-h)^2 - 4ak  ) / (-4a)\n",
    "    #    y = (4ak - (x-h)^2)\n",
    "    x = np.repeat(np.expand_dims(np.linspace(start,stop,num=100), axis=0), h.shape[0],axis=0)\n",
    "    z = np.divide(4*a*k -np.square(x-h), (4*a))\n",
    "   \n",
    "    x=np.expand_dims(x,axis=2)\n",
    "    z=np.expand_dims(z,axis=2)\n",
    "    y = np.zeros_like(x)\n",
    "    gen_para =  np.concatenate((x,y,z),axis = 2)\n",
    "    \n",
    "    return gen_para\n",
    "    \n",
    "#rotate parabola around the x axis\n",
    "# angleDeg = 90\n",
    "# gpi = 1\n",
    "# xfr=nu.xform_from_axis_angle_deg([1,0,0],angleDeg)\n",
    "# gp = np.hstack((g_para[gpi],np.ones_like(g_para[gpi,:,gpi].reshape((-1,1)))  ))\n",
    "# gp_z=nu.xform_npose(xfr,gp)[:,:3]\n",
    "# gp_z.shape\n",
    "\n",
    "# plt.scatter(gp_z[:,0], gp_z[:,2], alpha=0.5)\n",
    "# plt.axis('equal')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "def generate_pos_circle(start, stop, h=30, k=0, r=30, num_points= 100):\n",
    "    \"\"\"Generate guide points originating a the origin for a circle, positive values only. \"\"\"\n",
    "    #(x-h)^2 + (y-k)^2 = r^(2)\n",
    "    # y = sqrt(r^2-(x-h^2))+k\n",
    "    x = np.linspace(start,stop,num=num_points)\n",
    "    z = np.sqrt(np.square(r)-np.square(x-h))+k\n",
    "    y = np.zeros_like(x)\n",
    "    return np.concatenate((x.reshape((-1,1)),y.reshape((-1,1)),z.reshape((-1,1))),axis=1)\n",
    "\n",
    "def add_xline(inputP, line_length, num=60):\n",
    "    line = np.zeros((num,3))\n",
    "    line[:,0] = np.linspace(0, line_length, num=num)\n",
    "    cline = inputP[-1]+line\n",
    "    ci_line = np.concatenate((inputP,cline),axis=0)\n",
    "    \n",
    "    return ci_line\n",
    "\n",
    "circ_gp = generate_pos_circle(0,60,h=60,k=0,r=60, num_points= 200)\n",
    "ci_line = add_xline(circ_gp[:-1], 20, num=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "7cf7f878",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_helices= start_hel\n",
    "guide_points = ci_line\n",
    "batch=100\n",
    "next_mp_dist = 9\n",
    "mp_deviation_limit = 5\n",
    "maxOut=1000\n",
    "maxClash_num = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4b2147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "cc1a37e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max next indices 2\n",
      "(2000, 8, 3)\n",
      "Input Size: 800000\n",
      "backprop time :  53.266685247421265\n",
      "Passing Filters: 1221 \n",
      "MDS time:  29.249637365341187\n",
      "clash time:  0.6629998683929443\n",
      "final pass filter 1011\n",
      "max next indices 12\n",
      "(1011, 8, 3)\n",
      "Input Size: 404400\n",
      "backprop time :  31.153035163879395\n",
      "Passing Filters: 1168 \n",
      "MDS time:  25.630427360534668\n",
      "clash time:  0.6030006408691406\n",
      "final pass filter 434\n",
      "max next indices 28\n",
      "(434, 8, 3)\n",
      "Input Size: 173600\n",
      "backprop time :  14.481377601623535\n",
      "Passing Filters: 1061 \n",
      "MDS time:  23.354416608810425\n",
      "clash time:  0.4930000305175781\n",
      "final pass filter 257\n",
      "max next indices 51\n",
      "(257, 8, 3)\n",
      "Input Size: 102800\n",
      "backprop time :  9.519353151321411\n",
      "Passing Filters: 1011 \n",
      "MDS time:  22.619755744934082\n",
      "clash time:  0.49899864196777344\n",
      "final pass filter 274\n",
      "max next indices 74\n",
      "(274, 8, 3)\n",
      "Input Size: 109600\n",
      "backprop time :  9.748801946640015\n",
      "Passing Filters: 1129 \n",
      "MDS time:  24.296998023986816\n",
      "clash time:  0.49399805068969727\n",
      "final pass filter 196\n",
      "max next indices 98\n",
      "(196, 8, 3)\n",
      "Input Size: 78400\n",
      "backprop time :  7.833369493484497\n",
      "Passing Filters: 1024 \n",
      "MDS time:  22.02600383758545\n",
      "clash time:  0.5690004825592041\n",
      "final pass filter 270\n",
      "max next indices 127\n",
      "(270, 8, 3)\n",
      "Input Size: 108000\n",
      "backprop time :  9.67380142211914\n",
      "Passing Filters: 1035 \n",
      "MDS time:  21.861732721328735\n",
      "clash time:  0.4300551414489746\n",
      "final pass filter 98\n",
      "max next indices 155\n",
      "(98, 8, 3)\n",
      "Input Size: 39200\n",
      "backprop time :  5.383257627487183\n",
      "Passing Filters: 1016 \n",
      "MDS time:  22.02910304069519\n",
      "clash time:  0.556995153427124\n",
      "final pass filter 343\n",
      "max next indices 190\n",
      "(343, 8, 3)\n",
      "Input Size: 137200\n",
      "backprop time :  11.074109077453613\n",
      "Passing Filters: 1053 \n",
      "MDS time:  23.100870847702026\n",
      "clash time:  0.3719954490661621\n",
      "final pass filter 157\n",
      "max next indices 219\n",
      "(157, 8, 3)\n",
      "Input Size: 62800\n",
      "backprop time :  6.870514631271362\n",
      "Passing Filters: 1003 \n",
      "MDS time:  21.78679895401001\n",
      "clash time:  0.415999174118042\n",
      "final pass filter 138\n",
      "max next indices 247\n",
      "(138, 8, 3)\n",
      "Input Size: 55200\n",
      "backprop time :  6.200040102005005\n",
      "Passing Filters: 1030 \n",
      "MDS time:  21.95991849899292\n",
      "clash time:  0.504997730255127\n",
      "final pass filter 252\n",
      "max next indices 258\n",
      "(126, 8, 3)\n",
      "Input Size: 50400\n",
      "backprop time :  6.038856029510498\n",
      "Passing Filters: 1014 \n",
      "MDS time:  21.717536211013794\n",
      "clash time:  0.5540103912353516\n",
      "final pass filter 461\n"
     ]
    }
   ],
   "source": [
    "out_ep = build_protein_on_guide_clash(start_hel, ci_line, batch=400, \n",
    "                           next_mp_dist = 9, mp_deviation_limit = 5, maxOut=2000, maxClash_num = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d6309fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 126\n",
      "1 461\n",
      "total 587\n"
     ]
    }
   ],
   "source": [
    "sumL = 0\n",
    "\n",
    "for i,x in enumerate(out_ep):\n",
    "    sumL += len(x)\n",
    "    print(i, len(x))\n",
    "print('total',sumL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e848cd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "vp(out_ep[0], ci_line,  name='test', max_out=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9ee42a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vp(ep_in, guide_in,  name='test', max_out=10):\n",
    "    \n",
    "    outDirec='output/'\n",
    "    \n",
    "    if len(ep_in)>max_out:\n",
    "        it = max_out\n",
    "    else:\n",
    "        it = len(ep_in)\n",
    "    \n",
    "    for i in range(it):\n",
    "        hf.HelicalProtein.makePointPDB(ep_in[i], f'{name}{i}.pdb', outDirec='output/')\n",
    "        hf.HelicalProtein.makePointPDB(ep_in[i], f'{name}{i}.pdb', outDirec='output/')\n",
    "        \n",
    "        \n",
    "    hf.HelicalProtein.makePointPDB(guide_in,f'{name}_guide.pdb',outDirec='output/')\n",
    "        \n",
    "    cmd.delete(\"all\")\n",
    "    cmd.load(f'{outDirec}/{name}_guide.pdb')\n",
    "    for i in range(it):\n",
    "        cmd.load(f'{outDirec}/{name}{i}.pdb')\n",
    "        \n",
    "    \n",
    "    cmd.save(f'{outDirec}/viewEP_{name}.pse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4c726ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.0"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(30*24)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f2adfec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_protein_on_guide_clash(start_helices, guide_points, batch=200, \n",
    "                           next_mp_dist = 9, mp_deviation_limit = 5, maxOut=1000, maxClash_num = 3):\n",
    "    \n",
    "    clash_thresh = 2.85\n",
    "    loopCount = 0\n",
    "    \n",
    "    #this orientation promotes most likely growth to [0,0,1] (see distribution of reference set)\n",
    "    sh_xy = align_points_to_XYplane(start_helices, keep_orig_trans=False)\n",
    "    ci = np.zeros(start_helices.shape[0],dtype=np.int32)\n",
    "    master_ep = sh_xy[:,:4,...]# if there are more than 4ep (2 helices) just take the first two\n",
    "    \n",
    "    roomToGrow = True\n",
    "    output_ep_list = []\n",
    "\n",
    "    while roomToGrow:\n",
    "        \n",
    "\n",
    "        #second set of added points unused except for maintaining distance maps indexing from gen\n",
    "        #(based around 4 helices)\n",
    "        current_quad_prez = np.concatenate((master_ep[:,-4:,:], master_ep[:,-4:,:] ), axis=1)\n",
    "\n",
    "        #get a point on the desired line on mp dist away\n",
    "        mp_start = get_midpoint(current_quad_prez,helices_desired=[0,1])\n",
    "\n",
    "        #guide_start = gp[ci]\n",
    "        vg = np.repeat( np.expand_dims(guide_points, axis=0) , current_quad_prez.shape[0],axis=0)\n",
    "        #make that move backwards much larger than next_mp_dist\n",
    "        boo = np.repeat(np.expand_dims(np.arange(vg.shape[1]),axis=0), vg.shape[0],axis=0)\n",
    "        boo2 = (boo<np.expand_dims(np.expand_dims(ci,axis=1),axis=1))[:,0]\n",
    "        vg[boo2] = -1e6\n",
    "\n",
    "\n",
    "        dmp = np.linalg.norm(vg - np.expand_dims(mp_start,axis=1),axis=2)\n",
    "        am = np.argmin(np.abs(dmp - next_mp_dist),axis=1)\n",
    "        print('max next indices',max(am))\n",
    "        \n",
    "        tmp = vg[np.ix_(np.array(range(vg.shape[0])), am, np.array(range(3)))][0]\n",
    "\n",
    "        cqpz_dmp = np.concatenate((current_quad_prez, np.expand_dims(tmp,axis=1)), axis=1)\n",
    "        print(current_quad_prez.shape)\n",
    "\n",
    "        #rotate points and desired midpoint into trilaterization place\n",
    "        current_quad_tmp = rotate_base_tri_Zplane(cqpz_dmp,  target_point=8, index_mobile=[1,2,3])\n",
    "\n",
    "        target_midpoint = current_quad_tmp[:, 8, :]\n",
    "        current_quad = current_quad_tmp[:, :8, :]\n",
    "        #create distance map for generator\n",
    "        start_dist = np.expand_dims(current_quad,axis=1) - np.expand_dims(current_quad,axis=2)\n",
    "        dist = np.sqrt(np.sum(start_dist**2, 3))\n",
    "        dist = dist.reshape((dist.shape[0],-1))\n",
    "\n",
    "        #indices for reference map\n",
    "        ref_map_base = ref_distmap_index(dist, num_helices=4)\n",
    "\n",
    "        #GPU ##33s  with 500,000 samples with 200 cycles (average of 7 runs)\n",
    "        #CPU ##39s\n",
    "        #maybe there is something I can do to make this more effecient, not pipeline bottleneck so okay\n",
    "        #for small models like this tensor flow says gpu may not be more effecient\n",
    "        \n",
    "        start_loss = time.time()\n",
    "\n",
    "        print(f'Input Size: {ref_map_base.shape[0]*batch}')\n",
    "        \n",
    "        output_z, loss_mask, loss_mp, batchInd = fullBUTT_GPU(gen_obj, ref_map_base , target_midpoint, \n",
    "                                                            batch_size=batch, cycles=200, input_z=None, \n",
    "                                                            rate=0.05, target_ep=[4,5,6,7], num_helices=4, \n",
    "                                                            oneRef=True, scale=100.0, z_size=12)\n",
    "        \n",
    "        end_loss = time.time()\n",
    "        print('backprop time : ',end_loss-start_loss)\n",
    "\n",
    "        out_ep, uInd = mask_mp_filterBatch(gen_obj, output_z[-1], loss_mask[-1], loss_mp[-1], \n",
    "                                                  batchInd, max_mp_loss = 1e-3, max_loss_mask = 0.002, \n",
    "                                                  max_out=maxOut, print_stats= True)\n",
    "        \n",
    "        print(f'Passing Filters: {out_ep.shape[0]} ')\n",
    "        end_mds = time.time()\n",
    "        print('MDS time: ',end_mds-end_loss)\n",
    "        \n",
    "        if len(uInd) < 1:\n",
    "            print(f'Failed at helices length {len(master_ep)/2}')\n",
    "            return output_ep_list\n",
    "\n",
    "        fa, fa_reflect = align_generated_to_starting_ep(out_ep, current_quad_prez[uInd])\n",
    "\n",
    "        #use the reflection that promotes the most movement along the guide points\n",
    "        final_dr = determine_reflection(fa, fa_reflect, vg[uInd])\n",
    "\n",
    "        #ensure that midpoint is close enough\n",
    "        mpf = get_midpoint(final_dr,helices_desired=[2,3])\n",
    "        mpfdb = np.linalg.norm(tmp[uInd].squeeze()-mpf,axis=1) < mp_deviation_limit\n",
    "\n",
    "        final = final_dr[mpfdb]\n",
    "        \n",
    "        #remove steric clashes\n",
    "        build_epr = ge.EP_Recon(master_ep[uInd,:,:][mpfdb][:,-6:,:]) #pretty this need to be even slice,okay to be full too\n",
    "        #build_epr = ge.EP_Recon(master_ep[uInd,:,:][mpfdb][:,:,:])\n",
    "        \n",
    "        start_clash = time.time()\n",
    "            \n",
    "        query_epr = ge.EP_Recon(final[:,4:,:])\n",
    "        build = build_epr.to_npose()\n",
    "        query = query_epr.to_npose()\n",
    "        cc = np.zeros((len(build),),dtype=np.int32)\n",
    "        for x in range(len(build)) :\n",
    "            axa = scipy.spatial.distance.cdist(build[x],query[x])\n",
    "            cc[x] = np.sum(axa<clash_thresh)\n",
    "\n",
    "        remClash = cc<maxClash_num\n",
    "        \n",
    "        end_clash = time.time()\n",
    "        print('clash time: ', end_clash-start_clash)\n",
    "        \n",
    "        \n",
    "        master_ep = np.concatenate((master_ep[uInd,:,:][mpfdb][remClash], final[:,4:,:][remClash]), axis=1)\n",
    "        print(f'final pass filter' ,master_ep.shape[0])\n",
    "        if master_ep.shape[0]<2:\n",
    "            break\n",
    "\n",
    "        #remake ci here with new indices\n",
    "        final_mp = get_midpoint(final[remClash], helices_desired=[2,3])\n",
    "        dmp = np.linalg.norm(guide_points - np.expand_dims(final_mp,axis=1),axis=2)\n",
    "        ci = np.argmin(np.abs(dmp),axis=1)\n",
    "\n",
    "        end_dist = np.linalg.norm(guide_points[ci]-guide_points[-1],axis=1)\n",
    "        outOfRoom = end_dist<next_mp_dist\n",
    "\n",
    "        getOut = master_ep[outOfRoom]\n",
    "        master_ep = master_ep[~outOfRoom]\n",
    "        ci = ci[~outOfRoom]\n",
    "\n",
    "        if len(getOut)>1:\n",
    "            output_ep_list.append(getOut)\n",
    "\n",
    "        if len(master_ep) < 2:\n",
    "             roomToGrow = False\n",
    "                \n",
    "    return output_ep_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fd8f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195ca817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37becec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e69ea8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max next indices 2\n",
      "(2000, 8, 3)\n",
      "\n",
      "backprop time :  4.144015789031982\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "clash_thresh = 2.85\n",
    "loopCount = 0\n",
    "\n",
    "#this orientation promotes most likely growth to [0,0,1] (see distribution of reference set)\n",
    "sh_xy = align_points_to_XYplane(start_helices, keep_orig_trans=False)\n",
    "ci = np.zeros(start_helices.shape[0],dtype=np.int32)\n",
    "master_ep = sh_xy[:,:4,...]# if there are more than 4ep (2 helices) just take the first two\n",
    "\n",
    "roomToGrow = True\n",
    "output_ep_list = []\n",
    "\n",
    "#second set of added points unused except for maintaining distance maps indexing from gen\n",
    "#(based around 4 helices)\n",
    "current_quad_prez = np.concatenate((master_ep[:,-4:,:], master_ep[:,-4:,:] ), axis=1)\n",
    "\n",
    "#get a point on the desired line on mp dist away\n",
    "mp_start = get_midpoint(current_quad_prez,helices_desired=[0,1])\n",
    "\n",
    "#guide_start = gp[ci]\n",
    "vg = np.repeat( np.expand_dims(guide_points, axis=0) , current_quad_prez.shape[0],axis=0)\n",
    "#make that move backwards much larger than next_mp_dist\n",
    "boo = np.repeat(np.expand_dims(np.arange(vg.shape[1]),axis=0), vg.shape[0],axis=0)\n",
    "boo2 = (boo<np.expand_dims(np.expand_dims(ci,axis=1),axis=1))[:,0]\n",
    "vg[boo2] = -1e6\n",
    "\n",
    "\n",
    "dmp = np.linalg.norm(vg - np.expand_dims(mp_start,axis=1),axis=2)\n",
    "am = np.argmin(np.abs(dmp - next_mp_dist),axis=1)\n",
    "print('max next indices',max(am))\n",
    "\n",
    "tmp = vg[np.ix_(np.array(range(vg.shape[0])), am, np.array(range(3)))][0]\n",
    "\n",
    "cqpz_dmp = np.concatenate((current_quad_prez, np.expand_dims(tmp,axis=1)), axis=1)\n",
    "print(current_quad_prez.shape)\n",
    "\n",
    "#rotate points and desired midpoint into trilaterization place\n",
    "current_quad_tmp = rotate_base_tri_Zplane(cqpz_dmp,  target_point=8, index_mobile=[1,2,3])\n",
    "\n",
    "target_midpoint = current_quad_tmp[:, 8, :]\n",
    "current_quad = current_quad_tmp[:, :8, :]\n",
    "#create distance map for generator\n",
    "start_dist = np.expand_dims(current_quad,axis=1) - np.expand_dims(current_quad,axis=2)\n",
    "dist = np.sqrt(np.sum(start_dist**2, 3))\n",
    "dist = dist.reshape((dist.shape[0],-1))\n",
    "\n",
    "#indices for reference map\n",
    "ref_map_base = ref_distmap_index(dist, num_helices=4)\n",
    "\n",
    "#GPU ##33s  with 500,000 samples with 200 cycles (average of 7 runs)\n",
    "#CPU ##39s\n",
    "#maybe there is something I can do to make this more effecient, not pipeline bottleneck so okay\n",
    "#for small models like this tensor flow says gpu may not be more effecient\n",
    "\n",
    "start_loss = time.time()\n",
    "\n",
    "print('')\n",
    "\n",
    "output_z, loss_mask, loss_mp, batchInd = fullBUTT_GPU(gen_obj, ref_map_base , target_midpoint, \n",
    "                                                    batch_size=batch, cycles=200, input_z=None, \n",
    "                                                    rate=0.05, target_ep=[4,5,6,7], num_helices=4, \n",
    "                                                    oneRef=True, scale=100.0, z_size=12)\n",
    "\n",
    "end_loss = time.time()\n",
    "print('backprop time : ',end_loss-start_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "57ef4be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run!\n",
      "run!\n",
      "(50,)\n"
     ]
    }
   ],
   "source": [
    "out_ep, uInd =  mask_mp_filterBatch(gen_obj, output_z[-1], loss_mask[-1], loss_mp[-1], batchInd, max_mp_loss = 1e-3,\n",
    "                     max_loss_mask = 0.002, max_out=50, print_stats= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e95c8b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_mp_filterBatch(gen_obj, gen_z, loss_masked, loss_mp_ed, batchIndices, max_mp_loss = 1e-3,\n",
    "                     max_loss_mask = 0.002, max_out=200, print_stats= False):\n",
    "    \"\"\"Applies filter to backprop. Priotizes maximum batch diversity\"\"\"\n",
    "    \n",
    "    totalIndices = np.arange(len(batchIndices.flatten()))\n",
    "    #boolean arrays\n",
    "    mpl_b = loss_mp_ed < max_mp_loss\n",
    "    mask_b  = loss_masked < max_loss_mask\n",
    "    #indices for all passing \n",
    "    mp_b2 = np.nonzero(np.sum(mpl_b,axis=1)==mpl_b.shape[-1])[0] #index size1 tuple\n",
    "    mask_b2 = np.nonzero(np.sum(mask_b,axis=1)==mask_b.shape[-1])[0]\n",
    "    #places where both passed filters\n",
    "    mpmaskind = np.intersect1d(mask_b2, mp_b2)\n",
    "    \n",
    "    tI = totalIndices[mpmaskind]\n",
    "    bI = batchIndices[mpmaskind]\n",
    "    \n",
    "    indOut = np.array([],dtype=np.int32)\n",
    "    #pull out unique batches until desired number\n",
    "    while len(indOut)<maxOut and len(tI)>0:\n",
    "        uBI, uBI_ind  = np.unique(bI,return_index=True)\n",
    "        indOut = np.concatenate((indOut,tI[uBI_ind]))\n",
    "        opInd = np.setdiff1d(np.arange(len(bI)), uBI_ind)\n",
    "\n",
    "        bI = bI[opInd]\n",
    "        tI = tI[opInd]\n",
    "    \n",
    "    \n",
    "    indOut= indOut[:max_out]\n",
    "    \n",
    "        \n",
    "    identified_z = gen_z[indOut]\n",
    "    \n",
    "    gen_obj.generate(z=12, input_z = identified_z, batch_size=identified_z.shape[0])\n",
    "    gen_obj.MDS_reconstruct_()\n",
    "    \n",
    "    out_ep = np.array(gen_obj.reconsMDS_)\n",
    "    \n",
    "    return out_ep, batchIndices[indOut]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf83b697",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_masked = loss_mask[-1]\n",
    "loss_mp_ed = loss_mp[-1]\n",
    "max_mp_loss = 1e-3\n",
    "max_loss_mask = 0.002\n",
    "max_out=200\n",
    "print_stats= False\n",
    "mask_first=True \n",
    "randomReduce=True\n",
    "\n",
    "totalInd = np.arange(len(batchInd.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0432b9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl_b = loss_mp_ed < max_mp_loss\n",
    "mask_b  = loss_masked < max_loss_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6b96472e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 3)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpl_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ada42487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 28)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed55fdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_b2 = np.nonzero(np.sum(mpl_b,axis=1)==3)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bedbf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_b2 = np.nonzero(np.sum(mask_b,axis=1)==28)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1b0a4b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpmaskind = np.intersect1d(mask_b2, mp_b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a7c85cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tI = totalInd[mpmaskind]\n",
    "bI = batchInd[mpmaskind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "382719b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "indOut = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e817dbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "906 2067\n",
      "1585 1388\n"
     ]
    }
   ],
   "source": [
    "while len(indOut)<maxOut and len(tI)>0:\n",
    "    uBI, uBI_ind  = np.unique(bI,return_index=True)\n",
    "    \n",
    "\n",
    "    \n",
    "    indOut = np.concatenate((indOut,tI[uBI_ind]))\n",
    "    opInd = np.setdiff1d(np.arange(len(bI)), uBI_ind)\n",
    "    \n",
    "    bI = bI[opInd]\n",
    "    tI = tI[opInd]\n",
    "    \n",
    "    print(len(indOut),len(tI))\n",
    "    \n",
    "indOut= indOut[:maxOut]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "67cf2990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3d90f883",
   "metadata": {},
   "outputs": [],
   "source": [
    " ?np.setdiff1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9d421c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   17,    32,    33, ..., 19977, 19979, 19991])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eb7a1dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1,    3,    3, ..., 1997, 1997, 1999])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1a45fe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "uBI, uBI_ind  = np.unique(bI,return_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b92d8db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "?np.unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f90a7ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(906,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uBI.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12d422c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   17,    32,    33, ..., 19977, 19979, 19991], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpmaskint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e48cd5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 17,  32,  33,  35,  38,  45,  60,  64,  74,  75, 121, 122, 123,\n",
       "       125, 127, 129, 131, 152, 153, 156, 157, 163, 200, 201, 203, 204,\n",
       "       209, 231, 233, 238, 290, 291, 294, 297, 302, 303, 304, 307, 308,\n",
       "       316, 319, 333, 337, 341, 343, 344, 349, 353, 355, 356, 357, 359,\n",
       "       371, 375, 377, 381, 385, 391, 392, 395, 397, 398, 415, 416, 419,\n",
       "       443, 444, 445, 447, 448, 449, 451, 452, 453, 456, 459, 471, 473,\n",
       "       479, 483, 492, 532, 537, 539, 540, 541, 542, 545, 546, 547, 548,\n",
       "       549, 571, 576, 577, 578, 581, 582, 583, 587], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpmaskint[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "663c871b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 200)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchInd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "dfc70692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6,\n",
       "       6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "       8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchInd[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5ef82c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1d82be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8540df75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda34813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f60aa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buttress_ep_from_z_mask_mp(gen_obj, gen_z, loss_masked, loss_mp_ed, batchIndices, max_mp_loss = 1e-3,\n",
    "                                 max_loss_mask = 0.002, max_out=200, print_stats= False, mask_first=True, randomReduce=True):\n",
    "    \n",
    "    if mask_first:\n",
    "        #make sure the input two helices are maintained in generator output\n",
    "        sm = np.sum(loss_masked,axis=1)\n",
    "        smi = np.argsort(sm) #get sorted indices of lowest mask loss\n",
    "        sm_sort = sm[smi]\n",
    "        smi_o = smi[sm_sort < max_loss_mask]\n",
    "        smp = np.sum(loss_mp_ed, axis=1)\n",
    "\n",
    "        ind2 = np.array(range(len(sm)))\n",
    "        uInd = batchIndices[smi_o][smp[smi_o]<max_mp_loss]\n",
    "        uiInd = ind2[smi_o][smp[smi_o]<max_mp_loss]\n",
    "    else:\n",
    "        #make sure the input two helices are maintained in generator output\n",
    "        sm = np.sum(loss_mp_ed,axis=1)\n",
    "        smi = np.argsort(sm) #get sorted indices of lowest mask loss\n",
    "        sm_sort = sm[smi]\n",
    "        smi_o = smi[sm_sort < max_mp_loss]\n",
    "        smp = np.sum(loss_mp_ed, axis=1)\n",
    "\n",
    "        ind2 = np.array(range(len(sm)))\n",
    "        uInd = batchIndices[smi_o][smp[smi_o]<max_loss_mask]\n",
    "        uiInd = ind2[smi_o][smp[smi_o]<max_loss_mask]\n",
    "        \n",
    "\n",
    "    if len(uInd) > max_out:\n",
    "        if randomReduce:\n",
    "            indexR = random_reduce(uInd, num_to_keep = max_out)\n",
    "            uInd = uInd[indexR]\n",
    "            uiInd = uiInd[indexR]\n",
    "            \n",
    "        else:\n",
    "            uInd = uInd[:max_out]\n",
    "            uiInd = uiInd[:max_out]\n",
    "    \n",
    "    if print_stats:\n",
    "        print('Input Size: ',      len(sm))\n",
    "        print('Passing Filters: ', len(uInd))\n",
    "        \n",
    "    if len(uInd) < 1:\n",
    "        return np.array([]), np.array([])\n",
    "    \n",
    "    \n",
    "    \n",
    "    identified_z = gen_z[uiInd]\n",
    "    \n",
    "    gen_obj.generate(z=12, input_z = identified_z, batch_size=identified_z.shape[0])\n",
    "    gen_obj.MDS_reconstruct_()\n",
    "    \n",
    "    out_ep = np.array(gen_obj.reconsMDS_)\n",
    "    \n",
    "    return out_ep, uInd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a97439",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fa_tfpy",
   "language": "python",
   "name": "fa_tfpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
